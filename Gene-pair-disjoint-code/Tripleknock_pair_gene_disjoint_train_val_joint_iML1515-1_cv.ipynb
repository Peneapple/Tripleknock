{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a07b062",
   "metadata": {},
   "source": [
    "# Tripleknock Revision — Pair-disjoint Cross-validation (E. coli iML1515)\n",
    "\n",
    "本 Notebook **只做双基因 pair 不重复（Pair-disjoint）** 的 5-fold 交叉验证，用来回应 Reviewer 关于 **gene-pair leakage** 的质疑。\n",
    "\n",
    "✅ 核心思想（Strict pair-disjoint / 严格双基因不重复）  \n",
    "对每个 gene pair（AB）分配一个 fold：`fold(pair)=hash(pair) % K`  \n",
    "一个三基因组合 (A,B,C) 只在以下条件成立时被允许进入某个 fold：\n",
    "\n",
    "- `fold(AB) == fold(AC) == fold(BC)`\n",
    "\n",
    "否则该 triple 会被丢弃（跨 fold 的 triple 直接丢弃）。\n",
    "\n",
    "这样做的好处：\n",
    "\n",
    "- ✅ Train/Test 之间 **不会共享任何 gene pair**\n",
    "- ✅ 不会出现 “ABC 导致 AB/AC/BC 分到不同 fold 的矛盾” —— 因为这种 triple 会直接被过滤掉\n",
    "\n",
    "---\n",
    "\n",
    "## 事先准备好的对象\n",
    "- `two_mer_dict`：每个基因的 400 维 2-mer 特征向量（代码里已生成）\n",
    "- `ae1_2({g1,g2,g3})`：输出 (3,400) tensor（代码里已定义并测试成功）\n",
    "- `device`：GPU device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0342442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "[2026-02-15 15:43:37] Config loaded.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Part 0: Imports + Global Config\n",
    "# =========================\n",
    "import os, time, random, traceback, math, gc, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ---- device ----\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "# =========================\n",
    "# Adjustable parameters / 可调参数\n",
    "# =========================\n",
    "\n",
    "# ---- Data files ----\n",
    "FILE_PARTS = [\n",
    "    '/data1/xpgeng/cross_pathogen/FBA/iML1515_parts/iML1515-1.csv',\n",
    "    # '/data1/xpgeng/cross_pathogen/FBA/iML1515_parts/iML1515-2.csv',\n",
    "]\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "PAIR_HASH_SEED = 2026\n",
    "\n",
    "# ---- Streaming CSV read ----\n",
    "CSV_CHUNK_SIZE = 200_000\n",
    "\n",
    "# ---- Build fold pools (sampling) ----\n",
    "POOL_TARGET_PER_FOLD = 400_000                             ### change!!! 1\n",
    "\n",
    "# ---- Sampling sizes inside each CV fold ----\n",
    "# We use 5 folds; each fold contains 200k samples (total 1,000,000).\n",
    "# In each CV iteration: 1 fold is Test (=200k), remaining 4 folds form the TrainPool (=800k).\n",
    "# Then we split TrainPool into Train/Val by VAL_FRACTION (default 10%).\n",
    "TRAIN_SIZE_PER_FOLD = None      # None => use full TrainPool\n",
    "VAL_FRACTION        = 0.10      # 10% of TrainPool as validation\n",
    "TEST_SIZE_PER_FOLD  = 400_000   # should match POOL_TARGET_PER_FOLD                            ### change!!! 2\n",
    "\n",
    "USE_STRATIFIED_SAMPLE = True\n",
    "\n",
    "# ---- Chunk training to avoid GPU OOM ----\n",
    "TRAIN_CHUNK_SIZE = 20_000\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# ---- Optimization ----\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "MAX_EPOCHS = 12\n",
    "PATIENCE = 3\n",
    "MIN_DELTA = 5e-4\n",
    "\n",
    "# ---- Model architecture (unchanged; only dropout adjustable) ----\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "# ---- Feature options ----\n",
    "REST_SCALE = 0.1\n",
    "NORM_MODE = 'block'   # 'block' | 'per_sample' | 'none'\n",
    "\n",
    "# ---- Threshold search options ----\n",
    "THRESH_METRIC = 'youden'\n",
    "MIN_PRECISION_POS = None\n",
    "THRESH_MIN  = 0.05\n",
    "THRESH_MAX  = 0.95\n",
    "THRESH_STEP = 0.005\n",
    "\n",
    "# ---- Plot saving ----\n",
    "SAVE_PLOTS = True\n",
    "PLOT_PREFIX = \"pair_gene_disjoint_cv_iML1515-1-2000k_data\"                             ### change!!! 3\n",
    "\n",
    "# -------------------------\n",
    "# Logging\n",
    "# -------------------------\n",
    "workdir = os.getcwd()\n",
    "log_file = os.path.join(workdir, f'{PLOT_PREFIX}_log.txt')\n",
    "err_file = os.path.join(workdir, f'{PLOT_PREFIX}_err.txt')\n",
    "\n",
    "def log_print(msg: str):\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    line = f'[{ts}] {msg}'\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(line + \"\\n\")\n",
    "    print(line)\n",
    "\n",
    "def log_error(msg: str):\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    line = f'[{ts}] {msg}'\n",
    "    with open(err_file, 'a') as f:\n",
    "        f.write(line + \"\\n\")\n",
    "    print(line)\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "log_print('Config loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9774a",
   "metadata": {},
   "source": [
    "## Helper utilities / 工具函数\n",
    "\n",
    "包含：\n",
    "\n",
    "- `stratified_subsample()`：分层抽样保持 0/1 比例  \n",
    "- `find_best_threshold()`：在 val 上找阈值（youden / balanced_acc / f1_pos）  \n",
    "- `stable_pair_fold()`：为每个 pair 赋予 fold（稳定、可复现，不依赖 Python 内置 hash）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1272fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Part 1: Utilities\n",
    "# =========================\n",
    "\n",
    "def stratified_subsample(df: pd.DataFrame, n: int, seed: int = 0):\n",
    "    # Simple stratified sampling to keep y=1 ratio similar\n",
    "    if n is None or n <= 0 or n >= len(df):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos = df[df['y'] == 1]\n",
    "    neg = df[df['y'] == 0]\n",
    "\n",
    "    if len(pos) == 0 or len(neg) == 0:\n",
    "        return df.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    pos_n = int(n * (len(pos) / len(df)))\n",
    "    neg_n = n - pos_n\n",
    "\n",
    "    pos_idx = rng.choice(pos.index.to_numpy(), size=min(pos_n, len(pos)), replace=False)\n",
    "    neg_idx = rng.choice(neg.index.to_numpy(), size=min(neg_n, len(neg)), replace=False)\n",
    "\n",
    "    out = pd.concat([df.loc[pos_idx], df.loc[neg_idx]], axis=0).sample(frac=1.0, random_state=seed)\n",
    "    out = out.reset_index(drop=True)\n",
    "\n",
    "    if len(out) < n:\n",
    "        remain = df.drop(out.index, errors='ignore')\n",
    "        extra = remain.sample(n=min(n-len(out), len(remain)), random_state=seed)\n",
    "        out = pd.concat([out, extra], axis=0).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "def find_best_threshold(\n",
    "    y_true,\n",
    "    y_prob,\n",
    "    metric=\"youden\",\n",
    "    min_precision_pos=None,\n",
    "    t_min=0.01,\n",
    "    t_max=0.99,\n",
    "    t_step=0.01,\n",
    "):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).astype(float)\n",
    "\n",
    "    best = None\n",
    "\n",
    "    for thr in np.arange(t_min, t_max + 1e-12, t_step):\n",
    "        y_hat = (y_prob >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn + 1e-12)\n",
    "        fpr = fp / (fp + tn + 1e-12)\n",
    "        tnr = tn / (tn + fp + 1e-12)\n",
    "\n",
    "        precision_pos = tp / (tp + fp + 1e-12)\n",
    "        recall_pos = tpr\n",
    "        f1_pos = 2 * precision_pos * recall_pos / (precision_pos + recall_pos + 1e-12)\n",
    "\n",
    "        if min_precision_pos is not None and precision_pos < min_precision_pos:\n",
    "            continue\n",
    "\n",
    "        if metric == \"youden\":\n",
    "            score = tpr - fpr\n",
    "        elif metric == \"balanced_acc\":\n",
    "            score = 0.5 * (tpr + tnr)\n",
    "        elif metric == \"f1_pos\":\n",
    "            score = f1_pos\n",
    "        else:\n",
    "            score = tpr - fpr\n",
    "\n",
    "        if (best is None) or (score > best[\"score\"]):\n",
    "            best = {\n",
    "                \"threshold\": float(thr),\n",
    "                \"score\": float(score),\n",
    "                \"f1_pos\": float(f1_pos),\n",
    "                \"precision_pos\": float(precision_pos),\n",
    "                \"recall_pos\": float(recall_pos),\n",
    "                \"tn\": int(tn),\n",
    "                \"fp\": int(fp),\n",
    "                \"fn\": int(fn),\n",
    "                \"tp\": int(tp),\n",
    "            }\n",
    "\n",
    "    return best\n",
    "\n",
    "# ---- Stable pair -> fold assignment ----\n",
    "_pair_cache = {}\n",
    "\n",
    "def stable_pair_fold(gA: str, gB: str, K: int = 5, seed: int = 2026) -> int:\n",
    "    a = str(gA).strip()\n",
    "    b = str(gB).strip()\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "\n",
    "    key = (a, b)\n",
    "    if key in _pair_cache:\n",
    "        return _pair_cache[key]\n",
    "\n",
    "    s = f\"{a}|{b}|{seed}\".encode(\"utf-8\")\n",
    "    h = hashlib.blake2b(s, digest_size=8).digest()   # stable 64-bit\n",
    "    v = int.from_bytes(h, byteorder=\"little\", signed=False)\n",
    "    fold = int(v % K)\n",
    "    _pair_cache[key] = fold\n",
    "    return fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207caff",
   "metadata": {},
   "source": [
    "## Feature preparation / 特征准备\n",
    "\n",
    "这部分直接复用原来的 AE 与 2-mer 构建逻辑（保持不变）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be97f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total genes in FASTA: 4305\n",
      "Example: [('b0001', 'MKRISTTITTTITITTGNGAG')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building two_mer_dict: 100%|█████████████████████████████████████████████| 4305/4305 [00:00<00:00, 7640.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0001 [0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.  ]\n",
      "b0002 [0.01587302 0.001221   0.00854701 0.01098901 0.002442   0.00854701\n",
      " 0.         0.003663   0.00610501 0.00732601]\n",
      "b0003 [0.01294498 0.00647249 0.00647249 0.01294498 0.         0.00647249\n",
      " 0.00323625 0.00323625 0.00323625 0.01294498]\n",
      "[2026-02-15 15:44:09] two_mer_dict & ae1_2 ready.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Part 2: Build two_mer_dict (2-mer features) + load Autoencoders + define ae1_2\n",
    "# =========================\n",
    "\n",
    "# ---- (A) Build two_mer_dict from FASTA ----\n",
    "# 如果你已经有 two_mer_dict，可以把 BUILD_2MER=False，然后跳过。\n",
    "BUILD_2MER = True\n",
    "\n",
    "if BUILD_2MER:\n",
    "    from Bio import SeqIO\n",
    "    from collections import Counter\n",
    "\n",
    "    fasta_path = '/data1/xpgeng/cross_pathogen/autoencoder/E.coli.tag_seq.fasta'\n",
    "\n",
    "    def read_fasta(fp):\n",
    "        gene_sequence_dict = {}\n",
    "        for record in SeqIO.parse(fp, 'fasta'):\n",
    "            gene_sequence_dict[record.id] = str(record.seq)\n",
    "        return gene_sequence_dict\n",
    "\n",
    "    gene_sequence_dict = read_fasta(fasta_path)\n",
    "    all_genes = set(gene_sequence_dict.keys())\n",
    "\n",
    "    print('Total genes in FASTA:', len(all_genes))\n",
    "    print('Example:', list(gene_sequence_dict.items())[:1])\n",
    "\n",
    "    standard_amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    all_2mers = [a + b for a in standard_amino_acids for b in standard_amino_acids]\n",
    "    two_mer_index = {two_mer: idx for idx, two_mer in enumerate(all_2mers)}\n",
    "\n",
    "    two_mer_dict = {}\n",
    "\n",
    "    for gene, sequence in tqdm(gene_sequence_dict.items(), desc='Building two_mer_dict'):\n",
    "        sequence = ''.join([aa for aa in sequence if aa in standard_amino_acids])\n",
    "\n",
    "        if len(sequence) < 2:\n",
    "            two_mer_dict[gene] = np.zeros(400, dtype=np.float32)\n",
    "            continue\n",
    "\n",
    "        two_mer_counts = Counter(sequence[i:i+2] for i in range(len(sequence)-1))\n",
    "        total_two_mers = sum(two_mer_counts.values())\n",
    "\n",
    "        feature_vector = np.zeros(400, dtype=np.float32)\n",
    "        for two_mer, count in two_mer_counts.items():\n",
    "            idx = two_mer_index.get(two_mer)\n",
    "            if idx is not None:\n",
    "                feature_vector[idx] = count / total_two_mers\n",
    "\n",
    "        two_mer_dict[gene] = feature_vector\n",
    "\n",
    "    for gene, vec in list(two_mer_dict.items())[:3]:\n",
    "        print(gene, vec[:10])\n",
    "\n",
    "# ---- (B) Load Autoencoders and define ae1_2 ----\n",
    "# 你可以保留你的架构和权重加载方式（与原来一致）\n",
    "\n",
    "LOAD_AUTOENCODERS = True\n",
    "\n",
    "if LOAD_AUTOENCODERS:\n",
    "\n",
    "    class Autoencoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Autoencoder, self).__init__()\n",
    "            self.encoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(400, 256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.35),\n",
    "                torch.nn.Linear(256, 128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.35),\n",
    "                torch.nn.Linear(128, 3),\n",
    "            )\n",
    "            self.decoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(3, 128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(128, 256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(256, 400),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "\n",
    "\n",
    "    class Autoencoder2(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Autoencoder2, self).__init__()\n",
    "            self.encoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(4304, 3000),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.2),\n",
    "                torch.nn.Linear(3000, 1000),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.3),\n",
    "                torch.nn.Linear(1000, 400),\n",
    "            )\n",
    "            self.decoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(400, 1000),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(1000, 3000),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(3000, 4304),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "\n",
    "\n",
    "    model = Autoencoder().to(device)\n",
    "    model.load_state_dict(torch.load('/data1/xpgeng/cross_pathogen/autoencoder/ae1_all_data_training.pth', map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    model2 = Autoencoder2().to(device)\n",
    "    model2.load_state_dict(torch.load('/data1/xpgeng/cross_pathogen/autoencoder/ae2_all_data_training.pth', map_location=device))\n",
    "    model2.eval()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ae1_2(three_genes):\n",
    "    # 输入：set({g1,g2,g3})\n",
    "    # 输出：(3,400) tensor\n",
    "    rest_genes = list(all_genes - three_genes)\n",
    "    inputs = np.vstack([two_mer_dict[gene] for gene in rest_genes]).astype(np.float32)\n",
    "\n",
    "    zeros_400 = np.zeros((2, 400), dtype=np.float32)\n",
    "    inputs = np.vstack([inputs, zeros_400])\n",
    "\n",
    "    inputs = torch.tensor(inputs).to(device)\n",
    "    inputs = model.encoder(inputs)\n",
    "    inputs = inputs.cpu().detach().numpy().T\n",
    "    inputs = torch.tensor(inputs).to(device)\n",
    "    outputs = model2.encoder(inputs)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "log_print('two_mer_dict & ae1_2 ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0099e",
   "metadata": {},
   "source": [
    "# Part A — Pair-disjoint CV (Strict unseen gene-pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064943aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-15 15:45:55] Reading CSV parts (streaming) and building pair-disjoint pools...\n",
      "[2026-02-15 15:45:55] File: /data1/xpgeng/cross_pathogen/FBA/iML1515_parts/iML1515-1.csv\n",
      "[2026-02-15 15:45:59]   chunk=5 | kept=40,301 dropped=959,699 | pool_sizes=[7975, 8166, 8017, 8075, 8068]\n",
      "[2026-02-15 15:46:03]   chunk=10 | kept=80,126 dropped=1,919,874 | pool_sizes=[15962, 16241, 15987, 15958, 15978]\n",
      "[2026-02-15 15:46:06]   chunk=15 | kept=119,709 dropped=2,880,291 | pool_sizes=[23889, 24216, 23936, 23764, 23904]\n",
      "[2026-02-15 15:46:09]   chunk=20 | kept=159,653 dropped=3,840,347 | pool_sizes=[31785, 32450, 31881, 31607, 31930]\n",
      "[2026-02-15 15:46:12]   chunk=25 | kept=199,725 dropped=4,800,275 | pool_sizes=[39711, 40559, 39897, 39564, 39994]\n",
      "[2026-02-15 15:46:16]   chunk=30 | kept=239,780 dropped=5,760,220 | pool_sizes=[47631, 48583, 47849, 47653, 48064]\n",
      "[2026-02-15 15:46:19]   chunk=35 | kept=279,996 dropped=6,720,004 | pool_sizes=[55447, 56712, 56076, 55677, 56084]\n",
      "[2026-02-15 15:46:22]   chunk=40 | kept=320,092 dropped=7,679,908 | pool_sizes=[63492, 64799, 64161, 63643, 63997]\n",
      "[2026-02-15 15:46:26]   chunk=45 | kept=360,141 dropped=8,639,859 | pool_sizes=[71483, 72837, 72137, 71684, 72000]\n",
      "[2026-02-15 15:46:29]   chunk=50 | kept=400,187 dropped=9,599,813 | pool_sizes=[79447, 81076, 80070, 79689, 79905]\n",
      "[2026-02-15 15:46:33]   chunk=55 | kept=439,966 dropped=10,560,034 | pool_sizes=[87316, 89072, 87981, 87622, 87975]\n",
      "[2026-02-15 15:46:37]   chunk=60 | kept=480,106 dropped=11,519,894 | pool_sizes=[95156, 97122, 96038, 95694, 96096]\n",
      "[2026-02-15 15:46:40]   chunk=65 | kept=520,156 dropped=12,479,844 | pool_sizes=[103041, 105133, 104071, 103677, 104234]\n",
      "[2026-02-15 15:46:43]   chunk=70 | kept=560,160 dropped=13,439,840 | pool_sizes=[110905, 113241, 112050, 111645, 112319]\n",
      "[2026-02-15 15:46:47]   chunk=75 | kept=600,375 dropped=14,399,625 | pool_sizes=[118812, 121418, 120261, 119532, 120352]\n",
      "[2026-02-15 15:46:50]   chunk=80 | kept=640,246 dropped=15,359,754 | pool_sizes=[126661, 129667, 128183, 127346, 128389]\n",
      "[2026-02-15 15:46:53]   chunk=85 | kept=680,045 dropped=16,319,955 | pool_sizes=[134468, 137755, 136084, 135314, 136424]\n",
      "[2026-02-15 15:46:57]   chunk=90 | kept=720,156 dropped=17,279,844 | pool_sizes=[142446, 145909, 144136, 143281, 144384]\n",
      "[2026-02-15 15:47:01]   chunk=95 | kept=760,087 dropped=18,239,913 | pool_sizes=[150351, 154020, 152101, 151222, 152393]\n",
      "[2026-02-15 15:47:05]   chunk=100 | kept=800,259 dropped=19,199,741 | pool_sizes=[158226, 162055, 160229, 159420, 160329]\n",
      "[2026-02-15 15:47:08]   chunk=105 | kept=840,479 dropped=20,159,521 | pool_sizes=[166178, 170126, 168397, 167378, 168400]\n",
      "[2026-02-15 15:47:12]   chunk=110 | kept=880,644 dropped=21,119,356 | pool_sizes=[174127, 178265, 176505, 175285, 176462]\n",
      "[2026-02-15 15:47:15]   chunk=115 | kept=920,814 dropped=22,079,186 | pool_sizes=[182124, 186377, 184602, 183273, 184438]\n",
      "[2026-02-15 15:47:19]   chunk=120 | kept=961,057 dropped=23,038,943 | pool_sizes=[190216, 194468, 192644, 191227, 192502]\n",
      "[2026-02-15 15:47:22]   chunk=125 | kept=1,000,910 dropped=23,999,090 | pool_sizes=[198107, 202556, 200649, 199148, 200450]\n",
      "[2026-02-15 15:47:25]   chunk=130 | kept=1,041,463 dropped=24,958,537 | pool_sizes=[206164, 210768, 208662, 207212, 208657]\n",
      "[2026-02-15 15:47:28]   chunk=135 | kept=1,081,604 dropped=25,918,396 | pool_sizes=[213964, 218814, 216713, 215343, 216770]\n",
      "[2026-02-15 15:47:31]   chunk=140 | kept=1,121,647 dropped=26,878,353 | pool_sizes=[221823, 226920, 224748, 223464, 224692]\n",
      "[2026-02-15 15:47:34]   chunk=145 | kept=1,161,896 dropped=27,838,104 | pool_sizes=[229830, 234995, 232837, 231521, 232713]\n",
      "[2026-02-15 15:47:37]   chunk=150 | kept=1,201,595 dropped=28,798,405 | pool_sizes=[237673, 242999, 240673, 239586, 240664]\n",
      "[2026-02-15 15:47:40]   chunk=155 | kept=1,241,741 dropped=29,758,259 | pool_sizes=[245644, 251019, 248794, 247546, 248738]\n",
      "[2026-02-15 15:47:43]   chunk=160 | kept=1,281,759 dropped=30,718,241 | pool_sizes=[253671, 259072, 256659, 255526, 256831]\n",
      "[2026-02-15 15:47:46]   chunk=165 | kept=1,321,667 dropped=31,678,333 | pool_sizes=[261570, 267121, 264602, 263537, 264837]\n",
      "[2026-02-15 15:47:49]   chunk=170 | kept=1,361,258 dropped=32,638,742 | pool_sizes=[269406, 275203, 272529, 271368, 272752]\n",
      "[2026-02-15 15:47:52]   chunk=175 | kept=1,401,322 dropped=33,598,678 | pool_sizes=[277331, 283340, 280511, 279408, 280732]\n",
      "[2026-02-15 15:47:55]   chunk=180 | kept=1,441,415 dropped=34,558,585 | pool_sizes=[285330, 291381, 288555, 287370, 288779]\n",
      "[2026-02-15 15:47:58]   chunk=185 | kept=1,481,436 dropped=35,518,564 | pool_sizes=[293358, 299368, 296455, 295376, 296879]\n",
      "[2026-02-15 15:48:01]   chunk=190 | kept=1,521,733 dropped=36,478,267 | pool_sizes=[301259, 307470, 304408, 303564, 305032]\n",
      "[2026-02-15 15:48:04]   chunk=195 | kept=1,561,792 dropped=37,438,208 | pool_sizes=[309173, 315486, 312400, 311660, 313073]\n",
      "[2026-02-15 15:48:06]   chunk=200 | kept=1,601,536 dropped=38,398,464 | pool_sizes=[317058, 323544, 320349, 319479, 321106]\n",
      "[2026-02-15 15:48:09]   chunk=205 | kept=1,641,373 dropped=39,358,627 | pool_sizes=[324810, 331633, 328377, 327355, 329198]\n",
      "[2026-02-15 15:48:12]   chunk=210 | kept=1,681,290 dropped=40,318,710 | pool_sizes=[332681, 339699, 336525, 335243, 337142]\n",
      "[2026-02-15 15:48:15]   chunk=215 | kept=1,721,080 dropped=41,278,920 | pool_sizes=[340465, 347840, 344611, 343152, 345012]\n",
      "[2026-02-15 15:48:18]   chunk=220 | kept=1,761,029 dropped=42,238,971 | pool_sizes=[348483, 355861, 352529, 351075, 353081]\n",
      "[2026-02-15 15:48:22]   chunk=225 | kept=1,800,837 dropped=43,199,163 | pool_sizes=[356161, 364000, 360502, 359121, 361053]\n",
      "[2026-02-15 15:48:25]   chunk=230 | kept=1,840,622 dropped=44,159,378 | pool_sizes=[363980, 372007, 368385, 367255, 368995]\n",
      "[2026-02-15 15:48:28]   chunk=235 | kept=1,880,454 dropped=45,119,546 | pool_sizes=[371854, 380047, 376341, 375236, 376976]\n",
      "[2026-02-15 15:48:31]   chunk=240 | kept=1,920,878 dropped=46,079,122 | pool_sizes=[379898, 388277, 384518, 383235, 384950]\n",
      "[2026-02-15 15:48:34]   chunk=245 | kept=1,960,923 dropped=47,039,077 | pool_sizes=[387805, 396377, 392522, 391288, 392931]\n",
      "[2026-02-15 15:48:36]   chunk=250 | kept=1,994,882 dropped=47,999,118 | pool_sizes=[395756, 400000, 400000, 399126, 400000]\n",
      "[2026-02-15 15:48:38] All fold pools reached target, stop reading more data.\n",
      "[2026-02-15 15:48:39] Pair-disjoint pools built. kept=2,000,000, dropped=48,511,957\n",
      "[2026-02-15 15:48:39] Final fold pool sizes: [400000, 400000, 400000, 400000, 400000]\n",
      "[2026-02-15 15:48:39] [FoldPool 0] y=1 ratio = 0.3378\n",
      "[2026-02-15 15:48:39] [FoldPool 1] y=1 ratio = 0.3410\n",
      "[2026-02-15 15:48:39] [FoldPool 2] y=1 ratio = 0.3389\n",
      "[2026-02-15 15:48:39] [FoldPool 3] y=1 ratio = 0.3391\n",
      "[2026-02-15 15:48:39] [FoldPool 4] y=1 ratio = 0.3407\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Part A1: Read CSV(s) and build Pair-disjoint fold pools (streaming)\n",
    "# =========================\n",
    "\n",
    "def build_pair_disjoint_fold_pools(\n",
    "    file_parts,\n",
    "    K=5,\n",
    "    pair_seed=2026,\n",
    "    pool_target=200_000,\n",
    "    chunk_size=200_000,\n",
    "):\n",
    "    pools = [[] for _ in range(K)]\n",
    "    kept = 0\n",
    "    dropped = 0\n",
    "\n",
    "    def pools_full():\n",
    "        return all(len(p) >= pool_target for p in pools)\n",
    "\n",
    "    log_print(\"Reading CSV parts (streaming) and building pair-disjoint pools...\")\n",
    "\n",
    "    for fp in file_parts:\n",
    "        log_print(f\"File: {fp}\")\n",
    "        reader = pd.read_csv(\n",
    "            fp,\n",
    "            header=None,\n",
    "            names=['g1','g2','g3','y'],\n",
    "            chunksize=chunk_size,\n",
    "            dtype={0:str, 1:str, 2:str, 3:'int8'},\n",
    "            engine='c',\n",
    "            low_memory=False\n",
    "        )\n",
    "\n",
    "        for chunk_id, dfc in enumerate(reader, start=1):\n",
    "            dfc['g1'] = dfc['g1'].astype(str).str.strip()\n",
    "            dfc['g2'] = dfc['g2'].astype(str).str.strip()\n",
    "            dfc['g3'] = dfc['g3'].astype(str).str.strip()\n",
    "            dfc['y']  = dfc['y'].astype('int8')\n",
    "\n",
    "            for g1, g2, g3, y in dfc.itertuples(index=False, name=None):\n",
    "                f12 = stable_pair_fold(g1, g2, K=K, seed=pair_seed)\n",
    "                f13 = stable_pair_fold(g1, g3, K=K, seed=pair_seed)\n",
    "                if f12 != f13:\n",
    "                    dropped += 1\n",
    "                    continue\n",
    "                f23 = stable_pair_fold(g2, g3, K=K, seed=pair_seed)\n",
    "                if f12 != f23:\n",
    "                    dropped += 1\n",
    "                    continue\n",
    "\n",
    "                fold = f12\n",
    "                if len(pools[fold]) < pool_target:\n",
    "                    pools[fold].append((g1, g2, g3, int(y)))\n",
    "                    kept += 1\n",
    "\n",
    "                if pools_full():\n",
    "                    break\n",
    "\n",
    "            if chunk_id % 5 == 0:\n",
    "                sizes = [len(p) for p in pools]\n",
    "                log_print(f\"  chunk={chunk_id} | kept={kept:,} dropped={dropped:,} | pool_sizes={sizes}\")\n",
    "\n",
    "            if pools_full():\n",
    "                log_print(\"All fold pools reached target, stop reading more data.\")\n",
    "                break\n",
    "\n",
    "        if pools_full():\n",
    "            break\n",
    "\n",
    "    fold_dfs = []\n",
    "    for i in range(K):\n",
    "        dfi = pd.DataFrame(pools[i], columns=['g1','g2','g3','y'])\n",
    "        fold_dfs.append(dfi)\n",
    "\n",
    "    log_print(f\"Pair-disjoint pools built. kept={kept:,}, dropped={dropped:,}\")\n",
    "    log_print(\"Final fold pool sizes: \" + str([len(x) for x in fold_dfs]))\n",
    "    return fold_dfs\n",
    "\n",
    "pair_fold_pools = build_pair_disjoint_fold_pools(\n",
    "    FILE_PARTS,\n",
    "    K=N_FOLDS,\n",
    "    pair_seed=PAIR_HASH_SEED,\n",
    "    pool_target=POOL_TARGET_PER_FOLD,\n",
    "    chunk_size=CSV_CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "for i, dfi in enumerate(pair_fold_pools):\n",
    "    if len(dfi) > 0:\n",
    "        log_print(f\"[FoldPool {i}] y=1 ratio = {dfi['y'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c64ee1",
   "metadata": {},
   "source": [
    "## Part A2 — Train MLP + record train/val loss and val AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e5dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-15 15:49:30] \n",
      "================== Pair Fold 0 ==================\n",
      "[2026-02-15 15:49:30] Train pool=1,600,000 | Test pool=400,000\n",
      "[2026-02-15 15:49:31] Train=1,440,000, Val=160,000, Test=400,000\n",
      "[2026-02-15 15:49:31] y_train mean=0.3399, y_val mean=0.3399, y_test mean=0.3378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 VAL:  12%|██████▋                                                | 19496/160000 [04:10<27:37, 84.79it/s]"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Part A2: Feature builder + MLP + CV training (Pair-disjoint)\n",
    "# =========================\n",
    "\n",
    "def zscore(vec, eps=1e-8):\n",
    "    return (vec - vec.mean()) / (vec.std() + eps)\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_feature_vector(g1, g2, g3, rest_scale=0.05, norm_mode='block'):\n",
    "    three = np.array([two_mer_dict[g] for g in [g1, g2, g3]], dtype=np.float32).flatten()\n",
    "    rest = ae1_2({g1, g2, g3}).detach().cpu().numpy().astype(np.float32).flatten()\n",
    "\n",
    "    if norm_mode == 'block':\n",
    "        three = zscore(three)\n",
    "        rest  = zscore(rest)\n",
    "\n",
    "    feat = np.concatenate([three, rest * rest_scale], axis=0).astype(np.float32)\n",
    "\n",
    "    if norm_mode == 'per_sample':\n",
    "        feat = zscore(feat)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def build_XY_from_df(dfx: pd.DataFrame, rest_scale=0.05, norm_mode='block', desc='Build XY'):\n",
    "    triples = dfx[['g1','g2','g3']].values.tolist()\n",
    "    y = dfx['y'].values.astype(np.int64)\n",
    "\n",
    "    X = np.zeros((len(triples), 2400), dtype=np.float32)\n",
    "    for i, (g1, g2, g3) in enumerate(tqdm(triples, desc=desc, leave=False)):\n",
    "        X[i] = build_feature_vector(g1, g2, g3, rest_scale=rest_scale, norm_mode=norm_mode)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=2400, hidden_size1=512, hidden_size2=256, output_size=1, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "def train_one_epoch_streaming(\n",
    "    model, optimizer, criterion,\n",
    "    df_train_pool: pd.DataFrame,\n",
    "    batch_size=512,\n",
    "    train_chunk_size=20000,\n",
    "    rest_scale=0.05,\n",
    "    norm_mode='block',\n",
    "    fold=0,\n",
    "    epoch=1,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    idx = np.arange(len(df_train_pool))\n",
    "    rng = np.random.default_rng(SEED + fold * 100000 + epoch)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    seen = 0\n",
    "\n",
    "    for start in range(0, len(idx), train_chunk_size):\n",
    "        chunk_idx = idx[start:start+train_chunk_size]\n",
    "        dfx = df_train_pool.iloc[chunk_idx].reset_index(drop=True)\n",
    "\n",
    "        Xc, yc = build_XY_from_df(\n",
    "            dfx, rest_scale=rest_scale, norm_mode=norm_mode,\n",
    "            desc=f\"Train chunk {start//train_chunk_size+1}\"\n",
    "        )\n",
    "\n",
    "        ds = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(Xc),\n",
    "            torch.from_numpy(yc.astype(np.float32))\n",
    "        )\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pred = model(xb).view(-1)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * len(yb)\n",
    "            seen += len(yb)\n",
    "\n",
    "        del Xc, yc, ds, dl, dfx\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / max(seen, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loss_and_auc(model, X_np, y_np, criterion, batch_size=2048):\n",
    "    model.eval()\n",
    "    y_np = np.asarray(y_np).astype(np.int64)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    seen = 0\n",
    "    probs_all = []\n",
    "\n",
    "    for i in range(0, len(X_np), batch_size):\n",
    "        xb = torch.tensor(X_np[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "        yb = torch.tensor(y_np[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "\n",
    "        pb = model(xb).view(-1)\n",
    "        loss = criterion(pb, yb)\n",
    "\n",
    "        total_loss += loss.item() * len(yb)\n",
    "        seen += len(yb)\n",
    "        probs_all.append(pb.detach().cpu().numpy())\n",
    "\n",
    "    probs_all = np.concatenate(probs_all, axis=0)\n",
    "    auc = roc_auc_score(y_np, probs_all)\n",
    "    return total_loss / max(seen, 1), auc, probs_all\n",
    "\n",
    "def stratified_train_val_split(df: pd.DataFrame, val_frac: float = 0.10, seed: int = 0):\n",
    "    \"\"\"Split df into train/val with stratification on y and no sample overlap.\"\"\"\n",
    "    if val_frac <= 0:\n",
    "        return df.reset_index(drop=True), df.iloc[0:0].copy()\n",
    "    if val_frac >= 1:\n",
    "        return df.iloc[0:0].copy(), df.reset_index(drop=True)\n",
    "\n",
    "    # if only one class exists, fall back to random split without stratify\n",
    "    y_unique = df['y'].nunique(dropna=False)\n",
    "    if y_unique < 2:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = np.arange(len(df))\n",
    "        rng.shuffle(idx)\n",
    "        n_val = int(round(len(df) * val_frac))\n",
    "        val_idx = idx[:n_val]\n",
    "        train_idx = idx[n_val:]\n",
    "        df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "        df_val = df.iloc[val_idx].reset_index(drop=True)\n",
    "        return df_train, df_val\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        df.index.values,\n",
    "        test_size=val_frac,\n",
    "        random_state=seed,\n",
    "        shuffle=True,\n",
    "        stratify=df['y']\n",
    "    )\n",
    "    df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "    df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "    return df_train, df_val\n",
    "\n",
    "def run_pair_disjoint_cv(pair_fold_pools):\n",
    "    aucs = []\n",
    "\n",
    "    for fold in range(N_FOLDS):\n",
    "        log_print(\"\\n\" + \"=\"*18 + f\" Pair Fold {fold} \" + \"=\"*18)\n",
    "\n",
    "        df_test_pool = pair_fold_pools[fold].reset_index(drop=True)\n",
    "        df_train_pool = pd.concat(\n",
    "            [pair_fold_pools[i] for i in range(N_FOLDS) if i != fold],\n",
    "            axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        log_print(f\"Train pool={len(df_train_pool):,} | Test pool={len(df_test_pool):,}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train/Val/Test construction\n",
    "        # -----------------------------\n",
    "        if TRAIN_SIZE_PER_FOLD is None:\n",
    "            df_train_all = df_train_pool\n",
    "        else:\n",
    "            df_train_all = stratified_subsample(df_train_pool, TRAIN_SIZE_PER_FOLD, seed=SEED + fold)\n",
    "\n",
    "        # Split TrainPool -> Train / Val (10% by default), no overlap and stratified on y.\n",
    "        df_train, df_val = stratified_train_val_split(\n",
    "            df_train_all,\n",
    "            val_frac=VAL_FRACTION,\n",
    "            seed=SEED + fold + 100\n",
    "        )\n",
    "\n",
    "        # Test = one fold (200k). Keep stratified_subsample for safety if pool > TEST_SIZE_PER_FOLD.\n",
    "        df_test = stratified_subsample(\n",
    "            df_test_pool,\n",
    "            TEST_SIZE_PER_FOLD,\n",
    "            seed=SEED + fold + 200\n",
    "        )\n",
    "\n",
    "        log_print(f\"Train={len(df_train):,}, Val={len(df_val):,}, Test={len(df_test):,}\")\n",
    "        log_print(\n",
    "            f\"y_train mean={df_train.y.mean():.4f}, \"\n",
    "            f\"y_val mean={df_val.y.mean():.4f}, \"\n",
    "            f\"y_test mean={df_test.y.mean():.4f}\"\n",
    "        )\n",
    "\n",
    "        # -----------------------------\n",
    "        # Build features for VAL/TEST once per fold\n",
    "        # -----------------------------\n",
    "        X_val, y_val = build_XY_from_df(\n",
    "            df_val, rest_scale=REST_SCALE, norm_mode=NORM_MODE,\n",
    "            desc=f\"Fold {fold} VAL\"\n",
    "        )\n",
    "        X_test, y_test = build_XY_from_df(\n",
    "            df_test, rest_scale=REST_SCALE, norm_mode=NORM_MODE,\n",
    "            desc=f\"Fold {fold} TEST\"\n",
    "        )\n",
    "\n",
    "        # -----------------------------\n",
    "        # Model / Optimizer / Loss\n",
    "        # -----------------------------\n",
    "        model_mlp = MLP(dropout_rate=DROPOUT_RATE).to(device)\n",
    "        optimizer = optim.AdamW(model_mlp.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        best_auc = -1\n",
    "        best_state = None\n",
    "        best_epoch = 0\n",
    "        patience_left = PATIENCE\n",
    "\n",
    "        history = {\"train_loss\": [], \"val_loss\": [], \"val_auc\": []}\n",
    "\n",
    "        for epoch in range(1, MAX_EPOCHS + 1):\n",
    "            train_loss = train_one_epoch_streaming(\n",
    "                model_mlp, optimizer, criterion,\n",
    "                df_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_chunk_size=TRAIN_CHUNK_SIZE,\n",
    "                rest_scale=REST_SCALE,\n",
    "                norm_mode=NORM_MODE,\n",
    "                fold=fold,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "\n",
    "            val_loss, val_auc, y_val_prob = eval_loss_and_auc(model_mlp, X_val, y_val, criterion)\n",
    "\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_auc\"].append(val_auc)\n",
    "\n",
    "            log_print(f\"[Fold {fold}] Epoch {epoch}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}, val_auc={val_auc:.6f}\")\n",
    "\n",
    "            if val_auc > best_auc + MIN_DELTA:\n",
    "                best_auc = val_auc\n",
    "                best_epoch = epoch\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model_mlp.state_dict().items()}\n",
    "                patience_left = PATIENCE\n",
    "            else:\n",
    "                patience_left -= 1\n",
    "                if patience_left <= 0:\n",
    "                    log_print(f\"[Fold {fold}] Early stop at epoch {epoch} (best_val_auc={best_auc:.6f} @epoch {best_epoch})\")\n",
    "                    break\n",
    "\n",
    "        if best_state is not None:\n",
    "            model_mlp.load_state_dict(best_state)\n",
    "\n",
    "        val_loss_best, val_auc_best, y_val_prob = eval_loss_and_auc(model_mlp, X_val, y_val, criterion)\n",
    "\n",
    "        best_t = find_best_threshold(\n",
    "            y_true=y_val,\n",
    "            y_prob=y_val_prob,\n",
    "            metric=THRESH_METRIC,\n",
    "            min_precision_pos=MIN_PRECISION_POS,\n",
    "            t_min=THRESH_MIN,\n",
    "            t_max=THRESH_MAX,\n",
    "            t_step=THRESH_STEP,\n",
    "        )\n",
    "\n",
    "        if best_t is None:\n",
    "            best_thr = 0.5\n",
    "            log_print(f\"[Fold {fold}] No valid threshold found, fallback thr=0.5\")\n",
    "        else:\n",
    "            best_thr = best_t[\"threshold\"]\n",
    "            log_print(f\"[Fold {fold}] Best threshold from VAL = {best_thr:.3f} | metric={THRESH_METRIC}\")\n",
    "            log_print(f\"[Fold {fold}] VAL best stats = {best_t}\")\n",
    "\n",
    "        test_loss, test_auc, y_test_prob = eval_loss_and_auc(model_mlp, X_test, y_test, criterion)\n",
    "        aucs.append(test_auc)\n",
    "\n",
    "        y_pred = (y_test_prob >= best_thr).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "        rep = classification_report(y_test, y_pred, digits=4)\n",
    "\n",
    "        log_print(f\"[Fold {fold}] ✅ TEST AUC = {test_auc:.6f}\")\n",
    "        log_print(f\"[Fold {fold}] TEST loss = {test_loss:.6f}\")\n",
    "        log_print(f\"[Fold {fold}] Confusion Matrix [[TN,FP],[FN,TP]]:\\n{cm}\")\n",
    "        log_print(f\"[Fold {fold}] Report:\\n{rep}\")\n",
    "\n",
    "        if SAVE_PLOTS:\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "            plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.title(f\"Pair Fold {fold} Loss Curves\")\n",
    "            plt.savefig(f\"{PLOT_PREFIX}_fold{fold}_LOSS.png\", dpi=600, bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(history[\"val_auc\"], label=\"val_auc\")\n",
    "            plt.legend()\n",
    "            plt.title(f\"Pair Fold {fold} Validation AUC\")\n",
    "            plt.savefig(f\"{PLOT_PREFIX}_fold{fold}_AUC.png\", dpi=600, bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "\n",
    "    log_print(\"\\n\" + \"=\"*20 + \" Pair-disjoint CV Summary \" + \"=\"*20)\n",
    "    log_print(f\"AUCs per fold: {aucs}\")\n",
    "    log_print(f\"Mean AUC = {np.mean(aucs):.6f}, Std = {np.std(aucs):.6f}\")\n",
    "\n",
    "run_pair_disjoint_cv(pair_fold_pools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f276188-293c-48cd-b4ca-1f44d7a148fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorchh] *",
   "language": "python",
   "name": "conda-env-.conda-pytorchh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
