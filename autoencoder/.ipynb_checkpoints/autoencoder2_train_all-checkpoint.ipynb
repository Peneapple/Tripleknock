{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b667737",
   "metadata": {},
   "source": [
    "After the autoencoder compress the gene feature from 400 to 3, the autoencoder2 is used to compress the gene number from 4303 to 400, here we add a zero row to change the gene number from 4303 to 4304. Aim to make our model could be "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d62a4f",
   "metadata": {},
   "source": [
    "To avoid confusion, we still use the model, Autoencoder to represent the model1, use the Autoencoder2, model2 to represent the model2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240da0a5",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1207ae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene: b0001, Sequence: MKRISTTITTTITITTGNGAG...\n",
      "Gene: b0002, Sequence: MRVLKFGGTSVANAERFLRVADILESNARQ...\n",
      "Gene: b0003, Sequence: MVKVYAPASSANMSVGFDVLGAAVTPVDGA...\n",
      "Gene: b0004, Sequence: MKLYNLKDHNEQVSFAQAVTQGLGKNQGLF...\n",
      "Gene: b0005, Sequence: MKKMQSIVLALSLVLVAPMAAQAAEITLVP...\n",
      "Gene: b0001\n",
      "2-mer Feature Vector: [0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.05\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.05 0.15 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.05 0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.05 0.   0.15 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "Gene: b0002\n",
      "2-mer Feature Vector: [0.01587302 0.001221   0.00854701 0.01098901 0.002442   0.00854701\n",
      " 0.         0.003663   0.00610501 0.00732601 0.00610501 0.00610501\n",
      " 0.002442   0.00732601 0.01343101 0.002442   0.003663   0.004884\n",
      " 0.         0.         0.         0.001221   0.001221   0.001221\n",
      " 0.         0.001221   0.         0.         0.         0.002442\n",
      " 0.         0.         0.001221   0.         0.002442   0.\n",
      " 0.001221   0.002442   0.         0.         0.004884   0.003663\n",
      " 0.003663   0.00732601 0.001221   0.004884   0.001221   0.003663\n",
      " 0.         0.004884   0.         0.         0.004884   0.003663\n",
      " 0.         0.001221   0.001221   0.004884   0.         0.002442\n",
      " 0.003663   0.         0.002442   0.002442   0.003663   0.00610501\n",
      " 0.         0.002442   0.00610501 0.01098901 0.001221   0.003663\n",
      " 0.004884   0.001221   0.00610501 0.003663   0.001221   0.002442\n",
      " 0.         0.002442   0.00732601 0.001221   0.         0.\n",
      " 0.001221   0.003663   0.001221   0.         0.001221   0.003663\n",
      " 0.001221   0.003663   0.001221   0.001221   0.         0.003663\n",
      " 0.001221   0.002442   0.         0.002442   0.00610501 0.\n",
      " 0.003663   0.003663   0.003663   0.003663   0.002442   0.004884\n",
      " 0.002442   0.004884   0.00732601 0.00610501 0.001221   0.003663\n",
      " 0.003663   0.003663   0.002442   0.01098901 0.         0.002442\n",
      " 0.         0.         0.         0.         0.         0.002442\n",
      " 0.         0.001221   0.         0.002442   0.001221   0.001221\n",
      " 0.001221   0.002442   0.         0.         0.         0.002442\n",
      " 0.         0.002442   0.00732601 0.001221   0.003663   0.00610501\n",
      " 0.002442   0.002442   0.         0.001221   0.002442   0.003663\n",
      " 0.001221   0.002442   0.002442   0.         0.001221   0.01098901\n",
      " 0.003663   0.002442   0.001221   0.         0.003663   0.\n",
      " 0.         0.003663   0.004884   0.003663   0.002442   0.002442\n",
      " 0.001221   0.004884   0.001221   0.003663   0.         0.\n",
      " 0.001221   0.002442   0.002442   0.003663   0.         0.\n",
      " 0.01465201 0.         0.002442   0.00976801 0.003663   0.004884\n",
      " 0.002442   0.00732601 0.00732601 0.01343101 0.002442   0.004884\n",
      " 0.00610501 0.001221   0.01098901 0.00854701 0.002442   0.004884\n",
      " 0.         0.001221   0.00610501 0.         0.002442   0.001221\n",
      " 0.001221   0.001221   0.         0.001221   0.002442   0.001221\n",
      " 0.         0.         0.         0.001221   0.002442   0.004884\n",
      " 0.         0.002442   0.         0.         0.00732601 0.\n",
      " 0.003663   0.001221   0.         0.002442   0.001221   0.004884\n",
      " 0.002442   0.00732601 0.001221   0.002442   0.002442   0.\n",
      " 0.         0.001221   0.003663   0.003663   0.001221   0.\n",
      " 0.003663   0.001221   0.003663   0.         0.001221   0.003663\n",
      " 0.         0.001221   0.         0.00610501 0.         0.002442\n",
      " 0.         0.002442   0.003663   0.         0.         0.00610501\n",
      " 0.         0.         0.003663   0.001221   0.001221   0.004884\n",
      " 0.001221   0.002442   0.         0.002442   0.         0.004884\n",
      " 0.001221   0.001221   0.002442   0.001221   0.         0.003663\n",
      " 0.         0.003663   0.         0.001221   0.00610501 0.\n",
      " 0.003663   0.004884   0.001221   0.004884   0.         0.004884\n",
      " 0.002442   0.004884   0.         0.001221   0.         0.003663\n",
      " 0.002442   0.001221   0.003663   0.00854701 0.         0.002442\n",
      " 0.003663   0.         0.003663   0.003663   0.002442   0.00610501\n",
      " 0.001221   0.004884   0.001221   0.002442   0.002442   0.002442\n",
      " 0.         0.002442   0.004884   0.00610501 0.002442   0.00610501\n",
      " 0.002442   0.003663   0.002442   0.001221   0.002442   0.002442\n",
      " 0.001221   0.004884   0.001221   0.002442   0.         0.004884\n",
      " 0.         0.003663   0.002442   0.001221   0.001221   0.003663\n",
      " 0.002442   0.003663   0.         0.         0.01221001 0.002442\n",
      " 0.00610501 0.001221   0.003663   0.00854701 0.001221   0.00610501\n",
      " 0.004884   0.01465201 0.         0.001221   0.002442   0.\n",
      " 0.002442   0.001221   0.00610501 0.00732601 0.         0.001221\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.001221   0.001221   0.         0.\n",
      " 0.         0.001221   0.         0.         0.001221   0.\n",
      " 0.         0.         0.002442   0.         0.001221   0.\n",
      " 0.001221   0.001221   0.002442   0.001221   0.         0.002442\n",
      " 0.         0.         0.         0.002442   0.         0.003663\n",
      " 0.002442   0.001221   0.         0.002442  ]\n",
      "Gene: b0003\n",
      "2-mer Feature Vector: [0.01294498 0.00647249 0.00647249 0.01294498 0.         0.00647249\n",
      " 0.00323625 0.00323625 0.00323625 0.01294498 0.00647249 0.00323625\n",
      " 0.00647249 0.00647249 0.00970874 0.00647249 0.         0.00970874\n",
      " 0.         0.00323625 0.         0.         0.00323625 0.\n",
      " 0.00323625 0.00323625 0.         0.00323625 0.         0.\n",
      " 0.         0.         0.         0.00323625 0.00323625 0.00323625\n",
      " 0.         0.         0.00323625 0.00323625 0.         0.00323625\n",
      " 0.         0.00323625 0.         0.00323625 0.         0.00323625\n",
      " 0.00647249 0.         0.         0.00323625 0.         0.\n",
      " 0.         0.         0.00647249 0.00970874 0.00323625 0.\n",
      " 0.00647249 0.         0.         0.00323625 0.         0.00647249\n",
      " 0.00323625 0.00323625 0.00323625 0.00970874 0.         0.00970874\n",
      " 0.00647249 0.         0.00647249 0.         0.00647249 0.\n",
      " 0.00323625 0.         0.00647249 0.00323625 0.00647249 0.\n",
      " 0.         0.         0.         0.00323625 0.         0.00323625\n",
      " 0.         0.         0.         0.         0.00323625 0.00323625\n",
      " 0.         0.00323625 0.         0.         0.01294498 0.\n",
      " 0.00323625 0.00323625 0.01618123 0.00323625 0.         0.00647249\n",
      " 0.00970874 0.00323625 0.00323625 0.         0.00323625 0.\n",
      " 0.00970874 0.01294498 0.         0.         0.         0.\n",
      " 0.00323625 0.00323625 0.         0.         0.         0.00323625\n",
      " 0.         0.00323625 0.         0.00323625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00323625 0.00647249 0.00323625 0.         0.00323625\n",
      " 0.         0.00647249 0.00647249 0.00323625 0.00323625 0.00323625\n",
      " 0.         0.         0.00323625 0.         0.         0.00970874\n",
      " 0.         0.00323625 0.         0.         0.         0.\n",
      " 0.00323625 0.         0.         0.         0.         0.\n",
      " 0.         0.00647249 0.         0.00647249 0.00647249 0.00323625\n",
      " 0.         0.         0.         0.00647249 0.         0.\n",
      " 0.01294498 0.00323625 0.00323625 0.00970874 0.00323625 0.02265372\n",
      " 0.         0.         0.         0.00970874 0.01294498 0.00647249\n",
      " 0.00970874 0.00323625 0.         0.         0.         0.\n",
      " 0.00323625 0.         0.00323625 0.         0.         0.\n",
      " 0.         0.00323625 0.         0.00323625 0.00323625 0.\n",
      " 0.         0.00323625 0.00323625 0.00323625 0.         0.00323625\n",
      " 0.00323625 0.00323625 0.         0.         0.         0.\n",
      " 0.00647249 0.00323625 0.         0.         0.         0.00323625\n",
      " 0.         0.00323625 0.00647249 0.00323625 0.         0.00323625\n",
      " 0.         0.         0.         0.00323625 0.         0.00323625\n",
      " 0.00647249 0.00323625 0.         0.00647249 0.         0.00970874\n",
      " 0.         0.00323625 0.         0.00323625 0.         0.\n",
      " 0.         0.         0.00323625 0.00323625 0.00323625 0.00647249\n",
      " 0.         0.00323625 0.00647249 0.00323625 0.00323625 0.00647249\n",
      " 0.         0.         0.         0.00323625 0.         0.00323625\n",
      " 0.         0.00323625 0.00323625 0.00323625 0.00323625 0.\n",
      " 0.         0.00323625 0.         0.00323625 0.00323625 0.\n",
      " 0.         0.00647249 0.00647249 0.         0.00323625 0.00323625\n",
      " 0.         0.00970874 0.         0.         0.         0.01294498\n",
      " 0.00323625 0.         0.         0.00647249 0.         0.\n",
      " 0.00647249 0.         0.         0.00323625 0.         0.01618123\n",
      " 0.         0.00323625 0.         0.00323625 0.         0.\n",
      " 0.         0.00323625 0.00323625 0.00647249 0.00323625 0.00647249\n",
      " 0.         0.         0.00970874 0.         0.         0.\n",
      " 0.00323625 0.         0.         0.         0.         0.00647249\n",
      " 0.         0.         0.00323625 0.         0.00323625 0.\n",
      " 0.         0.00323625 0.         0.         0.01941748 0.\n",
      " 0.00323625 0.00323625 0.         0.00323625 0.00323625 0.00323625\n",
      " 0.00323625 0.00970874 0.         0.         0.00323625 0.\n",
      " 0.         0.00323625 0.00647249 0.00647249 0.         0.00647249\n",
      " 0.         0.         0.         0.00323625 0.         0.\n",
      " 0.         0.         0.         0.00647249 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00323625\n",
      " 0.         0.         0.00323625 0.         0.00323625 0.\n",
      " 0.         0.         0.         0.         0.         0.00323625\n",
      " 0.         0.         0.00323625 0.00323625 0.00647249 0.00323625\n",
      " 0.         0.         0.         0.        ]\n",
      "Gene: b0004\n",
      "2-mer Feature Vector: [0.01405152 0.00234192 0.00468384 0.00234192 0.01405152 0.00468384\n",
      " 0.00468384 0.00468384 0.00702576 0.00936768 0.00234192 0.00468384\n",
      " 0.00234192 0.00936768 0.         0.         0.00936768 0.0117096\n",
      " 0.         0.00234192 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00234192\n",
      " 0.         0.         0.         0.00234192 0.         0.\n",
      " 0.00234192 0.         0.         0.00234192 0.00234192 0.\n",
      " 0.00468384 0.00936768 0.00936768 0.00468384 0.00234192 0.\n",
      " 0.00234192 0.00936768 0.         0.         0.         0.00234192\n",
      " 0.         0.         0.00468384 0.00468384 0.         0.\n",
      " 0.00468384 0.         0.         0.00702576 0.00234192 0.\n",
      " 0.         0.00702576 0.00234192 0.01639344 0.00234192 0.\n",
      " 0.00234192 0.00234192 0.00468384 0.00468384 0.00936768 0.\n",
      " 0.         0.00234192 0.00702576 0.00234192 0.00468384 0.00234192\n",
      " 0.00234192 0.00468384 0.00234192 0.00468384 0.00468384 0.00468384\n",
      " 0.00234192 0.         0.00468384 0.         0.00234192 0.00234192\n",
      " 0.         0.00234192 0.         0.00234192 0.00234192 0.00234192\n",
      " 0.0117096  0.00468384 0.         0.00468384 0.         0.\n",
      " 0.00468384 0.01639344 0.         0.00468384 0.00234192 0.00234192\n",
      " 0.00234192 0.         0.00234192 0.         0.         0.00468384\n",
      " 0.00468384 0.         0.00468384 0.         0.         0.00234192\n",
      " 0.         0.00234192 0.         0.         0.         0.00468384\n",
      " 0.00234192 0.00234192 0.         0.         0.         0.\n",
      " 0.         0.         0.00468384 0.00234192 0.00468384 0.00234192\n",
      " 0.         0.00234192 0.         0.         0.         0.0117096\n",
      " 0.         0.00234192 0.00234192 0.         0.         0.00468384\n",
      " 0.         0.         0.00234192 0.         0.00234192 0.\n",
      " 0.00468384 0.00936768 0.00234192 0.         0.         0.00702576\n",
      " 0.         0.00936768 0.         0.00234192 0.00234192 0.00234192\n",
      " 0.00234192 0.00234192 0.         0.00468384 0.         0.\n",
      " 0.00936768 0.         0.00468384 0.00468384 0.0117096  0.01873536\n",
      " 0.00234192 0.         0.0117096  0.00702576 0.00234192 0.00468384\n",
      " 0.01639344 0.00234192 0.00468384 0.00702576 0.00936768 0.00468384\n",
      " 0.         0.00468384 0.00234192 0.         0.00234192 0.\n",
      " 0.         0.         0.         0.         0.00234192 0.00468384\n",
      " 0.00234192 0.00234192 0.         0.         0.00234192 0.\n",
      " 0.         0.         0.         0.         0.00234192 0.\n",
      " 0.00234192 0.00234192 0.00234192 0.         0.00234192 0.00468384\n",
      " 0.         0.00468384 0.         0.00234192 0.00234192 0.00468384\n",
      " 0.         0.00468384 0.         0.00702576 0.00234192 0.\n",
      " 0.00702576 0.         0.         0.00234192 0.         0.00234192\n",
      " 0.00468384 0.         0.00468384 0.00468384 0.         0.00468384\n",
      " 0.         0.00468384 0.00702576 0.00234192 0.00234192 0.00702576\n",
      " 0.         0.         0.00936768 0.         0.         0.00702576\n",
      " 0.         0.00468384 0.         0.00234192 0.         0.00936768\n",
      " 0.00234192 0.         0.00234192 0.00234192 0.         0.\n",
      " 0.00234192 0.00234192 0.00234192 0.         0.00702576 0.\n",
      " 0.00234192 0.00234192 0.00702576 0.00234192 0.         0.\n",
      " 0.00468384 0.00234192 0.         0.00234192 0.         0.\n",
      " 0.00234192 0.00234192 0.         0.00468384 0.         0.\n",
      " 0.00702576 0.         0.00234192 0.00234192 0.00234192 0.00468384\n",
      " 0.00234192 0.00234192 0.         0.00468384 0.         0.00234192\n",
      " 0.00468384 0.00234192 0.00234192 0.         0.         0.00468384\n",
      " 0.         0.         0.00702576 0.         0.         0.00234192\n",
      " 0.         0.00234192 0.00234192 0.00234192 0.         0.00936768\n",
      " 0.00234192 0.00234192 0.         0.00702576 0.00468384 0.00468384\n",
      " 0.00234192 0.00468384 0.         0.         0.01405152 0.\n",
      " 0.00234192 0.00702576 0.         0.00234192 0.         0.00234192\n",
      " 0.00702576 0.         0.         0.00234192 0.00468384 0.\n",
      " 0.00234192 0.00702576 0.00702576 0.00468384 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00234192 0.00234192 0.         0.00234192 0.         0.\n",
      " 0.         0.         0.00234192 0.         0.         0.\n",
      " 0.00234192 0.00468384 0.         0.         0.         0.\n",
      " 0.         0.00234192 0.00234192 0.         0.00234192 0.\n",
      " 0.00234192 0.         0.         0.00234192]\n",
      "Gene: b0005\n",
      "2-mer Feature Vector: [0.02061856 0.         0.         0.01030928 0.         0.\n",
      " 0.         0.         0.         0.01030928 0.         0.\n",
      " 0.02061856 0.01030928 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01030928 0.02061856 0.\n",
      " 0.         0.         0.         0.01030928 0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01030928 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.         0.         0.02061856 0.02061856 0.\n",
      " 0.01030928 0.         0.         0.01030928 0.02061856 0.\n",
      " 0.         0.         0.         0.         0.01030928 0.01030928\n",
      " 0.         0.         0.01030928 0.         0.         0.04123711\n",
      " 0.03092784 0.         0.01030928 0.01030928 0.         0.\n",
      " 0.         0.         0.01030928 0.         0.         0.\n",
      " 0.01030928 0.01030928 0.         0.         0.         0.\n",
      " 0.         0.01030928 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.01030928 0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.         0.01030928 0.\n",
      " 0.02061856 0.01030928 0.01030928 0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.\n",
      " 0.         0.01030928 0.         0.01030928 0.         0.03092784\n",
      " 0.         0.         0.01030928 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.01030928 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02061856 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01030928\n",
      " 0.01030928 0.         0.         0.         0.01030928 0.\n",
      " 0.04123711 0.         0.01030928 0.01030928 0.         0.\n",
      " 0.         0.         0.01030928 0.         0.         0.\n",
      " 0.         0.         0.01030928 0.01030928 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02061856 0.         0.         0.02061856 0.01030928 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01030928 0.         0.01030928 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.02061856 0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01030928 0.         0.         0.\n",
      " 0.01030928 0.         0.01030928 0.         0.         0.\n",
      " 0.         0.         0.02061856 0.         0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01030928 0.01030928]\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 定义读取FASTA文件并返回基因-序列字典的函数\n",
    "def read_fasta(file_path):\n",
    "    gene_sequence_dict = {}\n",
    "    \n",
    "    # 使用SeqIO解析FASTA文件\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        # 获取基因名称（FASTA文件的标头部分）\n",
    "        gene_name = record.id\n",
    "        # 获取蛋白质序列（序列部分）\n",
    "        sequence = str(record.seq)\n",
    "        # 将基因-序列的键值对添加到字典中\n",
    "        gene_sequence_dict[gene_name] = sequence\n",
    "\n",
    "    return gene_sequence_dict\n",
    "\n",
    "# 读取FASTA文件并生成字典\n",
    "file_path = \"E.coli.tag_seq.fasta\"  # 请替换为你的FASTA文件路径\n",
    "gene_sequence_dict = read_fasta(file_path)\n",
    "\n",
    "# 打印字典的前几个项以确认\n",
    "for gene, sequence in list(gene_sequence_dict.items())[:5]:\n",
    "    print(f\"Gene: {gene}, Sequence: {sequence[:30]}...\")  # 只打印前30个氨基酸\n",
    "\n",
    "# 定义所有可能的2-mer组合\n",
    "standard_amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "all_2mers = [a + b for a in standard_amino_acids for b in standard_amino_acids]\n",
    "two_mer_index = {two_mer: idx for idx, two_mer in enumerate(all_2mers)}\n",
    "\n",
    "# 生成基因2-mer特征字典\n",
    "two_mer_dict = {}\n",
    "\n",
    "# 遍历每个基因和序列\n",
    "for gene, sequence in gene_sequence_dict.items():\n",
    "    # 清洗序列，移除非标准氨基酸字符\n",
    "    sequence = ''.join([aa for aa in sequence if aa in standard_amino_acids])\n",
    "    \n",
    "    # 计算2-mer出现次数\n",
    "    two_mer_counts = Counter([sequence[i:i+2] for i in range(len(sequence)-1)])\n",
    "    \n",
    "    # 计算2-mer的总数\n",
    "    total_two_mers = sum(two_mer_counts.values())\n",
    "    \n",
    "    # 初始化400维的零向量\n",
    "    feature_vector = np.zeros(400)\n",
    "    \n",
    "    # 将2-mer的频率映射到向量的对应位置\n",
    "    for two_mer, count in two_mer_counts.items():\n",
    "        if two_mer in two_mer_index:\n",
    "            # 计算频率而不是计数\n",
    "            frequency = count / total_two_mers\n",
    "            feature_vector[two_mer_index[two_mer]] = frequency\n",
    "            \n",
    "    # 将计算的特征向量保存到字典中\n",
    "    two_mer_dict[gene] = feature_vector\n",
    "\n",
    "# 打印前5个基因的2-mer特征查看\n",
    "for gene, feature_vector in list(two_mer_dict.items())[:5]:\n",
    "    print(f\"Gene: {gene}\")\n",
    "    print(f\"2-mer Feature Vector: {feature_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb3571",
   "metadata": {},
   "source": [
    "### AutoEncoder2 training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083fc81",
   "metadata": {},
   "source": [
    "#### Cross-Validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96a17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train file 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file 2:  48%|██████████▌           | 223/464 [18:27<19:23,  4.83s/it]"
     ]
    }
   ],
   "source": [
    "# cross-validation no batch in training dataloader - batch when read data - 6 epochs - test only calculate mse\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "zero_row = np.zeros((1, 400)) # define zero row to add row number from 4303 to 4304\n",
    "\n",
    "# Define the model\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(400, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.35),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.35),\n",
    "            torch.nn.Linear(128, 3),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 400),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "class Autoencoder2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder2, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4304, 3000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(3000, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(1000, 400),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(400, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 3000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(3000, 4304),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Check if CUDA device is available\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_file = os.path.join(os.getcwd(), \"ae2_crossval_training_log_test1.txt\")\n",
    "error_log_file = os.path.join(os.getcwd(), \"ae2_crossval_error_log_test1.txt\")\n",
    "\n",
    "# Function to load and update del_twogenes files in batches\n",
    "def load_and_update_del_twogenes(file_idx, batch_size):\n",
    "    pickle_filename = f'del_twogenes_{file_idx}.pkl'\n",
    "    \n",
    "    with open(pickle_filename, 'rb') as f:\n",
    "        del_twogenes = pickle.load(f)\n",
    "\n",
    "    # Split del_twogenes into batches of size batch_size\n",
    "    keys = list(del_twogenes.keys())\n",
    "    batches = [keys[i:i + batch_size] for i in range(0, len(keys), batch_size)]\n",
    "\n",
    "    # Process each batch separately\n",
    "    del_twogenes_batches = []\n",
    "    for batch in tqdm(batches, desc=f\"Processing file {file_idx}\"):\n",
    "        batch_data = {key: del_twogenes[key] for key in batch}\n",
    "        updated_batch = {key: random.sample(val, 4303) for key, val in batch_data.items()}\n",
    "        #updated_batch = {key: random.shuffle(val) for key, val in batch_data.items()}\n",
    "        del_twogenes_batches.append(updated_batch)\n",
    "        \n",
    "        #break # 1\n",
    "\n",
    "    return del_twogenes_batches\n",
    "\n",
    "# Redirect print to a file\n",
    "def log_print(message):\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(message)\n",
    "\n",
    "# Log error messages to a file\n",
    "def log_error(message):\n",
    "    with open(error_log_file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(f\"Error logged: {message}\")  # Debugging line to ensure errors are logged\n",
    "\n",
    "# Training function\n",
    "def train_autoencoder(train_data, model, criterion, optimizer, num_epochs=6):\n",
    "    model.train()\n",
    "\n",
    "    # Store training logs\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"Epoch,Train Loss MSE\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for data in tqdm(train_data, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            inputs = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Log results\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{epoch+1},{epoch_loss/len(train_data)}\\n\")\n",
    "\n",
    "        log_print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_data):.8f}\")\n",
    "\n",
    "# Evaluation function (MSE computation, since we're not using labels in unsupervised learning)\n",
    "def evaluate_mse(model, test_loader):\n",
    "    model.eval()\n",
    "    total_mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs = data  # Only use inputs for autoencoder (no labels)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate MSE between input and output\n",
    "            mse = nn.MSELoss()(outputs, inputs)  # Compare reconstructed output with original input\n",
    "            total_mse += mse.item()\n",
    "\n",
    "    return total_mse / len(test_loader)\n",
    "\n",
    "# Main function to perform 10-fold cross-validation\n",
    "def cross_validate():\n",
    "    #all_indices = list(range(1, 11))\n",
    "    #total_mse = []\n",
    "\n",
    "    for fold in range(1):\n",
    "        try:\n",
    "            # Prepare training and testing sets\n",
    "            train_files = [2,3,4,5,6,7,8,9,10]\n",
    "            test_file = 1\n",
    "\n",
    "            # Initialize the model1\n",
    "            model = Autoencoder().to(device) \n",
    "            model.load_state_dict(torch.load('ae1_all_data_training.pth'))  # 加载训练好的模型\n",
    "            model.eval()  # 切换到推理模式（关闭 dropout 等）\n",
    "            \n",
    "            # Initialize the model2, criterion, and optimizer\n",
    "            model2 = Autoencoder2().to(device)\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "            # Load and update training files (9 files)\n",
    "            for file_idx in train_files:\n",
    "                log_print(f\"Processing train file {file_idx}\")\n",
    "                del_twogenes_batches = load_and_update_del_twogenes(file_idx, batch_size=2000)\n",
    "                for batch_idx, batch in enumerate(del_twogenes_batches):\n",
    "                    \n",
    "                    train_data = []\n",
    "\n",
    "                    for key, val in batch.items():\n",
    "                        gene_features = np.array([two_mer_dict[gene] for gene in val])\n",
    "                        gene_features = np.vstack([gene_features, zero_row])                       \n",
    "                        inputs2 = torch.tensor(gene_features).to(device).float() \n",
    "                        encoded = model.encoder(inputs2)                       \n",
    "                        \n",
    "                        gene_features = encoded.cpu().detach().numpy().T # transpose otherwise T for 3-D will error                     \n",
    "                        \n",
    "                        # 如果所有元素都为0，保持不变 normalization step\n",
    "                        if np.all(gene_features == 0):\n",
    "                            normalized_gene_features = gene_features\n",
    "                        else:\n",
    "                            # 最小-最大归一化\n",
    "                            min_val = np.min(gene_features)\n",
    "                            max_val = np.max(gene_features)\n",
    "\n",
    "                            # 防止除零错误\n",
    "                            if max_val != min_val:\n",
    "                                normalized_gene_features = (gene_features - min_val) / (max_val - min_val)\n",
    "                            else:\n",
    "                                normalized_gene_features = gene_features  # 如果 min == max，保持原样                        \n",
    "                                             \n",
    "                        train_data.append(normalized_gene_features)\n",
    "                    # print(np.array(train_data).shape) = (batch_size, 3, 4304)\n",
    "                        \n",
    "                    # Convert train data to tensor and train the model\n",
    "                    train_data = torch.tensor(np.array(train_data), dtype=torch.float32) \n",
    "                    train_autoencoder(train_data, model2, criterion, optimizer) # change as model2 for train\n",
    "                    \n",
    "                    #break # 2\n",
    "\n",
    "            # Load and update test file (1 file)\n",
    "            log_print(f\"Processing test file {test_file}\")\n",
    "            del_twogenes_batches = load_and_update_del_twogenes(test_file, batch_size=2000)\n",
    "            fold_mse = []\n",
    "            for batch in del_twogenes_batches:\n",
    "                test_data = []\n",
    "                for key, val in batch.items():\n",
    "                    gene_features = np.array([two_mer_dict[gene] for gene in val])\n",
    "                    gene_features = np.vstack([gene_features, zero_row])                                         \n",
    "                    inputs2 = torch.tensor(gene_features).to(device).float() \n",
    "                    encoded = model.encoder(inputs2) # model1\n",
    "                    gene_features = encoded.cpu().detach().numpy().T \n",
    "                    \n",
    "                    # 如果所有元素都为0，保持不变\n",
    "                    if np.all(gene_features == 0):\n",
    "                        normalized_gene_features = gene_features\n",
    "                    else:\n",
    "                        # 最小-最大归一化\n",
    "                        min_val = np.min(gene_features)\n",
    "                        max_val = np.max(gene_features)\n",
    "\n",
    "                        # 防止除零错误\n",
    "                        if max_val != min_val:\n",
    "                            normalized_gene_features = (gene_features - min_val) / (max_val - min_val)\n",
    "                        else:\n",
    "                            normalized_gene_features = gene_features  # 如果 min == max，保持原样          \n",
    "                    \n",
    "                    test_data.append(normalized_gene_features)                    \n",
    "\n",
    "                # Convert test data to tensor\n",
    "                test_data = torch.tensor(np.array(test_data), dtype=torch.float32)\n",
    "\n",
    "                # Evaluate model on the test set\n",
    "                test_loader = DataLoader(test_data, batch_size=500) # In train, no additional batchsize in model\n",
    "                mse = evaluate_mse(model2, test_loader)\n",
    "                fold_mse.append(mse)\n",
    "                #total_mse.append(mse)\n",
    "\n",
    "            log_print(f\"Fold MSE: {np.mean(fold_mse):.8f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error in fold {fold+1}: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            log_error(error_message)\n",
    "\n",
    "    #log_print(f\"Average MSE over 10 folds: {np.mean(total_mse):.8f}\")\n",
    "\n",
    "# Start cross-validation\n",
    "cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b7aa0",
   "metadata": {},
   "source": [
    "#### Train all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a722b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time is 1745039603.1843216\n",
      "Processing train file 1\n"
     ]
    }
   ],
   "source": [
    "# train all data no batch in training dataloader - batch when read data - 6 epochs\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "zero_row = np.zeros((1, 400)) # define zero row to add row number from 4303 to 4304\n",
    "\n",
    "# Define the model\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(400, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.35),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.35),\n",
    "            torch.nn.Linear(128, 3),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 400),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "class Autoencoder2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder2, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4304, 3000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(3000, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(1000, 400),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(400, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 3000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(3000, 4304),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Check if CUDA device is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_file = os.path.join(os.getcwd(), \"ae2_all_data_training_log.txt\")\n",
    "error_log_file = os.path.join(os.getcwd(), \"ae2_all_data_error_log.txt\")\n",
    "\n",
    "# Function to load and update del_twogenes files in batches\n",
    "def load_and_update_del_twogenes(file_idx, batch_size):\n",
    "    pickle_filename = f'del_twogenes_whole_random_{file_idx}.pkl'   # read files\n",
    "    \n",
    "    with open(pickle_filename, 'rb') as f:\n",
    "        del_twogenes = pickle.load(f)\n",
    "\n",
    "    # Split del_twogenes into batches of size batch_size\n",
    "    keys = list(del_twogenes.keys())\n",
    "    batches = [keys[i:i + batch_size] for i in range(0, len(keys), batch_size)]\n",
    "\n",
    "    # Process each batch separately\n",
    "    del_twogenes_batches = []\n",
    "    for batch in tqdm(batches, desc=f\"Processing file {file_idx}\"):\n",
    "        batch_data = {key: del_twogenes[key] for key in batch}\n",
    "        updated_batch = {key: random.sample(val, 4303) for key, val in batch_data.items()}\n",
    "        #updated_batch = {key: random.shuffle(val) for key, val in batch_data.items()}\n",
    "        del_twogenes_batches.append(updated_batch)\n",
    "        \n",
    "        #break # 1\n",
    "\n",
    "    return del_twogenes_batches\n",
    "\n",
    "# Redirect print to a file\n",
    "def log_print(message):\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(message)\n",
    "\n",
    "# Log error messages to a file\n",
    "def log_error(message):\n",
    "    with open(error_log_file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(f\"Error logged: {message}\")  # Debugging line to ensure errors are logged\n",
    "\n",
    "# Training function\n",
    "def train_autoencoder(train_data, model, criterion, optimizer, num_epochs=6):\n",
    "    model.train()\n",
    "\n",
    "    # Store training logs\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"Epoch,Train Loss MSE\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for data in tqdm(train_data, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            inputs = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Log results\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{epoch+1},{epoch_loss/len(train_data)}\\n\")\n",
    "\n",
    "        log_print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_data):.8f}\")\n",
    "\n",
    "# Evaluation function (MSE computation, since we're not using labels in unsupervised learning)\n",
    "def evaluate_mse(model, test_loader):\n",
    "    model.eval()\n",
    "    total_mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs = data  # Only use inputs for autoencoder (no labels)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate MSE between input and output\n",
    "            mse = nn.MSELoss()(outputs, inputs)  # Compare reconstructed output with original input\n",
    "            total_mse += mse.item()\n",
    "\n",
    "    return total_mse / len(test_loader)\n",
    "\n",
    "def train_all():\n",
    "    all_indices = list(range(1, 11)) # files from 1-10\n",
    "\n",
    "    for fold in range(1): # loop one time\n",
    "        try:\n",
    "            # Prepare training and testing sets\n",
    "            train_files = all_indices\n",
    "            #test_file = fold + 1\n",
    "\n",
    "            # Initialize the model1\n",
    "            model = Autoencoder().to(device) \n",
    "            model.load_state_dict(torch.load('ae1_all_data_training.pth'))  # 加载训练好的模型\n",
    "            model.eval()  # 切换到推理模式（关闭 dropout 等）\n",
    "            \n",
    "            # Initialize the model2, criterion, and optimizer\n",
    "            model2 = Autoencoder2().to(device)\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "            # Load and update training files (10 files)\n",
    "            log_print(f\"start time is {time.time()}\")\n",
    "            \n",
    "            for file_idx in train_files:\n",
    "                log_print(f\"Processing train file {file_idx}\")\n",
    "                del_twogenes_batches = load_and_update_del_twogenes(file_idx, batch_size=2000)\n",
    "                for batch_idx, batch in enumerate(del_twogenes_batches):\n",
    "                    \n",
    "                    train_data = []\n",
    "\n",
    "                    for key, val in batch.items():\n",
    "                        gene_features = np.array([two_mer_dict[gene] for gene in val])\n",
    "                        gene_features = np.vstack([gene_features, zero_row])                       \n",
    "                        inputs2 = torch.tensor(gene_features).to(device).float() \n",
    "                        encoded = model.encoder(inputs2)                    \n",
    "                        gene_features = encoded.cpu().detach().numpy().T # transpose otherwise T for 3-D will error                     \n",
    "                        \n",
    "                        # 如果所有元素都为0，保持不变\n",
    "                        if np.all(gene_features == 0):\n",
    "                            normalized_gene_features = gene_features\n",
    "                        else:\n",
    "                            # 最小-最大归一化\n",
    "                            min_val = np.min(gene_features)\n",
    "                            max_val = np.max(gene_features)\n",
    "\n",
    "                            # 防止除零错误\n",
    "                            if max_val != min_val:\n",
    "                                normalized_gene_features = (gene_features - min_val) / (max_val - min_val)\n",
    "                            else:\n",
    "                                normalized_gene_features = gene_features  # 如果 min == max，保持原样\n",
    "\n",
    "                        train_data.append(normalized_gene_features)\n",
    "                    # print(np.array(train_data).shape) = (batch_size, 3, 4304)\n",
    "                        \n",
    "                    # Convert train data to tensor and train the model\n",
    "                    train_data = torch.tensor(np.array(train_data), dtype=torch.float32) \n",
    "                    train_autoencoder(train_data, model2, criterion, optimizer) # change as model2 for train\n",
    "                    \n",
    "                    #break # 2\n",
    "\n",
    "            # Save the model parameters after each fold\n",
    "            torch.save(model2.state_dict(), f'ae2_all_data_training.pth') #save model2 state\n",
    "            log_print(f\"end time is {time.time()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error in fold {fold+1}: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            log_error(error_message)\n",
    "\n",
    "# train_all\n",
    "train_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020d551",
   "metadata": {},
   "source": [
    "111 hours to train all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc89b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorchh] *",
   "language": "python",
   "name": "conda-env-.conda-pytorchh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
