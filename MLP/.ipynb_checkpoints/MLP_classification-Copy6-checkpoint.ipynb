{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3fde04",
   "metadata": {},
   "source": [
    "### Get the E.coli Genome list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c24108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene: b0001, Sequence: MKRISTTITTTITITTGNGAG...\n",
      "Gene: b0002, Sequence: MRVLKFGGTSVANAERFLRVADILESNARQ...\n",
      "Gene: b0003, Sequence: MVKVYAPASSANMSVGFDVLGAAVTPVDGA...\n",
      "Gene: b0004, Sequence: MKLYNLKDHNEQVSFAQAVTQGLGKNQGLF...\n",
      "Gene: b0005, Sequence: MKKMQSIVLALSLVLVAPMAAQAAEITLVP...\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# 定义读取FASTA文件并返回基因-序列字典的函数\n",
    "def read_fasta(file_path):\n",
    "    gene_sequence_dict = {}\n",
    "    \n",
    "    # 使用SeqIO解析FASTA文件\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        # 获取基因名称（FASTA文件的标头部分）\n",
    "        gene_name = record.id\n",
    "        # 获取蛋白质序列（序列部分）\n",
    "        sequence = str(record.seq)\n",
    "        # 将基因-序列的键值对添加到字典中\n",
    "        gene_sequence_dict[gene_name] = sequence\n",
    "\n",
    "    return gene_sequence_dict\n",
    "\n",
    "# 读取FASTA文件并生成字典\n",
    "file_path = \"/data1/xpgeng/cross_pathogen/autoencoder/E.coli.tag_seq.fasta\"  # 请替换为你的FASTA文件路径\n",
    "gene_sequence_dict = read_fasta(file_path)\n",
    "\n",
    "# 打印字典的前几个项以确认\n",
    "for gene, sequence in list(gene_sequence_dict.items())[:5]:\n",
    "    print(f\"Gene: {gene}, Sequence: {sequence[:30]}...\")  # 只打印前30个氨基酸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9f00e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genes = set(list(gene_sequence_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9345614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene: b0001\n",
      "2-mer Feature Vector: [0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.05\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.05 0.15 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.05 0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.05 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.05 0.   0.15 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "Gene: b0002\n",
      "2-mer Feature Vector: [0.01587302 0.001221   0.00854701 0.01098901 0.002442   0.00854701\n",
      " 0.         0.003663   0.00610501 0.00732601 0.00610501 0.00610501\n",
      " 0.002442   0.00732601 0.01343101 0.002442   0.003663   0.004884\n",
      " 0.         0.         0.         0.001221   0.001221   0.001221\n",
      " 0.         0.001221   0.         0.         0.         0.002442\n",
      " 0.         0.         0.001221   0.         0.002442   0.\n",
      " 0.001221   0.002442   0.         0.         0.004884   0.003663\n",
      " 0.003663   0.00732601 0.001221   0.004884   0.001221   0.003663\n",
      " 0.         0.004884   0.         0.         0.004884   0.003663\n",
      " 0.         0.001221   0.001221   0.004884   0.         0.002442\n",
      " 0.003663   0.         0.002442   0.002442   0.003663   0.00610501\n",
      " 0.         0.002442   0.00610501 0.01098901 0.001221   0.003663\n",
      " 0.004884   0.001221   0.00610501 0.003663   0.001221   0.002442\n",
      " 0.         0.002442   0.00732601 0.001221   0.         0.\n",
      " 0.001221   0.003663   0.001221   0.         0.001221   0.003663\n",
      " 0.001221   0.003663   0.001221   0.001221   0.         0.003663\n",
      " 0.001221   0.002442   0.         0.002442   0.00610501 0.\n",
      " 0.003663   0.003663   0.003663   0.003663   0.002442   0.004884\n",
      " 0.002442   0.004884   0.00732601 0.00610501 0.001221   0.003663\n",
      " 0.003663   0.003663   0.002442   0.01098901 0.         0.002442\n",
      " 0.         0.         0.         0.         0.         0.002442\n",
      " 0.         0.001221   0.         0.002442   0.001221   0.001221\n",
      " 0.001221   0.002442   0.         0.         0.         0.002442\n",
      " 0.         0.002442   0.00732601 0.001221   0.003663   0.00610501\n",
      " 0.002442   0.002442   0.         0.001221   0.002442   0.003663\n",
      " 0.001221   0.002442   0.002442   0.         0.001221   0.01098901\n",
      " 0.003663   0.002442   0.001221   0.         0.003663   0.\n",
      " 0.         0.003663   0.004884   0.003663   0.002442   0.002442\n",
      " 0.001221   0.004884   0.001221   0.003663   0.         0.\n",
      " 0.001221   0.002442   0.002442   0.003663   0.         0.\n",
      " 0.01465201 0.         0.002442   0.00976801 0.003663   0.004884\n",
      " 0.002442   0.00732601 0.00732601 0.01343101 0.002442   0.004884\n",
      " 0.00610501 0.001221   0.01098901 0.00854701 0.002442   0.004884\n",
      " 0.         0.001221   0.00610501 0.         0.002442   0.001221\n",
      " 0.001221   0.001221   0.         0.001221   0.002442   0.001221\n",
      " 0.         0.         0.         0.001221   0.002442   0.004884\n",
      " 0.         0.002442   0.         0.         0.00732601 0.\n",
      " 0.003663   0.001221   0.         0.002442   0.001221   0.004884\n",
      " 0.002442   0.00732601 0.001221   0.002442   0.002442   0.\n",
      " 0.         0.001221   0.003663   0.003663   0.001221   0.\n",
      " 0.003663   0.001221   0.003663   0.         0.001221   0.003663\n",
      " 0.         0.001221   0.         0.00610501 0.         0.002442\n",
      " 0.         0.002442   0.003663   0.         0.         0.00610501\n",
      " 0.         0.         0.003663   0.001221   0.001221   0.004884\n",
      " 0.001221   0.002442   0.         0.002442   0.         0.004884\n",
      " 0.001221   0.001221   0.002442   0.001221   0.         0.003663\n",
      " 0.         0.003663   0.         0.001221   0.00610501 0.\n",
      " 0.003663   0.004884   0.001221   0.004884   0.         0.004884\n",
      " 0.002442   0.004884   0.         0.001221   0.         0.003663\n",
      " 0.002442   0.001221   0.003663   0.00854701 0.         0.002442\n",
      " 0.003663   0.         0.003663   0.003663   0.002442   0.00610501\n",
      " 0.001221   0.004884   0.001221   0.002442   0.002442   0.002442\n",
      " 0.         0.002442   0.004884   0.00610501 0.002442   0.00610501\n",
      " 0.002442   0.003663   0.002442   0.001221   0.002442   0.002442\n",
      " 0.001221   0.004884   0.001221   0.002442   0.         0.004884\n",
      " 0.         0.003663   0.002442   0.001221   0.001221   0.003663\n",
      " 0.002442   0.003663   0.         0.         0.01221001 0.002442\n",
      " 0.00610501 0.001221   0.003663   0.00854701 0.001221   0.00610501\n",
      " 0.004884   0.01465201 0.         0.001221   0.002442   0.\n",
      " 0.002442   0.001221   0.00610501 0.00732601 0.         0.001221\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.001221   0.001221   0.         0.\n",
      " 0.         0.001221   0.         0.         0.001221   0.\n",
      " 0.         0.         0.002442   0.         0.001221   0.\n",
      " 0.001221   0.001221   0.002442   0.001221   0.         0.002442\n",
      " 0.         0.         0.         0.002442   0.         0.003663\n",
      " 0.002442   0.001221   0.         0.002442  ]\n",
      "Gene: b0003\n",
      "2-mer Feature Vector: [0.01294498 0.00647249 0.00647249 0.01294498 0.         0.00647249\n",
      " 0.00323625 0.00323625 0.00323625 0.01294498 0.00647249 0.00323625\n",
      " 0.00647249 0.00647249 0.00970874 0.00647249 0.         0.00970874\n",
      " 0.         0.00323625 0.         0.         0.00323625 0.\n",
      " 0.00323625 0.00323625 0.         0.00323625 0.         0.\n",
      " 0.         0.         0.         0.00323625 0.00323625 0.00323625\n",
      " 0.         0.         0.00323625 0.00323625 0.         0.00323625\n",
      " 0.         0.00323625 0.         0.00323625 0.         0.00323625\n",
      " 0.00647249 0.         0.         0.00323625 0.         0.\n",
      " 0.         0.         0.00647249 0.00970874 0.00323625 0.\n",
      " 0.00647249 0.         0.         0.00323625 0.         0.00647249\n",
      " 0.00323625 0.00323625 0.00323625 0.00970874 0.         0.00970874\n",
      " 0.00647249 0.         0.00647249 0.         0.00647249 0.\n",
      " 0.00323625 0.         0.00647249 0.00323625 0.00647249 0.\n",
      " 0.         0.         0.         0.00323625 0.         0.00323625\n",
      " 0.         0.         0.         0.         0.00323625 0.00323625\n",
      " 0.         0.00323625 0.         0.         0.01294498 0.\n",
      " 0.00323625 0.00323625 0.01618123 0.00323625 0.         0.00647249\n",
      " 0.00970874 0.00323625 0.00323625 0.         0.00323625 0.\n",
      " 0.00970874 0.01294498 0.         0.         0.         0.\n",
      " 0.00323625 0.00323625 0.         0.         0.         0.00323625\n",
      " 0.         0.00323625 0.         0.00323625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00323625 0.00647249 0.00323625 0.         0.00323625\n",
      " 0.         0.00647249 0.00647249 0.00323625 0.00323625 0.00323625\n",
      " 0.         0.         0.00323625 0.         0.         0.00970874\n",
      " 0.         0.00323625 0.         0.         0.         0.\n",
      " 0.00323625 0.         0.         0.         0.         0.\n",
      " 0.         0.00647249 0.         0.00647249 0.00647249 0.00323625\n",
      " 0.         0.         0.         0.00647249 0.         0.\n",
      " 0.01294498 0.00323625 0.00323625 0.00970874 0.00323625 0.02265372\n",
      " 0.         0.         0.         0.00970874 0.01294498 0.00647249\n",
      " 0.00970874 0.00323625 0.         0.         0.         0.\n",
      " 0.00323625 0.         0.00323625 0.         0.         0.\n",
      " 0.         0.00323625 0.         0.00323625 0.00323625 0.\n",
      " 0.         0.00323625 0.00323625 0.00323625 0.         0.00323625\n",
      " 0.00323625 0.00323625 0.         0.         0.         0.\n",
      " 0.00647249 0.00323625 0.         0.         0.         0.00323625\n",
      " 0.         0.00323625 0.00647249 0.00323625 0.         0.00323625\n",
      " 0.         0.         0.         0.00323625 0.         0.00323625\n",
      " 0.00647249 0.00323625 0.         0.00647249 0.         0.00970874\n",
      " 0.         0.00323625 0.         0.00323625 0.         0.\n",
      " 0.         0.         0.00323625 0.00323625 0.00323625 0.00647249\n",
      " 0.         0.00323625 0.00647249 0.00323625 0.00323625 0.00647249\n",
      " 0.         0.         0.         0.00323625 0.         0.00323625\n",
      " 0.         0.00323625 0.00323625 0.00323625 0.00323625 0.\n",
      " 0.         0.00323625 0.         0.00323625 0.00323625 0.\n",
      " 0.         0.00647249 0.00647249 0.         0.00323625 0.00323625\n",
      " 0.         0.00970874 0.         0.         0.         0.01294498\n",
      " 0.00323625 0.         0.         0.00647249 0.         0.\n",
      " 0.00647249 0.         0.         0.00323625 0.         0.01618123\n",
      " 0.         0.00323625 0.         0.00323625 0.         0.\n",
      " 0.         0.00323625 0.00323625 0.00647249 0.00323625 0.00647249\n",
      " 0.         0.         0.00970874 0.         0.         0.\n",
      " 0.00323625 0.         0.         0.         0.         0.00647249\n",
      " 0.         0.         0.00323625 0.         0.00323625 0.\n",
      " 0.         0.00323625 0.         0.         0.01941748 0.\n",
      " 0.00323625 0.00323625 0.         0.00323625 0.00323625 0.00323625\n",
      " 0.00323625 0.00970874 0.         0.         0.00323625 0.\n",
      " 0.         0.00323625 0.00647249 0.00647249 0.         0.00647249\n",
      " 0.         0.         0.         0.00323625 0.         0.\n",
      " 0.         0.         0.         0.00647249 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00323625\n",
      " 0.         0.         0.00323625 0.         0.00323625 0.\n",
      " 0.         0.         0.         0.         0.         0.00323625\n",
      " 0.         0.         0.00323625 0.00323625 0.00647249 0.00323625\n",
      " 0.         0.         0.         0.        ]\n",
      "Gene: b0004\n",
      "2-mer Feature Vector: [0.01405152 0.00234192 0.00468384 0.00234192 0.01405152 0.00468384\n",
      " 0.00468384 0.00468384 0.00702576 0.00936768 0.00234192 0.00468384\n",
      " 0.00234192 0.00936768 0.         0.         0.00936768 0.0117096\n",
      " 0.         0.00234192 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00234192\n",
      " 0.         0.         0.         0.00234192 0.         0.\n",
      " 0.00234192 0.         0.         0.00234192 0.00234192 0.\n",
      " 0.00468384 0.00936768 0.00936768 0.00468384 0.00234192 0.\n",
      " 0.00234192 0.00936768 0.         0.         0.         0.00234192\n",
      " 0.         0.         0.00468384 0.00468384 0.         0.\n",
      " 0.00468384 0.         0.         0.00702576 0.00234192 0.\n",
      " 0.         0.00702576 0.00234192 0.01639344 0.00234192 0.\n",
      " 0.00234192 0.00234192 0.00468384 0.00468384 0.00936768 0.\n",
      " 0.         0.00234192 0.00702576 0.00234192 0.00468384 0.00234192\n",
      " 0.00234192 0.00468384 0.00234192 0.00468384 0.00468384 0.00468384\n",
      " 0.00234192 0.         0.00468384 0.         0.00234192 0.00234192\n",
      " 0.         0.00234192 0.         0.00234192 0.00234192 0.00234192\n",
      " 0.0117096  0.00468384 0.         0.00468384 0.         0.\n",
      " 0.00468384 0.01639344 0.         0.00468384 0.00234192 0.00234192\n",
      " 0.00234192 0.         0.00234192 0.         0.         0.00468384\n",
      " 0.00468384 0.         0.00468384 0.         0.         0.00234192\n",
      " 0.         0.00234192 0.         0.         0.         0.00468384\n",
      " 0.00234192 0.00234192 0.         0.         0.         0.\n",
      " 0.         0.         0.00468384 0.00234192 0.00468384 0.00234192\n",
      " 0.         0.00234192 0.         0.         0.         0.0117096\n",
      " 0.         0.00234192 0.00234192 0.         0.         0.00468384\n",
      " 0.         0.         0.00234192 0.         0.00234192 0.\n",
      " 0.00468384 0.00936768 0.00234192 0.         0.         0.00702576\n",
      " 0.         0.00936768 0.         0.00234192 0.00234192 0.00234192\n",
      " 0.00234192 0.00234192 0.         0.00468384 0.         0.\n",
      " 0.00936768 0.         0.00468384 0.00468384 0.0117096  0.01873536\n",
      " 0.00234192 0.         0.0117096  0.00702576 0.00234192 0.00468384\n",
      " 0.01639344 0.00234192 0.00468384 0.00702576 0.00936768 0.00468384\n",
      " 0.         0.00468384 0.00234192 0.         0.00234192 0.\n",
      " 0.         0.         0.         0.         0.00234192 0.00468384\n",
      " 0.00234192 0.00234192 0.         0.         0.00234192 0.\n",
      " 0.         0.         0.         0.         0.00234192 0.\n",
      " 0.00234192 0.00234192 0.00234192 0.         0.00234192 0.00468384\n",
      " 0.         0.00468384 0.         0.00234192 0.00234192 0.00468384\n",
      " 0.         0.00468384 0.         0.00702576 0.00234192 0.\n",
      " 0.00702576 0.         0.         0.00234192 0.         0.00234192\n",
      " 0.00468384 0.         0.00468384 0.00468384 0.         0.00468384\n",
      " 0.         0.00468384 0.00702576 0.00234192 0.00234192 0.00702576\n",
      " 0.         0.         0.00936768 0.         0.         0.00702576\n",
      " 0.         0.00468384 0.         0.00234192 0.         0.00936768\n",
      " 0.00234192 0.         0.00234192 0.00234192 0.         0.\n",
      " 0.00234192 0.00234192 0.00234192 0.         0.00702576 0.\n",
      " 0.00234192 0.00234192 0.00702576 0.00234192 0.         0.\n",
      " 0.00468384 0.00234192 0.         0.00234192 0.         0.\n",
      " 0.00234192 0.00234192 0.         0.00468384 0.         0.\n",
      " 0.00702576 0.         0.00234192 0.00234192 0.00234192 0.00468384\n",
      " 0.00234192 0.00234192 0.         0.00468384 0.         0.00234192\n",
      " 0.00468384 0.00234192 0.00234192 0.         0.         0.00468384\n",
      " 0.         0.         0.00702576 0.         0.         0.00234192\n",
      " 0.         0.00234192 0.00234192 0.00234192 0.         0.00936768\n",
      " 0.00234192 0.00234192 0.         0.00702576 0.00468384 0.00468384\n",
      " 0.00234192 0.00468384 0.         0.         0.01405152 0.\n",
      " 0.00234192 0.00702576 0.         0.00234192 0.         0.00234192\n",
      " 0.00702576 0.         0.         0.00234192 0.00468384 0.\n",
      " 0.00234192 0.00702576 0.00702576 0.00468384 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00234192 0.00234192 0.         0.00234192 0.         0.\n",
      " 0.         0.         0.00234192 0.         0.         0.\n",
      " 0.00234192 0.00468384 0.         0.         0.         0.\n",
      " 0.         0.00234192 0.00234192 0.         0.00234192 0.\n",
      " 0.00234192 0.         0.         0.00234192]\n",
      "Gene: b0005\n",
      "2-mer Feature Vector: [0.02061856 0.         0.         0.01030928 0.         0.\n",
      " 0.         0.         0.         0.01030928 0.         0.\n",
      " 0.02061856 0.01030928 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01030928 0.02061856 0.\n",
      " 0.         0.         0.         0.01030928 0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01030928 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.         0.         0.02061856 0.02061856 0.\n",
      " 0.01030928 0.         0.         0.01030928 0.02061856 0.\n",
      " 0.         0.         0.         0.         0.01030928 0.01030928\n",
      " 0.         0.         0.01030928 0.         0.         0.04123711\n",
      " 0.03092784 0.         0.01030928 0.01030928 0.         0.\n",
      " 0.         0.         0.01030928 0.         0.         0.\n",
      " 0.01030928 0.01030928 0.         0.         0.         0.\n",
      " 0.         0.01030928 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.01030928 0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.         0.01030928 0.\n",
      " 0.02061856 0.01030928 0.01030928 0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.\n",
      " 0.         0.01030928 0.         0.01030928 0.         0.03092784\n",
      " 0.         0.         0.01030928 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.01030928 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02061856 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01030928\n",
      " 0.01030928 0.         0.         0.         0.01030928 0.\n",
      " 0.04123711 0.         0.01030928 0.01030928 0.         0.\n",
      " 0.         0.         0.01030928 0.         0.         0.\n",
      " 0.         0.         0.01030928 0.01030928 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02061856 0.         0.         0.02061856 0.01030928 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01030928 0.         0.01030928 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01030928 0.02061856 0.         0.         0.01030928 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01030928 0.         0.         0.\n",
      " 0.01030928 0.         0.01030928 0.         0.         0.\n",
      " 0.         0.         0.02061856 0.         0.         0.\n",
      " 0.01030928 0.         0.         0.         0.         0.01030928\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01030928 0.01030928]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 定义所有可能的2-mer组合\n",
    "standard_amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "all_2mers = [a + b for a in standard_amino_acids for b in standard_amino_acids]\n",
    "two_mer_index = {two_mer: idx for idx, two_mer in enumerate(all_2mers)}\n",
    "\n",
    "# 生成基因2-mer特征字典\n",
    "two_mer_dict = {}\n",
    "\n",
    "# 遍历每个基因和序列\n",
    "for gene, sequence in gene_sequence_dict.items():\n",
    "    # 清洗序列，移除非标准氨基酸字符\n",
    "    sequence = ''.join([aa for aa in sequence if aa in standard_amino_acids])\n",
    "    \n",
    "    # 计算2-mer出现次数\n",
    "    two_mer_counts = Counter([sequence[i:i+2] for i in range(len(sequence)-1)])\n",
    "    \n",
    "    # 计算2-mer的总数\n",
    "    total_two_mers = sum(two_mer_counts.values())\n",
    "    \n",
    "    # 初始化400维的零向量\n",
    "    feature_vector = np.zeros(400)\n",
    "    \n",
    "    # 将2-mer的频率映射到向量的对应位置\n",
    "    for two_mer, count in two_mer_counts.items():\n",
    "        if two_mer in two_mer_index:\n",
    "            # 计算频率而不是计数\n",
    "            frequency = count / total_two_mers\n",
    "            feature_vector[two_mer_index[two_mer]] = frequency\n",
    "            \n",
    "    # 将计算的特征向量保存到字典中\n",
    "    two_mer_dict[gene] = feature_vector\n",
    "\n",
    "# 打印前5个基因的2-mer特征查看\n",
    "for gene, feature_vector in list(two_mer_dict.items())[:5]:\n",
    "    print(f\"Gene: {gene}\")\n",
    "    print(f\"2-mer Feature Vector: {feature_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3bed2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4305"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(two_mer_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13d5fa",
   "metadata": {},
   "source": [
    "### Define autoencoder to process genome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a31c6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(400, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.35),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.35),\n",
    "            torch.nn.Linear(128, 3),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 400),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "class Autoencoder2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder2, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4304, 3000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(3000, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(1000, 400),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(400, 1000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1000, 3000),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(3000, 4304),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Load the models once\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Autoencoder().to(device)\n",
    "model.load_state_dict(torch.load('/data1/xpgeng/cross_pathogen/autoencoder/ae1_all_data_training.pth'))\n",
    "model.eval()\n",
    "\n",
    "model2 = Autoencoder2().to(device)\n",
    "model2.load_state_dict(torch.load('/data1/xpgeng/cross_pathogen/autoencoder/ae2_all_data_training.pth'))\n",
    "model2.eval()\n",
    "\n",
    "def ae1_2(three_genes):\n",
    "    rest_genes = list(all_genes - three_genes)\n",
    "    inputs = np.vstack([two_mer_dict[gene] for gene in rest_genes]).astype(np.float32)\n",
    "\n",
    "    # Create two rows of zeros and append them to the inputs\n",
    "    zeros_400 = np.zeros((2, 400), dtype=np.float32)\n",
    "    inputs = np.vstack([inputs, zeros_400])\n",
    "\n",
    "    inputs = torch.tensor(inputs).to(device)\n",
    "    inputs = model.encoder(inputs)  # autoencoder1\n",
    "    inputs = inputs.cpu().detach().numpy().T  # transpose for autoencoder2 input\n",
    "    inputs = torch.tensor(inputs).to(device)\n",
    "    outputs = model2.encoder(inputs)  # autoencoder2 [3, 400] shape\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cdf78d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-15.5663, -17.6825,  16.1380,  ...,   9.6880,  35.6039,  -7.4324],\n",
       "        [-11.3250, -14.5221,  12.7913,  ...,   7.4453,  27.4692,  -5.9294],\n",
       "        [ -9.1131, -10.2462,   9.7529,  ...,   6.0358,  21.2655,  -4.7610]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae1_2({'b4356','b4358','b4366'})[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f4e03",
   "metadata": {},
   "source": [
    "### MLP model 10 fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d395d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Check if CUDA device is available\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_file = os.path.join(os.getcwd(), \"mlp_cross_training_log.txt\") ###########################\n",
    "error_log_file = os.path.join(os.getcwd(), \"mlp_cross_error_log.txt\") ###########################\n",
    "\n",
    "# Redirect print to a file\n",
    "def log_print(message):\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(message)\n",
    "\n",
    "# Log error messages to a file\n",
    "def log_error(message):\n",
    "    with open(error_log_file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(f\"Error logged: {message}\")  # Debugging line to ensure errors are logged\n",
    "    \n",
    "# Function to load and update del_twogenes files in batches\n",
    "def load_three_genes(file_idx, batch_size):\n",
    "    file_path = f'/data1/xpgeng/cross_pathogen/FBA/iML1515_parts/iML1515-{file_idx}.csv' ###########################\n",
    "    #file_path = f'/data1/xpgeng/cross_pathogen/MLP/iML1515-{file_idx}.csv' ###########################\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # Split file into batches of size batch_size\n",
    "    batches = [df.iloc[i:i + batch_size].values for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    return batches  # each batch contains three genes and a label\n",
    "\n",
    "# Function to compute AUC\n",
    "def compute_auc(predicted, labels):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    return roc_auc_score(labels.cpu(), predicted.cpu())\n",
    "\n",
    "# Define MLP model class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=2400, hidden_size1=512, hidden_size2=256, output_size=1, dropout_rate=0.1):  #######\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "def train_mlp(train_data, train_labels, model, criterion, optimizer, num_epochs=4):  \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_data)\n",
    "    loss = criterion(outputs.view(-1), train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    log_print(f\"Loss: {epoch_loss / len(train_data):.6f}\")  # Average loss over the epoch\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Fold 6 - Batch 1:  30%|██▍     | 18204/60000 [09:10<32:39, 21.33it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# 假设 load_three_genes, two_mer_dict, 和 ae1_2 函数都是有效且已定义的\n",
    "# two_mer_dict[gene] 返回基因的 400 维向量\n",
    "# ae1_2({gene1, gene2, gene3}) 返回 (3, 400) 维的张量\n",
    "\n",
    "def process_and_save(file_idx):\n",
    "    data = load_three_genes(file_idx, batch_size=60000)  # 读取文件，按 batch_size 分批\n",
    "\n",
    "    for batch_idx, batch in enumerate(data):\n",
    "        # 检查文件是否已经存在\n",
    "        output_file = os.path.join(\"/data2/xpgeng/iML1515_MLP/\", f\"{file_idx}_{batch_idx+1}.pkl\")\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping already processed file {output_file}\")\n",
    "            continue  # 如果文件存在，跳过此次循环\n",
    "\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for row in tqdm(batch, desc=f\"Processing Fold {file_idx} - Batch {batch_idx+1}\"):\n",
    "            # 获取 three_gene_features 并进行归一化\n",
    "            three_gene_features = np.array([two_mer_dict[gene] for gene in [row[0], row[1], row[2]]])\n",
    "            three_gene_features = three_gene_features.flatten()\n",
    "            three_gene_features = (three_gene_features - np.mean(three_gene_features)) / (np.std(three_gene_features) + 1e-8)  # 避免除以 0\n",
    "                        \n",
    "            rest_gene_features = ae1_2({row[0], row[1], row[2]}).detach().cpu().numpy() \n",
    "            rest_gene_features = rest_gene_features.flatten()\n",
    "            rest_gene_features = (rest_gene_features - np.mean(rest_gene_features)) / (np.std(rest_gene_features) + 1e-8)  # 归一化，避免除 0\n",
    "\n",
    "            normalized_features = np.concatenate([three_gene_features, rest_gene_features])\n",
    "\n",
    "            batch_data.append(normalized_features)\n",
    "            batch_labels.append(row[3])  # 最后一列是 Label\n",
    "\n",
    "        # 存储为 pickle 文件\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pickle.dump((np.array(batch_data), np.array(batch_labels)), f)\n",
    "\n",
    "        print(f\"Saved {output_file}\")\n",
    "\n",
    "# 例如，处理 file_idx 为 2 的数据\n",
    "process_and_save(6)\n",
    "# 如果要处理多个文件，可以循环调用 process_and_save 函数，例如从 1 到 10\n",
    "# for file_idx in range(1, 11):\n",
    "#     process_and_save(file_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47b97582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 1739344842.6392941\n",
      "Loading train file 2\n",
      "Loss: 0.003247\n",
      "Loss: 0.002686\n",
      "Loss: 0.002959\n",
      "Loss: 0.002530\n",
      "Loss: 0.002949\n",
      "Loss: 0.002661\n",
      "Loss: 0.002951\n",
      "Loss: 0.002635\n",
      "Loss: 0.002785\n",
      "Loss: 0.002670\n",
      "Loss: 0.002780\n",
      "Loss: 0.003170\n",
      "Loss: 0.002836\n",
      "Loss: 0.002442\n",
      "Loss: 0.002737\n",
      "Loss: 0.003037\n",
      "Loss: 0.002634\n",
      "Loss: 0.002784\n",
      "Loss: 0.002498\n",
      "Loss: 0.002814\n",
      "Loss: 0.002719\n",
      "Loss: 0.002647\n",
      "Loss: 0.002499\n",
      "Loss: 0.002525\n",
      "Loss: 0.002657\n",
      "Loss: 0.002233\n",
      "Loss: 0.002803\n",
      "Loss: 0.002543\n",
      "Loss: 0.002326\n",
      "Loss: 0.002469\n",
      "Loss: 0.002782\n",
      "Loss: 0.002853\n",
      "Loss: 0.002654\n",
      "Loss: 0.002428\n",
      "Loss: 0.002527\n",
      "Loss: 0.002276\n",
      "Loss: 0.002168\n",
      "Loss: 0.002717\n",
      "Loss: 0.002312\n",
      "Loss: 0.002533\n",
      "Loss: 0.002361\n",
      "Loss: 0.002698\n",
      "Loss: 0.002530\n",
      "Loss: 0.002332\n",
      "Loss: 0.003047\n",
      "Loss: 0.002234\n",
      "Loss: 0.002373\n",
      "Loss: 0.002374\n",
      "Loss: 0.002591\n",
      "Loss: 0.002288\n",
      "Loss: 0.002297\n",
      "Loss: 0.002404\n",
      "Loss: 0.002265\n",
      "Loss: 0.002309\n",
      "Loss: 0.002366\n",
      "Loss: 0.002647\n",
      "Loss: 0.002394\n",
      "Loss: 0.002410\n",
      "Loss: 0.002529\n",
      "Loss: 0.002268\n",
      "Loss: 0.002339\n",
      "Loss: 0.002690\n",
      "Loss: 0.002405\n",
      "Loss: 0.002359\n",
      "Loss: 0.002521\n",
      "Loss: 0.002245\n",
      "Loss: 0.002495\n",
      "Loss: 0.002155\n",
      "Loss: 0.002124\n",
      "Loss: 0.002361\n",
      "Loss: 0.002332\n",
      "Loss: 0.002380\n",
      "Loss: 0.002034\n",
      "Loss: 0.002091\n",
      "Loss: 0.002177\n",
      "Loss: 0.002665\n",
      "Loss: 0.002433\n",
      "Loss: 0.002306\n",
      "Loss: 0.002358\n",
      "Loss: 0.002159\n",
      "Loss: 0.002424\n",
      "Loss: 0.002698\n",
      "Loss: 0.002286\n",
      "Loss: 0.002112\n",
      "Loss: 0.002426\n",
      "Loss: 0.002014\n",
      "Loss: 0.002389\n",
      "Loss: 0.002414\n",
      "Loss: 0.001889\n",
      "Loss: 0.002164\n",
      "Loss: 0.002288\n",
      "Loss: 0.002159\n",
      "Loss: 0.001935\n",
      "Loss: 0.002369\n",
      "Loss: 0.002258\n",
      "Loss: 0.002068\n",
      "Loss: 0.002043\n",
      "Loss: 0.002680\n",
      "Loss: 0.002132\n",
      "Loss: 0.002159\n",
      "Loss: 0.002091\n",
      "Loss: 0.002053\n",
      "Loss: 0.002373\n",
      "Loss: 0.001664\n",
      "Loss: 0.001980\n",
      "Loss: 0.002033\n",
      "Loss: 0.002022\n",
      "Loss: 0.001875\n",
      "Loss: 0.002005\n",
      "Loss: 0.002252\n",
      "Loss: 0.001837\n",
      "Loss: 0.001870\n",
      "Loss: 0.001722\n",
      "Loss: 0.002152\n",
      "Loss: 0.002257\n",
      "Loss: 0.001872\n",
      "Loss: 0.002112\n",
      "Loss: 0.002492\n",
      "Loss: 0.001853\n",
      "Loss: 0.002485\n",
      "Loss: 0.002148\n",
      "Loss: 0.001841\n",
      "Loss: 0.002470\n",
      "Loss: 0.002167\n",
      "Loss: 0.002037\n",
      "Loss: 0.001823\n",
      "Loss: 0.001775\n",
      "Loss: 0.002215\n",
      "Loss: 0.001908\n",
      "Loss: 0.001828\n",
      "Loss: 0.002175\n",
      "Loss: 0.002184\n",
      "Loss: 0.001686\n",
      "Loss: 0.002237\n",
      "Loss: 0.002097\n",
      "Loss: 0.001878\n",
      "Loss: 0.002032\n",
      "Loss: 0.002080\n",
      "Loss: 0.001966\n",
      "Loss: 0.002170\n",
      "Loss: 0.002227\n",
      "Loss: 0.001706\n",
      "Loss: 0.001827\n",
      "Loss: 0.001893\n",
      "Loss: 0.001832\n",
      "Loss: 0.001495\n",
      "Loss: 0.002145\n",
      "Loss: 0.001952\n",
      "Loss: 0.001980\n",
      "Loss: 0.001699\n",
      "Loss: 0.001469\n",
      "Loss: 0.002098\n",
      "Loss: 0.002081\n",
      "Loss: 0.002304\n",
      "Loss: 0.002345\n",
      "Loss: 0.001694\n",
      "Loss: 0.001831\n",
      "Loss: 0.002611\n",
      "Loss: 0.001565\n",
      "Loss: 0.002073\n",
      "Loss: 0.002342\n",
      "Loss: 0.002235\n",
      "Loss: 0.001824\n",
      "Loss: 0.001922\n",
      "Loss: 0.002173\n",
      "Loss: 0.002132\n",
      "Loss: 0.001895\n",
      "Loss: 0.002076\n",
      "Loss: 0.002070\n",
      "Loss: 0.001816\n",
      "Loss: 0.002095\n",
      "Loss: 0.001722\n",
      "Loss: 0.002085\n",
      "Loss: 0.001578\n",
      "Loss: 0.001844\n",
      "Loss: 0.001770\n",
      "Loss: 0.001990\n",
      "Loss: 0.001635\n",
      "Loss: 0.002058\n",
      "Loss: 0.001865\n",
      "Loss: 0.001795\n",
      "Loss: 0.001872\n",
      "Loss: 0.001755\n",
      "Loss: 0.001832\n",
      "Loss: 0.001601\n",
      "Loss: 0.002113\n",
      "Loss: 0.001822\n",
      "Loss: 0.001485\n",
      "Loss: 0.001502\n",
      "Loss: 0.002031\n",
      "Loss: 0.001558\n",
      "Loss: 0.001812\n",
      "Loss: 0.001805\n",
      "Loss: 0.001894\n",
      "Loss: 0.001661\n",
      "Loss: 0.001251\n",
      "Loss: 0.001359\n",
      "Loss: 0.001860\n",
      "Loss: 0.001721\n",
      "Loss: 0.001327\n",
      "Loss: 0.001671\n",
      "Loss: 0.001803\n",
      "Loss: 0.001533\n",
      "Loss: 0.001967\n",
      "Loss: 0.001924\n",
      "Loss: 0.001396\n",
      "Loss: 0.001613\n",
      "Loss: 0.001694\n",
      "Loss: 0.001440\n",
      "Loss: 0.001618\n",
      "Loss: 0.001571\n",
      "Loss: 0.001632\n",
      "Loss: 0.001686\n",
      "Loss: 0.001648\n",
      "Loss: 0.001964\n",
      "Loss: 0.001702\n",
      "Loss: 0.001510\n",
      "Loss: 0.001602\n",
      "Loss: 0.001403\n",
      "Loss: 0.001558\n",
      "Loss: 0.001746\n",
      "Loss: 0.001626\n",
      "Loss: 0.001363\n",
      "Loss: 0.001554\n",
      "Loss: 0.001882\n",
      "Loss: 0.001433\n",
      "Loss: 0.001547\n",
      "Loss: 0.001501\n",
      "Loss: 0.001959\n",
      "Loss: 0.001461\n",
      "Loss: 0.001430\n",
      "Loss: 0.001834\n",
      "Loss: 0.001272\n",
      "Loss: 0.001971\n",
      "Loss: 0.001678\n",
      "Loss: 0.001792\n",
      "Loss: 0.001435\n",
      "Loss: 0.001266\n",
      "Loss: 0.001693\n",
      "Loss: 0.001440\n",
      "Loss: 0.001522\n",
      "Loss: 0.001292\n",
      "Loss: 0.001258\n",
      "Loss: 0.001272\n",
      "Loss: 0.001261\n",
      "Loss: 0.001784\n",
      "Loss: 0.001291\n",
      "Loss: 0.001602\n",
      "Loss: 0.001568\n",
      "Loss: 0.001577\n",
      "Loss: 0.001629\n",
      "Loss: 0.001501\n",
      "Loss: 0.001414\n",
      "Loss: 0.001367\n",
      "Loss: 0.001350\n",
      "Loss: 0.001334\n",
      "Loss: 0.001219\n",
      "Loss: 0.001479\n",
      "Loss: 0.001442\n",
      "Loss: 0.001814\n",
      "Loss: 0.001359\n",
      "Loss: 0.001122\n",
      "Loss: 0.001660\n",
      "Loss: 0.001390\n",
      "Loss: 0.001417\n",
      "Loss: 0.001720\n",
      "Loss: 0.001393\n",
      "Loss: 0.001590\n",
      "Loss: 0.001542\n",
      "Loss: 0.001321\n",
      "Loss: 0.001138\n",
      "Loss: 0.001351\n",
      "Loss: 0.001378\n",
      "Loss: 0.001350\n",
      "Loss: 0.001370\n",
      "Loss: 0.001355\n",
      "Loss: 0.001388\n",
      "Loss: 0.001040\n",
      "Loss: 0.001339\n",
      "Loss: 0.001086\n",
      "Loss: 0.001029\n",
      "Loss: 0.001211\n",
      "Loss: 0.001373\n",
      "Loss: 0.001129\n",
      "Loss: 0.001073\n",
      "Loss: 0.001688\n",
      "Loss: 0.001374\n",
      "Loss: 0.001245\n",
      "Loss: 0.001295\n",
      "Loss: 0.001206\n",
      "Loss: 0.001350\n",
      "Loss: 0.001708\n",
      "Loss: 0.001387\n",
      "Loss: 0.001373\n",
      "Loss: 0.001574\n",
      "Loss: 0.001150\n",
      "Loss: 0.001487\n",
      "Loss: 0.001456\n",
      "Loss: 0.001180\n",
      "Loss: 0.001298\n",
      "Trained on 2 - Batch 1\n",
      "Loss: 0.000943\n",
      "Loss: 0.001254\n",
      "Loss: 0.001047\n",
      "Loss: 0.001394\n",
      "Loss: 0.000884\n",
      "Loss: 0.001005\n",
      "Loss: 0.001056\n",
      "Loss: 0.001056\n",
      "Loss: 0.000975\n",
      "Loss: 0.000955\n",
      "Loss: 0.001646\n",
      "Loss: 0.001233\n",
      "Loss: 0.001286\n",
      "Loss: 0.001023\n",
      "Loss: 0.001224\n",
      "Loss: 0.001298\n",
      "Loss: 0.001088\n",
      "Loss: 0.001245\n",
      "Loss: 0.001462\n",
      "Loss: 0.001209\n",
      "Loss: 0.000856\n",
      "Loss: 0.000969\n",
      "Loss: 0.000987\n",
      "Loss: 0.001147\n",
      "Loss: 0.001068\n",
      "Loss: 0.000985\n",
      "Loss: 0.001084\n",
      "Loss: 0.000966\n",
      "Loss: 0.000925\n",
      "Loss: 0.001137\n",
      "Loss: 0.001034\n",
      "Loss: 0.001221\n",
      "Loss: 0.000943\n",
      "Loss: 0.000989\n",
      "Loss: 0.000972\n",
      "Loss: 0.001215\n",
      "Loss: 0.001057\n",
      "Loss: 0.001353\n",
      "Loss: 0.000835\n",
      "Loss: 0.001164\n",
      "Loss: 0.001151\n",
      "Loss: 0.001085\n",
      "Loss: 0.000951\n",
      "Loss: 0.000959\n",
      "Loss: 0.000869\n",
      "Loss: 0.000839\n",
      "Loss: 0.000848\n",
      "Loss: 0.001070\n",
      "Loss: 0.001145\n",
      "Loss: 0.001220\n",
      "Loss: 0.000641\n",
      "Loss: 0.001058\n",
      "Loss: 0.000935\n",
      "Loss: 0.000694\n",
      "Loss: 0.000958\n",
      "Loss: 0.000863\n",
      "Loss: 0.000845\n",
      "Loss: 0.000837\n",
      "Loss: 0.001108\n",
      "Loss: 0.000827\n",
      "Loss: 0.000918\n",
      "Loss: 0.000729\n",
      "Loss: 0.001067\n",
      "Loss: 0.000955\n",
      "Loss: 0.000856\n",
      "Loss: 0.000764\n",
      "Loss: 0.000909\n",
      "Loss: 0.000885\n",
      "Loss: 0.000878\n",
      "Loss: 0.000889\n",
      "Loss: 0.000761\n",
      "Loss: 0.001040\n",
      "Loss: 0.001022\n",
      "Loss: 0.000802\n",
      "Loss: 0.001024\n",
      "Loss: 0.001109\n",
      "Loss: 0.000659\n",
      "Loss: 0.000860\n",
      "Loss: 0.000808\n",
      "Loss: 0.000781\n",
      "Loss: 0.000595\n",
      "Loss: 0.001272\n",
      "Loss: 0.000569\n",
      "Loss: 0.000568\n",
      "Loss: 0.000986\n",
      "Loss: 0.000893\n",
      "Loss: 0.000926\n",
      "Loss: 0.000876\n",
      "Loss: 0.001072\n",
      "Loss: 0.000683\n",
      "Loss: 0.001105\n",
      "Loss: 0.000823\n",
      "Loss: 0.000824\n",
      "Loss: 0.000890\n",
      "Loss: 0.000901\n",
      "Loss: 0.001130\n",
      "Loss: 0.001055\n",
      "Loss: 0.001123\n",
      "Loss: 0.000686\n",
      "Loss: 0.000844\n",
      "Loss: 0.000919\n",
      "Loss: 0.000692\n",
      "Loss: 0.000636\n",
      "Loss: 0.000860\n",
      "Loss: 0.000732\n",
      "Loss: 0.000584\n",
      "Loss: 0.000665\n",
      "Loss: 0.000686\n",
      "Loss: 0.000608\n",
      "Loss: 0.000646\n",
      "Loss: 0.000842\n",
      "Loss: 0.000503\n",
      "Loss: 0.000634\n",
      "Loss: 0.000654\n",
      "Loss: 0.000834\n",
      "Loss: 0.000433\n",
      "Loss: 0.000542\n",
      "Loss: 0.000933\n",
      "Loss: 0.000600\n",
      "Loss: 0.000875\n",
      "Loss: 0.000910\n",
      "Loss: 0.000644\n",
      "Loss: 0.000709\n",
      "Loss: 0.000654\n",
      "Loss: 0.000755\n",
      "Loss: 0.000419\n",
      "Loss: 0.000456\n",
      "Loss: 0.000879\n",
      "Loss: 0.000819\n",
      "Loss: 0.000524\n",
      "Loss: 0.000556\n",
      "Loss: 0.000772\n",
      "Loss: 0.000551\n",
      "Loss: 0.000910\n",
      "Loss: 0.000952\n",
      "Loss: 0.000494\n",
      "Loss: 0.000480\n",
      "Loss: 0.000593\n",
      "Loss: 0.000652\n",
      "Loss: 0.000530\n",
      "Loss: 0.000645\n",
      "Loss: 0.000695\n",
      "Loss: 0.000597\n",
      "Loss: 0.000750\n",
      "Loss: 0.000830\n",
      "Loss: 0.000495\n",
      "Loss: 0.000352\n",
      "Loss: 0.000461\n",
      "Loss: 0.000504\n",
      "Loss: 0.000752\n",
      "Loss: 0.000689\n",
      "Loss: 0.000403\n",
      "Loss: 0.000503\n",
      "Loss: 0.000593\n",
      "Loss: 0.000530\n",
      "Loss: 0.000757\n",
      "Loss: 0.000620\n",
      "Loss: 0.000564\n",
      "Loss: 0.000403\n",
      "Loss: 0.000788\n",
      "Loss: 0.000459\n",
      "Loss: 0.000564\n",
      "Loss: 0.000785\n",
      "Loss: 0.000496\n",
      "Loss: 0.000776\n",
      "Loss: 0.000767\n",
      "Loss: 0.000770\n",
      "Loss: 0.000701\n",
      "Loss: 0.000426\n",
      "Loss: 0.000685\n",
      "Loss: 0.000671\n",
      "Loss: 0.000787\n",
      "Loss: 0.000812\n",
      "Loss: 0.000750\n",
      "Loss: 0.000828\n",
      "Loss: 0.000461\n",
      "Loss: 0.000934\n",
      "Loss: 0.000465\n",
      "Loss: 0.000606\n",
      "Loss: 0.001102\n",
      "Loss: 0.000617\n",
      "Loss: 0.000957\n",
      "Loss: 0.000476\n",
      "Loss: 0.000890\n",
      "Loss: 0.000867\n",
      "Loss: 0.000680\n",
      "Loss: 0.000567\n",
      "Loss: 0.000568\n",
      "Loss: 0.000777\n",
      "Loss: 0.000628\n",
      "Loss: 0.000595\n",
      "Loss: 0.000716\n",
      "Loss: 0.000524\n",
      "Loss: 0.000707\n",
      "Loss: 0.000832\n",
      "Loss: 0.000329\n",
      "Loss: 0.000711\n",
      "Loss: 0.000596\n",
      "Loss: 0.000547\n",
      "Loss: 0.000692\n",
      "Loss: 0.000749\n",
      "Loss: 0.000583\n",
      "Loss: 0.000544\n",
      "Loss: 0.000545\n",
      "Loss: 0.000548\n",
      "Loss: 0.000680\n",
      "Loss: 0.000418\n",
      "Loss: 0.000378\n",
      "Loss: 0.000761\n",
      "Loss: 0.000546\n",
      "Loss: 0.000322\n",
      "Loss: 0.000423\n",
      "Loss: 0.000473\n",
      "Loss: 0.000897\n",
      "Loss: 0.000830\n",
      "Loss: 0.000850\n",
      "Loss: 0.000588\n",
      "Loss: 0.000708\n",
      "Loss: 0.000565\n",
      "Loss: 0.000635\n",
      "Loss: 0.000526\n",
      "Loss: 0.000609\n",
      "Loss: 0.000635\n",
      "Loss: 0.000602\n",
      "Loss: 0.000618\n",
      "Loss: 0.000743\n",
      "Loss: 0.000460\n",
      "Loss: 0.000964\n",
      "Loss: 0.000581\n",
      "Loss: 0.000730\n",
      "Loss: 0.000615\n",
      "Loss: 0.000477\n",
      "Loss: 0.000564\n",
      "Loss: 0.000680\n",
      "Loss: 0.000633\n",
      "Loss: 0.000763\n",
      "Loss: 0.000579\n",
      "Loss: 0.000758\n",
      "Loss: 0.000709\n",
      "Loss: 0.000437\n",
      "Loss: 0.000654\n",
      "Loss: 0.000356\n",
      "Loss: 0.000287\n",
      "Loss: 0.000406\n",
      "Loss: 0.000615\n",
      "Loss: 0.000735\n",
      "Loss: 0.000765\n",
      "Loss: 0.000398\n",
      "Loss: 0.000494\n",
      "Trained on 2 - Batch 2\n",
      "Loading test file 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.996410\n",
      "Loading train file 1\n",
      "Loss: 0.003574\n",
      "Loss: 0.002784\n",
      "Loss: 0.003287\n",
      "Loss: 0.003097\n",
      "Loss: 0.002559\n",
      "Loss: 0.002770\n",
      "Loss: 0.002880\n",
      "Loss: 0.003013\n",
      "Loss: 0.002626\n",
      "Loss: 0.002613\n",
      "Loss: 0.002566\n",
      "Loss: 0.002621\n",
      "Loss: 0.002648\n",
      "Loss: 0.002966\n",
      "Loss: 0.002352\n",
      "Loss: 0.002658\n",
      "Loss: 0.002607\n",
      "Loss: 0.002769\n",
      "Loss: 0.002742\n",
      "Loss: 0.002642\n",
      "Loss: 0.002398\n",
      "Loss: 0.002639\n",
      "Loss: 0.002809\n",
      "Loss: 0.002609\n",
      "Loss: 0.002755\n",
      "Loss: 0.002753\n",
      "Loss: 0.002588\n",
      "Loss: 0.002620\n",
      "Loss: 0.002824\n",
      "Loss: 0.002434\n",
      "Loss: 0.002477\n",
      "Loss: 0.002616\n",
      "Loss: 0.002678\n",
      "Loss: 0.002359\n",
      "Loss: 0.002517\n",
      "Loss: 0.002486\n",
      "Loss: 0.002470\n",
      "Loss: 0.002835\n",
      "Loss: 0.002429\n",
      "Loss: 0.002569\n",
      "Loss: 0.002586\n",
      "Loss: 0.002465\n",
      "Loss: 0.002287\n",
      "Loss: 0.002371\n",
      "Loss: 0.002585\n",
      "Loss: 0.002934\n",
      "Loss: 0.002444\n",
      "Loss: 0.002381\n",
      "Loss: 0.002513\n",
      "Loss: 0.002523\n",
      "Loss: 0.002253\n",
      "Loss: 0.002193\n",
      "Loss: 0.002475\n",
      "Loss: 0.002154\n",
      "Loss: 0.002436\n",
      "Loss: 0.002365\n",
      "Loss: 0.002643\n",
      "Loss: 0.002449\n",
      "Loss: 0.002371\n",
      "Loss: 0.002231\n",
      "Loss: 0.001960\n",
      "Loss: 0.002438\n",
      "Loss: 0.002288\n",
      "Loss: 0.002458\n",
      "Loss: 0.002168\n",
      "Loss: 0.002467\n",
      "Loss: 0.002311\n",
      "Loss: 0.002716\n",
      "Loss: 0.002102\n",
      "Loss: 0.002611\n",
      "Loss: 0.002249\n",
      "Loss: 0.002370\n",
      "Loss: 0.002206\n",
      "Loss: 0.001799\n",
      "Loss: 0.002402\n",
      "Loss: 0.002666\n",
      "Loss: 0.002300\n",
      "Loss: 0.002150\n",
      "Loss: 0.002547\n",
      "Loss: 0.002359\n",
      "Loss: 0.002199\n",
      "Loss: 0.002187\n",
      "Loss: 0.002319\n",
      "Loss: 0.002444\n",
      "Loss: 0.002528\n",
      "Loss: 0.002425\n",
      "Loss: 0.002424\n",
      "Loss: 0.002329\n",
      "Loss: 0.002518\n",
      "Loss: 0.002245\n",
      "Loss: 0.002200\n",
      "Loss: 0.002249\n",
      "Loss: 0.001971\n",
      "Loss: 0.002291\n",
      "Loss: 0.002224\n",
      "Loss: 0.001978\n",
      "Loss: 0.002177\n",
      "Loss: 0.002150\n",
      "Loss: 0.002076\n",
      "Loss: 0.002154\n",
      "Loss: 0.002367\n",
      "Loss: 0.002147\n",
      "Loss: 0.001938\n",
      "Loss: 0.002466\n",
      "Loss: 0.002092\n",
      "Loss: 0.002086\n",
      "Loss: 0.002468\n",
      "Loss: 0.001920\n",
      "Loss: 0.001980\n",
      "Loss: 0.002249\n",
      "Loss: 0.002090\n",
      "Loss: 0.002168\n",
      "Loss: 0.002252\n",
      "Loss: 0.002159\n",
      "Loss: 0.002415\n",
      "Loss: 0.001896\n",
      "Loss: 0.002073\n",
      "Loss: 0.002284\n",
      "Loss: 0.002149\n",
      "Loss: 0.001841\n",
      "Loss: 0.002186\n",
      "Loss: 0.002181\n",
      "Loss: 0.002390\n",
      "Loss: 0.002184\n",
      "Loss: 0.002037\n",
      "Loss: 0.002207\n",
      "Loss: 0.002174\n",
      "Loss: 0.001945\n",
      "Loss: 0.001992\n",
      "Loss: 0.002019\n",
      "Loss: 0.002115\n",
      "Loss: 0.002032\n",
      "Loss: 0.001711\n",
      "Loss: 0.002570\n",
      "Loss: 0.001883\n",
      "Loss: 0.001783\n",
      "Loss: 0.001962\n",
      "Loss: 0.001823\n",
      "Loss: 0.002333\n",
      "Loss: 0.001758\n",
      "Loss: 0.001501\n",
      "Loss: 0.002090\n",
      "Loss: 0.002264\n",
      "Loss: 0.002099\n",
      "Loss: 0.002012\n",
      "Loss: 0.001850\n",
      "Loss: 0.002018\n",
      "Loss: 0.002180\n",
      "Loss: 0.002009\n",
      "Loss: 0.001936\n",
      "Loss: 0.001911\n",
      "Loss: 0.001962\n",
      "Loss: 0.001483\n",
      "Loss: 0.001953\n",
      "Loss: 0.002256\n",
      "Loss: 0.001910\n",
      "Loss: 0.002007\n",
      "Loss: 0.001933\n",
      "Loss: 0.002088\n",
      "Loss: 0.001922\n",
      "Loss: 0.002188\n",
      "Loss: 0.001829\n",
      "Loss: 0.002013\n",
      "Loss: 0.002003\n",
      "Loss: 0.001810\n",
      "Loss: 0.002199\n",
      "Loss: 0.002425\n",
      "Loss: 0.001877\n",
      "Loss: 0.001848\n",
      "Loss: 0.001703\n",
      "Loss: 0.001795\n",
      "Loss: 0.001775\n",
      "Loss: 0.002374\n",
      "Loss: 0.002225\n",
      "Loss: 0.001985\n",
      "Loss: 0.002257\n",
      "Loss: 0.002182\n",
      "Loss: 0.001911\n",
      "Loss: 0.002134\n",
      "Loss: 0.002071\n",
      "Loss: 0.002028\n",
      "Loss: 0.001745\n",
      "Loss: 0.001845\n",
      "Loss: 0.002085\n",
      "Loss: 0.001974\n",
      "Loss: 0.002067\n",
      "Loss: 0.001673\n",
      "Loss: 0.001668\n",
      "Loss: 0.002057\n",
      "Loss: 0.001779\n",
      "Loss: 0.001696\n",
      "Loss: 0.001808\n",
      "Loss: 0.001819\n",
      "Loss: 0.001555\n",
      "Loss: 0.001891\n",
      "Loss: 0.001985\n",
      "Loss: 0.001483\n",
      "Loss: 0.001683\n",
      "Loss: 0.001823\n",
      "Loss: 0.001759\n",
      "Loss: 0.001740\n",
      "Loss: 0.001733\n",
      "Loss: 0.001762\n",
      "Loss: 0.001640\n",
      "Loss: 0.001794\n",
      "Loss: 0.001801\n",
      "Loss: 0.001540\n",
      "Loss: 0.002004\n",
      "Loss: 0.001807\n",
      "Loss: 0.001784\n",
      "Loss: 0.001720\n",
      "Loss: 0.002214\n",
      "Loss: 0.001740\n",
      "Loss: 0.001637\n",
      "Loss: 0.001942\n",
      "Loss: 0.002143\n",
      "Loss: 0.001607\n",
      "Loss: 0.002147\n",
      "Loss: 0.001679\n",
      "Loss: 0.001494\n",
      "Loss: 0.001800\n",
      "Loss: 0.001742\n",
      "Loss: 0.001856\n",
      "Loss: 0.001642\n",
      "Loss: 0.001815\n",
      "Loss: 0.001591\n",
      "Loss: 0.001991\n",
      "Loss: 0.001571\n",
      "Loss: 0.001631\n",
      "Loss: 0.001563\n",
      "Loss: 0.001559\n",
      "Loss: 0.001606\n",
      "Loss: 0.001295\n",
      "Loss: 0.001392\n",
      "Loss: 0.001480\n",
      "Loss: 0.001872\n",
      "Loss: 0.001377\n",
      "Loss: 0.001429\n",
      "Loss: 0.001615\n",
      "Loss: 0.001673\n",
      "Loss: 0.001854\n",
      "Loss: 0.001656\n",
      "Loss: 0.001926\n",
      "Loss: 0.001969\n",
      "Loss: 0.001484\n",
      "Loss: 0.001515\n",
      "Loss: 0.001442\n",
      "Loss: 0.001494\n",
      "Loss: 0.001431\n",
      "Loss: 0.001367\n",
      "Loss: 0.001260\n",
      "Loss: 0.000988\n",
      "Loss: 0.001639\n",
      "Loss: 0.001295\n",
      "Loss: 0.002085\n",
      "Loss: 0.001560\n",
      "Loss: 0.001007\n",
      "Loss: 0.001853\n",
      "Loss: 0.001565\n",
      "Loss: 0.001625\n",
      "Loss: 0.001966\n",
      "Loss: 0.001042\n",
      "Loss: 0.001578\n",
      "Loss: 0.001635\n",
      "Loss: 0.001722\n",
      "Loss: 0.001664\n",
      "Loss: 0.001709\n",
      "Loss: 0.001166\n",
      "Loss: 0.001561\n",
      "Loss: 0.001501\n",
      "Loss: 0.001471\n",
      "Loss: 0.001472\n",
      "Loss: 0.001690\n",
      "Loss: 0.001539\n",
      "Loss: 0.001409\n",
      "Loss: 0.001677\n",
      "Loss: 0.001599\n",
      "Loss: 0.001179\n",
      "Loss: 0.001664\n",
      "Loss: 0.001642\n",
      "Loss: 0.001448\n",
      "Loss: 0.001485\n",
      "Loss: 0.001529\n",
      "Loss: 0.001298\n",
      "Loss: 0.001364\n",
      "Loss: 0.001496\n",
      "Loss: 0.001274\n",
      "Loss: 0.001243\n",
      "Loss: 0.001174\n",
      "Loss: 0.001405\n",
      "Loss: 0.001182\n",
      "Loss: 0.001362\n",
      "Loss: 0.001123\n",
      "Loss: 0.001222\n",
      "Loss: 0.001195\n",
      "Loss: 0.001266\n",
      "Loss: 0.001098\n",
      "Loss: 0.001138\n",
      "Loss: 0.001236\n",
      "Loss: 0.001263\n",
      "Trained on 1 - Batch 1\n",
      "Loss: 0.001021\n",
      "Loss: 0.001404\n",
      "Loss: 0.001129\n",
      "Loss: 0.000842\n",
      "Loss: 0.001290\n",
      "Loss: 0.001525\n",
      "Loss: 0.001196\n",
      "Loss: 0.001136\n",
      "Loss: 0.001300\n",
      "Loss: 0.001340\n",
      "Loss: 0.001048\n",
      "Loss: 0.001051\n",
      "Loss: 0.001263\n",
      "Loss: 0.001106\n",
      "Loss: 0.001105\n",
      "Loss: 0.001468\n",
      "Loss: 0.001320\n",
      "Loss: 0.001278\n",
      "Loss: 0.001454\n",
      "Loss: 0.001293\n",
      "Loss: 0.001162\n",
      "Loss: 0.000865\n",
      "Loss: 0.000952\n",
      "Loss: 0.000981\n",
      "Loss: 0.001281\n",
      "Loss: 0.001151\n",
      "Loss: 0.001135\n",
      "Loss: 0.000806\n",
      "Loss: 0.001039\n",
      "Loss: 0.001124\n",
      "Loss: 0.001186\n",
      "Loss: 0.001098\n",
      "Loss: 0.001040\n",
      "Loss: 0.000760\n",
      "Loss: 0.001140\n",
      "Loss: 0.001255\n",
      "Loss: 0.001277\n",
      "Loss: 0.000809\n",
      "Loss: 0.001843\n",
      "Loss: 0.001016\n",
      "Loss: 0.001024\n",
      "Loss: 0.000967\n",
      "Loss: 0.000941\n",
      "Loss: 0.001118\n",
      "Loss: 0.001195\n",
      "Loss: 0.001098\n",
      "Loss: 0.001088\n",
      "Loss: 0.000728\n",
      "Loss: 0.001707\n",
      "Loss: 0.000983\n",
      "Loss: 0.000971\n",
      "Loss: 0.000698\n",
      "Loss: 0.000809\n",
      "Loss: 0.001545\n",
      "Loss: 0.000998\n",
      "Loss: 0.001265\n",
      "Loss: 0.000919\n",
      "Loss: 0.001058\n",
      "Loss: 0.001210\n",
      "Loss: 0.000905\n",
      "Loss: 0.000981\n",
      "Loss: 0.001028\n",
      "Loss: 0.000984\n",
      "Loss: 0.001261\n",
      "Loss: 0.000810\n",
      "Loss: 0.001182\n",
      "Loss: 0.000862\n",
      "Loss: 0.000911\n",
      "Loss: 0.000841\n",
      "Loss: 0.000899\n",
      "Loss: 0.000954\n",
      "Loss: 0.000925\n",
      "Loss: 0.000773\n",
      "Loss: 0.001194\n",
      "Loss: 0.001125\n",
      "Loss: 0.000867\n",
      "Loss: 0.001221\n",
      "Loss: 0.000834\n",
      "Loss: 0.001412\n",
      "Loss: 0.001383\n",
      "Loss: 0.001144\n",
      "Loss: 0.001151\n",
      "Loss: 0.000885\n",
      "Loss: 0.001588\n",
      "Loss: 0.000963\n",
      "Loss: 0.000829\n",
      "Loss: 0.001199\n",
      "Loss: 0.000772\n",
      "Loss: 0.000680\n",
      "Loss: 0.001362\n",
      "Loss: 0.000662\n",
      "Loss: 0.000844\n",
      "Loss: 0.000786\n",
      "Loss: 0.000865\n",
      "Loss: 0.000963\n",
      "Loss: 0.000856\n",
      "Loss: 0.000538\n",
      "Loss: 0.000714\n",
      "Loss: 0.000622\n",
      "Loss: 0.000868\n",
      "Loss: 0.000899\n",
      "Loss: 0.000686\n",
      "Loss: 0.001021\n",
      "Loss: 0.000885\n",
      "Loss: 0.001037\n",
      "Loss: 0.000790\n",
      "Loss: 0.000621\n",
      "Loss: 0.000985\n",
      "Loss: 0.000985\n",
      "Loss: 0.000846\n",
      "Loss: 0.000800\n",
      "Loss: 0.000799\n",
      "Loss: 0.000740\n",
      "Loss: 0.000672\n",
      "Loss: 0.000969\n",
      "Loss: 0.000727\n",
      "Loss: 0.000779\n",
      "Loss: 0.000714\n",
      "Loss: 0.000568\n",
      "Loss: 0.000510\n",
      "Loss: 0.001146\n",
      "Loss: 0.000749\n",
      "Loss: 0.001100\n",
      "Loss: 0.000743\n",
      "Loss: 0.000942\n",
      "Loss: 0.000762\n",
      "Loss: 0.000582\n",
      "Loss: 0.000557\n",
      "Loss: 0.000801\n",
      "Loss: 0.000668\n",
      "Loss: 0.000836\n",
      "Loss: 0.000728\n",
      "Loss: 0.000575\n",
      "Loss: 0.000857\n",
      "Loss: 0.000596\n",
      "Loss: 0.000537\n",
      "Loss: 0.000895\n",
      "Loss: 0.000841\n",
      "Loss: 0.000760\n",
      "Loss: 0.000936\n",
      "Loss: 0.000602\n",
      "Loss: 0.000552\n",
      "Loss: 0.000709\n",
      "Loss: 0.000738\n",
      "Loss: 0.000769\n",
      "Loss: 0.000838\n",
      "Loss: 0.000835\n",
      "Loss: 0.000667\n",
      "Loss: 0.000799\n",
      "Loss: 0.000704\n",
      "Loss: 0.000892\n",
      "Loss: 0.000982\n",
      "Loss: 0.000726\n",
      "Loss: 0.000808\n",
      "Loss: 0.000550\n",
      "Loss: 0.000936\n",
      "Loss: 0.000755\n",
      "Loss: 0.000576\n",
      "Loss: 0.000798\n",
      "Loss: 0.000658\n",
      "Loss: 0.000563\n",
      "Loss: 0.000905\n",
      "Loss: 0.000884\n",
      "Loss: 0.001063\n",
      "Loss: 0.000767\n",
      "Loss: 0.000812\n",
      "Loss: 0.000936\n",
      "Loss: 0.000591\n",
      "Loss: 0.000756\n",
      "Loss: 0.000480\n",
      "Loss: 0.000660\n",
      "Loss: 0.000728\n",
      "Loss: 0.000543\n",
      "Loss: 0.000612\n",
      "Loss: 0.000621\n",
      "Loss: 0.000772\n",
      "Loss: 0.000594\n",
      "Loss: 0.000497\n",
      "Loss: 0.000628\n",
      "Loss: 0.000487\n",
      "Loss: 0.000563\n",
      "Loss: 0.000573\n",
      "Loss: 0.001040\n",
      "Loss: 0.000333\n",
      "Loss: 0.000610\n",
      "Loss: 0.000280\n",
      "Loss: 0.000792\n",
      "Loss: 0.000484\n",
      "Loss: 0.000545\n",
      "Loss: 0.000632\n",
      "Loss: 0.000677\n",
      "Loss: 0.000731\n",
      "Loss: 0.000685\n",
      "Loss: 0.000598\n",
      "Loss: 0.000615\n",
      "Loss: 0.000909\n",
      "Loss: 0.000564\n",
      "Loss: 0.000800\n",
      "Loss: 0.000418\n",
      "Loss: 0.000967\n",
      "Loss: 0.000856\n",
      "Loss: 0.000534\n",
      "Loss: 0.001000\n",
      "Loss: 0.000773\n",
      "Loss: 0.000585\n",
      "Loss: 0.001247\n",
      "Loss: 0.000543\n",
      "Loss: 0.000774\n",
      "Loss: 0.000974\n",
      "Loss: 0.000461\n",
      "Loss: 0.000556\n",
      "Loss: 0.000413\n",
      "Loss: 0.000849\n",
      "Loss: 0.000378\n",
      "Loss: 0.000855\n",
      "Loss: 0.000429\n",
      "Loss: 0.000626\n",
      "Loss: 0.000718\n",
      "Loss: 0.000369\n",
      "Loss: 0.000647\n",
      "Loss: 0.001037\n",
      "Loss: 0.000628\n",
      "Loss: 0.000742\n",
      "Loss: 0.000643\n",
      "Loss: 0.000585\n",
      "Loss: 0.000753\n",
      "Loss: 0.000479\n",
      "Loss: 0.000412\n",
      "Loss: 0.000538\n",
      "Loss: 0.000571\n",
      "Loss: 0.000551\n",
      "Loss: 0.000664\n",
      "Loss: 0.000471\n",
      "Loss: 0.000630\n",
      "Loss: 0.000341\n",
      "Loss: 0.000467\n",
      "Loss: 0.000276\n",
      "Loss: 0.000508\n",
      "Loss: 0.000383\n",
      "Loss: 0.000331\n",
      "Loss: 0.000394\n",
      "Loss: 0.000386\n",
      "Loss: 0.000765\n",
      "Loss: 0.000287\n",
      "Loss: 0.000513\n",
      "Loss: 0.000435\n",
      "Loss: 0.000518\n",
      "Loss: 0.000482\n",
      "Loss: 0.000610\n",
      "Trained on 1 - Batch 2\n",
      "Loading test file 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 AUC: 0.995686\n",
      "Average AUC over 10 folds: 0.996048\n",
      "End time: 1739344846.0796015\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cross_validate():\n",
    "    all_indices = list(range(1, 11))   ##########\n",
    "    total_auc = []\n",
    "\n",
    "    log_print(f\"Start time: {time.time()}\")\n",
    "\n",
    "    for fold in range(10):      ###########\n",
    "        try:\n",
    "            fold_auc = []\n",
    "            train_files = [i for i in all_indices if i != fold + 1]  # 训练集 9 份\n",
    "            test_file = fold + 1  # 测试集 1 份\n",
    "\n",
    "            # 创建 MLP 模型\n",
    "            model = MLP(input_size=2400).to(device)\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "            # 训练 9 份数据\n",
    "            for file_idx in train_files:\n",
    "                log_print(f\"Loading train file {file_idx}\")\n",
    "\n",
    "                batch_idx = 1\n",
    "                while True:\n",
    "                    pickle_file = f\"/data2/xpgeng/iML1515_MLP/{file_idx}_{batch_idx}.pkl\"\n",
    "                    if not os.path.exists(pickle_file):\n",
    "                        break  # 没有更多批次数据\n",
    "\n",
    "                    with open(pickle_file, \"rb\") as f:\n",
    "                        batch_data, batch_labels = pickle.load(f)\n",
    "                    \n",
    "                    # 转换为张量\n",
    "                    batch_data = torch.tensor(batch_data, dtype=torch.float32).to(device)\n",
    "                    batch_labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "                    \n",
    "                    # Split into smaller batches of size 200\n",
    "                    for i in range(0, len(batch_data), 200):\n",
    "                        small_batch_data = batch_data[i:i+200]\n",
    "                        small_batch_labels = batch_labels[i:i+200]\n",
    "                        \n",
    "                        # Train the model on this smaller batch\n",
    "                        model = train_mlp(small_batch_data, small_batch_labels, model, criterion, optimizer, num_epochs=4)\n",
    "\n",
    "                    log_print(f\"Trained on {file_idx} - Batch {batch_idx}\")\n",
    "                    batch_idx += 1\n",
    "\n",
    "            # 读取测试数据\n",
    "            log_print(f\"Loading test file {test_file}\")\n",
    "            \n",
    "            batch_idx = 1\n",
    "            while True:\n",
    "                pickle_file = f\"/data2/xpgeng/iML1515_MLP/{file_idx}_{batch_idx}.pkl\"\n",
    "                if not os.path.exists(pickle_file):\n",
    "                    break  # 没有更多批次数据\n",
    "\n",
    "                with open(pickle_file, \"rb\") as f:\n",
    "                    test_data, test_labels = pickle.load(f)\n",
    "\n",
    "                # 转换为张量\n",
    "                test_data = torch.tensor(np.array(test_data), dtype=torch.float32).to(device)\n",
    "                test_labels = torch.tensor(np.array(test_labels), dtype=torch.float32).to(device)\n",
    "                \n",
    "                # 评估\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    outputs = model(test_data)\n",
    "                    probabilities = torch.sigmoid(outputs.view(-1))  \n",
    "                    auc = compute_auc(probabilities, test_labels)\n",
    "                    fold_auc.append(auc)\n",
    "                    total_auc.append(auc)\n",
    "                \n",
    "                batch_idx += 1\n",
    "\n",
    "            log_print(f\"Fold {fold+1} AUC: {np.mean(fold_auc):.6f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error in fold {fold+1}: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            log_error(error_message)\n",
    "\n",
    "    log_print(f\"Average AUC over 10 folds: {np.mean(total_auc):.6f}\")\n",
    "    log_print(f\"End time: {time.time()}\")\n",
    "\n",
    "cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141895d",
   "metadata": {},
   "source": [
    "### MLP model train all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bfebaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Check if CUDA device is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_file = os.path.join(os.getcwd(), \"mlp_all_training_log.txt\")          ############\n",
    "error_log_file = os.path.join(os.getcwd(), \"mlp_all_error_log.txt\")       ############\n",
    "\n",
    "# Redirect print to a file\n",
    "def log_print(message):\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(message)\n",
    "\n",
    "# Log error messages to a file\n",
    "def log_error(message):\n",
    "    with open(error_log_file, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(f\"Error logged: {message}\")  # Debugging line to ensure errors are logged\n",
    "    \n",
    "# Function to load and update del_twogenes files in batches\n",
    "def load_three_genes(file_idx, batch_size):\n",
    "    file_path = f'/data1/xpgeng/cross_pathogen/FBA/iML1515_parts/iML1515-{file_idx}.csv'   ###############\n",
    "    #file_path = f'/data1/xpgeng/cross_pathogen/MLP/iML1515-{file_idx}.csv'    ###############\n",
    "    \n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # Split file into batches of size batch_size\n",
    "    batches = [df.iloc[i:i + batch_size].values for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    return batches  # each batch contains three genes and a label\n",
    "\n",
    "# Function to compute AUC\n",
    "def compute_auc(predicted, labels):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    return roc_auc_score(labels.cpu(), predicted.cpu())\n",
    "\n",
    "# Define MLP model class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=2400, hidden_size1=512, hidden_size2=256, output_size=1, dropout_rate=0.1):  #######\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "def train_mlp(train_data, train_labels, model, criterion, optimizer, num_epochs=4):  \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_data)\n",
    "    loss = criterion(outputs.view(-1), train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    log_print(f\"Loss: {epoch_loss / len(train_data):.6f}\")  # Average loss over the epoch\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b08a1f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 1739323692.455317\n",
      "Loading train file 1\n",
      "Loss: 0.003359\n",
      "Loss: 0.002945\n",
      "Loss: 0.003193\n",
      "Loss: 0.002935\n",
      "Loss: 0.002798\n",
      "Loss: 0.002839\n",
      "Loss: 0.002882\n",
      "Loss: 0.003016\n",
      "Loss: 0.002524\n",
      "Loss: 0.002499\n",
      "Loss: 0.002561\n",
      "Loss: 0.002598\n",
      "Loss: 0.002600\n",
      "Loss: 0.002817\n",
      "Loss: 0.002355\n",
      "Loss: 0.002614\n",
      "Loss: 0.002576\n",
      "Loss: 0.002827\n",
      "Loss: 0.002779\n",
      "Loss: 0.002529\n",
      "Loss: 0.002205\n",
      "Loss: 0.002601\n",
      "Loss: 0.002767\n",
      "Loss: 0.002490\n",
      "Loss: 0.002768\n",
      "Loss: 0.002615\n",
      "Loss: 0.002568\n",
      "Loss: 0.002616\n",
      "Loss: 0.002795\n",
      "Loss: 0.002365\n",
      "Loss: 0.002419\n",
      "Loss: 0.002601\n",
      "Loss: 0.002582\n",
      "Loss: 0.002301\n",
      "Loss: 0.002398\n",
      "Loss: 0.002401\n",
      "Loss: 0.002293\n",
      "Loss: 0.002770\n",
      "Loss: 0.002238\n",
      "Loss: 0.002502\n",
      "Loss: 0.002452\n",
      "Loss: 0.002345\n",
      "Loss: 0.002244\n",
      "Loss: 0.002194\n",
      "Loss: 0.002535\n",
      "Loss: 0.002809\n",
      "Loss: 0.002263\n",
      "Loss: 0.002317\n",
      "Loss: 0.002426\n",
      "Loss: 0.002409\n",
      "Loss: 0.002104\n",
      "Loss: 0.002100\n",
      "Loss: 0.002288\n",
      "Loss: 0.001981\n",
      "Loss: 0.002301\n",
      "Loss: 0.002384\n",
      "Loss: 0.002301\n",
      "Loss: 0.002279\n",
      "Loss: 0.002211\n",
      "Loss: 0.002080\n",
      "Loss: 0.001725\n",
      "Loss: 0.002065\n",
      "Loss: 0.002058\n",
      "Loss: 0.002147\n",
      "Loss: 0.001901\n",
      "Loss: 0.002138\n",
      "Loss: 0.002323\n",
      "Loss: 0.002377\n",
      "Loss: 0.002095\n",
      "Loss: 0.002192\n",
      "Loss: 0.002151\n",
      "Loss: 0.001960\n",
      "Loss: 0.002243\n",
      "Loss: 0.001479\n",
      "Loss: 0.002122\n",
      "Loss: 0.002298\n",
      "Loss: 0.002166\n",
      "Loss: 0.001836\n",
      "Loss: 0.002045\n",
      "Loss: 0.002231\n",
      "Loss: 0.001846\n",
      "Loss: 0.002018\n",
      "Loss: 0.001939\n",
      "Loss: 0.002180\n",
      "Loss: 0.002206\n",
      "Loss: 0.002303\n",
      "Loss: 0.002295\n",
      "Loss: 0.002193\n",
      "Loss: 0.002579\n",
      "Loss: 0.002102\n",
      "Loss: 0.002255\n",
      "Loss: 0.002143\n",
      "Loss: 0.001713\n",
      "Loss: 0.002211\n",
      "Loss: 0.002170\n",
      "Loss: 0.001735\n",
      "Loss: 0.002239\n",
      "Loss: 0.002036\n",
      "Loss: 0.001827\n",
      "Loss: 0.001944\n",
      "Loss: 0.002248\n",
      "Loss: 0.001899\n",
      "Loss: 0.001999\n",
      "Loss: 0.002304\n",
      "Loss: 0.001924\n",
      "Loss: 0.001801\n",
      "Loss: 0.002195\n",
      "Loss: 0.001603\n",
      "Loss: 0.001943\n",
      "Loss: 0.001886\n",
      "Loss: 0.001928\n",
      "Loss: 0.001883\n",
      "Loss: 0.002059\n",
      "Loss: 0.001933\n",
      "Loss: 0.002275\n",
      "Loss: 0.001767\n",
      "Loss: 0.001740\n",
      "Loss: 0.002077\n",
      "Loss: 0.001886\n",
      "Loss: 0.001749\n",
      "Loss: 0.001992\n",
      "Loss: 0.001878\n",
      "Loss: 0.002087\n",
      "Loss: 0.001911\n",
      "Loss: 0.001671\n",
      "Loss: 0.001904\n",
      "Loss: 0.001975\n",
      "Loss: 0.001833\n",
      "Loss: 0.001905\n",
      "Loss: 0.001860\n",
      "Loss: 0.001720\n",
      "Loss: 0.001710\n",
      "Loss: 0.001805\n",
      "Loss: 0.002180\n",
      "Loss: 0.001942\n",
      "Loss: 0.001794\n",
      "Loss: 0.001702\n",
      "Loss: 0.002121\n",
      "Loss: 0.002369\n",
      "Loss: 0.001841\n",
      "Loss: 0.002018\n",
      "Loss: 0.001894\n",
      "Loss: 0.002231\n",
      "Loss: 0.002283\n",
      "Loss: 0.001697\n",
      "Loss: 0.001909\n",
      "Loss: 0.002195\n",
      "Loss: 0.001826\n",
      "Loss: 0.001705\n",
      "Loss: 0.002066\n",
      "Loss: 0.002030\n",
      "Loss: 0.001717\n",
      "Loss: 0.001420\n",
      "Loss: 0.001800\n",
      "Loss: 0.001616\n",
      "Loss: 0.001650\n",
      "Loss: 0.001573\n",
      "Loss: 0.001555\n",
      "Loss: 0.001838\n",
      "Loss: 0.001645\n",
      "Loss: 0.001599\n",
      "Loss: 0.001928\n",
      "Loss: 0.001937\n",
      "Loss: 0.001684\n",
      "Loss: 0.001651\n",
      "Loss: 0.002034\n",
      "Loss: 0.001922\n",
      "Loss: 0.001657\n",
      "Loss: 0.001848\n",
      "Loss: 0.001466\n",
      "Loss: 0.001550\n",
      "Loss: 0.001697\n",
      "Loss: 0.002104\n",
      "Loss: 0.001916\n",
      "Loss: 0.002100\n",
      "Loss: 0.002437\n",
      "Loss: 0.001909\n",
      "Loss: 0.001649\n",
      "Loss: 0.002151\n",
      "Loss: 0.001735\n",
      "Loss: 0.001758\n",
      "Loss: 0.001757\n",
      "Loss: 0.001611\n",
      "Loss: 0.001925\n",
      "Loss: 0.001731\n",
      "Loss: 0.002076\n",
      "Loss: 0.001675\n",
      "Loss: 0.001606\n",
      "Loss: 0.001685\n",
      "Loss: 0.001566\n",
      "Loss: 0.001623\n",
      "Loss: 0.001809\n",
      "Loss: 0.001620\n",
      "Loss: 0.001544\n",
      "Loss: 0.001688\n",
      "Loss: 0.001919\n",
      "Loss: 0.001357\n",
      "Loss: 0.001514\n",
      "Loss: 0.002018\n",
      "Loss: 0.001786\n",
      "Loss: 0.001740\n",
      "Loss: 0.001371\n",
      "Loss: 0.001903\n",
      "Loss: 0.001721\n",
      "Loss: 0.001857\n",
      "Loss: 0.001759\n",
      "Loss: 0.001410\n",
      "Loss: 0.001646\n",
      "Loss: 0.001781\n",
      "Loss: 0.001588\n",
      "Loss: 0.001511\n",
      "Loss: 0.002024\n",
      "Loss: 0.001652\n",
      "Loss: 0.001601\n",
      "Loss: 0.001981\n",
      "Loss: 0.001891\n",
      "Loss: 0.001697\n",
      "Loss: 0.001692\n",
      "Loss: 0.001524\n",
      "Loss: 0.001508\n",
      "Loss: 0.001957\n",
      "Loss: 0.001426\n",
      "Loss: 0.001749\n",
      "Loss: 0.001813\n",
      "Loss: 0.001858\n",
      "Loss: 0.001499\n",
      "Loss: 0.002183\n",
      "Loss: 0.001439\n",
      "Loss: 0.001699\n",
      "Loss: 0.001576\n",
      "Loss: 0.001862\n",
      "Loss: 0.001325\n",
      "Loss: 0.001188\n",
      "Loss: 0.001449\n",
      "Loss: 0.001390\n",
      "Loss: 0.001497\n",
      "Loss: 0.001530\n",
      "Loss: 0.001390\n",
      "Loss: 0.001370\n",
      "Loss: 0.001805\n",
      "Loss: 0.001489\n",
      "Loss: 0.001525\n",
      "Loss: 0.001678\n",
      "Loss: 0.001741\n",
      "Loss: 0.001595\n",
      "Loss: 0.001687\n",
      "Loss: 0.001429\n",
      "Loss: 0.001506\n",
      "Loss: 0.001616\n",
      "Loss: 0.001310\n",
      "Loss: 0.001240\n",
      "Loss: 0.001054\n",
      "Loss: 0.001850\n",
      "Loss: 0.001721\n",
      "Loss: 0.001917\n",
      "Loss: 0.001528\n",
      "Loss: 0.001143\n",
      "Loss: 0.001505\n",
      "Loss: 0.001721\n",
      "Loss: 0.001307\n",
      "Loss: 0.001588\n",
      "Loss: 0.001284\n",
      "Loss: 0.001308\n",
      "Loss: 0.001333\n",
      "Loss: 0.001634\n",
      "Loss: 0.001542\n",
      "Loss: 0.001504\n",
      "Loss: 0.001251\n",
      "Loss: 0.001531\n",
      "Loss: 0.001092\n",
      "Loss: 0.001466\n",
      "Loss: 0.001627\n",
      "Loss: 0.001452\n",
      "Loss: 0.001517\n",
      "Loss: 0.001403\n",
      "Loss: 0.001341\n",
      "Loss: 0.001441\n",
      "Loss: 0.001207\n",
      "Loss: 0.001797\n",
      "Loss: 0.001441\n",
      "Loss: 0.001381\n",
      "Loss: 0.001358\n",
      "Loss: 0.001596\n",
      "Loss: 0.001345\n",
      "Loss: 0.001469\n",
      "Loss: 0.001461\n",
      "Loss: 0.001034\n",
      "Loss: 0.001293\n",
      "Loss: 0.001175\n",
      "Loss: 0.001553\n",
      "Loss: 0.001377\n",
      "Loss: 0.001120\n",
      "Loss: 0.001224\n",
      "Loss: 0.001246\n",
      "Loss: 0.000896\n",
      "Loss: 0.001289\n",
      "Loss: 0.001249\n",
      "Loss: 0.001020\n",
      "Loss: 0.001169\n",
      "Loss: 0.001282\n",
      "Trained on 1 - Batch 1\n",
      "Loss: 0.001281\n",
      "Loss: 0.001631\n",
      "Loss: 0.001369\n",
      "Loss: 0.001113\n",
      "Loss: 0.001649\n",
      "Loss: 0.001671\n",
      "Loss: 0.001107\n",
      "Loss: 0.001208\n",
      "Loss: 0.001357\n",
      "Loss: 0.001458\n",
      "Loss: 0.001107\n",
      "Loss: 0.001217\n",
      "Loss: 0.001267\n",
      "Loss: 0.001005\n",
      "Loss: 0.001212\n",
      "Loss: 0.001405\n",
      "Loss: 0.001270\n",
      "Loss: 0.001037\n",
      "Loss: 0.001581\n",
      "Loss: 0.001177\n",
      "Loss: 0.001232\n",
      "Loss: 0.000935\n",
      "Loss: 0.001024\n",
      "Loss: 0.000844\n",
      "Loss: 0.001171\n",
      "Loss: 0.001134\n",
      "Loss: 0.001102\n",
      "Loss: 0.001018\n",
      "Loss: 0.000881\n",
      "Loss: 0.001290\n",
      "Loss: 0.001344\n",
      "Loss: 0.000957\n",
      "Loss: 0.001172\n",
      "Loss: 0.000957\n",
      "Loss: 0.001320\n",
      "Loss: 0.001591\n",
      "Loss: 0.001375\n",
      "Loss: 0.001152\n",
      "Loss: 0.001267\n",
      "Loss: 0.000885\n",
      "Loss: 0.001112\n",
      "Loss: 0.000994\n",
      "Loss: 0.001068\n",
      "Loss: 0.001019\n",
      "Loss: 0.001251\n",
      "Loss: 0.001109\n",
      "Loss: 0.001025\n",
      "Loss: 0.000994\n",
      "Loss: 0.001089\n",
      "Loss: 0.000895\n",
      "Loss: 0.001137\n",
      "Loss: 0.000726\n",
      "Loss: 0.000801\n",
      "Loss: 0.001189\n",
      "Loss: 0.001377\n",
      "Loss: 0.000932\n",
      "Loss: 0.000911\n",
      "Loss: 0.001044\n",
      "Loss: 0.001199\n",
      "Loss: 0.001050\n",
      "Loss: 0.001098\n",
      "Loss: 0.001119\n",
      "Loss: 0.001179\n",
      "Loss: 0.001198\n",
      "Loss: 0.000949\n",
      "Loss: 0.001186\n",
      "Loss: 0.001003\n",
      "Loss: 0.001114\n",
      "Loss: 0.000984\n",
      "Loss: 0.000976\n",
      "Loss: 0.000968\n",
      "Loss: 0.000882\n",
      "Loss: 0.000900\n",
      "Loss: 0.001027\n",
      "Loss: 0.000864\n",
      "Loss: 0.001121\n",
      "Loss: 0.000982\n",
      "Loss: 0.000989\n",
      "Loss: 0.001193\n",
      "Loss: 0.001701\n",
      "Loss: 0.001245\n",
      "Loss: 0.001026\n",
      "Loss: 0.001054\n",
      "Loss: 0.001458\n",
      "Loss: 0.001095\n",
      "Loss: 0.000890\n",
      "Loss: 0.001494\n",
      "Loss: 0.000780\n",
      "Loss: 0.000896\n",
      "Loss: 0.001186\n",
      "Loss: 0.000827\n",
      "Loss: 0.000904\n",
      "Loss: 0.000974\n",
      "Loss: 0.000936\n",
      "Loss: 0.001169\n",
      "Loss: 0.000879\n",
      "Loss: 0.000599\n",
      "Loss: 0.000988\n",
      "Loss: 0.000797\n",
      "Loss: 0.000977\n",
      "Loss: 0.000990\n",
      "Loss: 0.001032\n",
      "Loss: 0.001031\n",
      "Loss: 0.000978\n",
      "Loss: 0.001157\n",
      "Loss: 0.001028\n",
      "Loss: 0.000881\n",
      "Loss: 0.001122\n",
      "Loss: 0.000894\n",
      "Loss: 0.001102\n",
      "Loss: 0.000950\n",
      "Loss: 0.001084\n",
      "Loss: 0.001053\n",
      "Loss: 0.000859\n",
      "Loss: 0.001058\n",
      "Loss: 0.000871\n",
      "Loss: 0.001191\n",
      "Loss: 0.000972\n",
      "Loss: 0.000732\n",
      "Loss: 0.001209\n",
      "Loss: 0.001088\n",
      "Loss: 0.000837\n",
      "Loss: 0.000884\n",
      "Loss: 0.000821\n",
      "Loss: 0.000992\n",
      "Loss: 0.001031\n",
      "Loss: 0.000580\n",
      "Loss: 0.000679\n",
      "Loss: 0.000889\n",
      "Loss: 0.000903\n",
      "Loss: 0.000703\n",
      "Loss: 0.000924\n",
      "Loss: 0.000949\n",
      "Loss: 0.000929\n",
      "Loss: 0.000884\n",
      "Loss: 0.000503\n",
      "Loss: 0.000869\n",
      "Loss: 0.001362\n",
      "Loss: 0.000704\n",
      "Loss: 0.001254\n",
      "Loss: 0.000694\n",
      "Loss: 0.000614\n",
      "Loss: 0.000782\n",
      "Loss: 0.000729\n",
      "Loss: 0.000646\n",
      "Loss: 0.000924\n",
      "Loss: 0.000825\n",
      "Loss: 0.000875\n",
      "Loss: 0.000934\n",
      "Loss: 0.000684\n",
      "Loss: 0.000728\n",
      "Loss: 0.001027\n",
      "Loss: 0.000758\n",
      "Loss: 0.001086\n",
      "Loss: 0.000522\n",
      "Loss: 0.000775\n",
      "Loss: 0.000716\n",
      "Loss: 0.000920\n",
      "Loss: 0.000775\n",
      "Loss: 0.000734\n",
      "Loss: 0.000680\n",
      "Loss: 0.000996\n",
      "Loss: 0.001094\n",
      "Loss: 0.000992\n",
      "Loss: 0.000829\n",
      "Loss: 0.000854\n",
      "Loss: 0.001105\n",
      "Loss: 0.000555\n",
      "Loss: 0.000755\n",
      "Loss: 0.000710\n",
      "Loss: 0.000870\n",
      "Loss: 0.000654\n",
      "Loss: 0.000728\n",
      "Loss: 0.000558\n",
      "Loss: 0.000867\n",
      "Loss: 0.000931\n",
      "Loss: 0.000813\n",
      "Loss: 0.001162\n",
      "Loss: 0.000880\n",
      "Loss: 0.000625\n",
      "Loss: 0.000751\n",
      "Loss: 0.000464\n",
      "Loss: 0.000907\n",
      "Loss: 0.000649\n",
      "Loss: 0.000611\n",
      "Loss: 0.000717\n",
      "Loss: 0.000869\n",
      "Loss: 0.000608\n",
      "Loss: 0.000775\n",
      "Loss: 0.000676\n",
      "Loss: 0.000765\n",
      "Loss: 0.000521\n",
      "Loss: 0.000861\n",
      "Loss: 0.000599\n",
      "Loss: 0.001253\n",
      "Loss: 0.001007\n",
      "Loss: 0.000739\n",
      "Loss: 0.000887\n",
      "Loss: 0.000581\n",
      "Loss: 0.000695\n",
      "Loss: 0.001186\n",
      "Loss: 0.000699\n",
      "Loss: 0.000640\n",
      "Loss: 0.000766\n",
      "Loss: 0.000693\n",
      "Loss: 0.000793\n",
      "Loss: 0.000789\n",
      "Loss: 0.000782\n",
      "Loss: 0.000880\n",
      "Loss: 0.000785\n",
      "Loss: 0.000789\n",
      "Loss: 0.000585\n",
      "Loss: 0.000640\n",
      "Loss: 0.000561\n",
      "Loss: 0.000867\n",
      "Loss: 0.000460\n",
      "Loss: 0.000835\n",
      "Loss: 0.000466\n",
      "Loss: 0.000464\n",
      "Loss: 0.000550\n",
      "Loss: 0.000821\n",
      "Loss: 0.000905\n",
      "Loss: 0.000785\n",
      "Loss: 0.000705\n",
      "Loss: 0.000444\n",
      "Loss: 0.000721\n",
      "Loss: 0.000354\n",
      "Loss: 0.000330\n",
      "Loss: 0.000569\n",
      "Loss: 0.000855\n",
      "Loss: 0.000818\n",
      "Loss: 0.000841\n",
      "Loss: 0.000449\n",
      "Loss: 0.000928\n",
      "Loss: 0.000640\n",
      "Loss: 0.000499\n",
      "Loss: 0.000581\n",
      "Loss: 0.000595\n",
      "Loss: 0.000526\n",
      "Loss: 0.000625\n",
      "Loss: 0.000607\n",
      "Loss: 0.000365\n",
      "Loss: 0.000589\n",
      "Loss: 0.000648\n",
      "Loss: 0.000626\n",
      "Loss: 0.000569\n",
      "Loss: 0.000528\n",
      "Loss: 0.000502\n",
      "Loss: 0.000656\n",
      "Trained on 1 - Batch 2\n",
      "Loading train file 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000618\n",
      "Loss: 0.000613\n",
      "Loss: 0.000464\n",
      "Loss: 0.000531\n",
      "Loss: 0.000606\n",
      "Loss: 0.000497\n",
      "Loss: 0.000401\n",
      "Loss: 0.000317\n",
      "Loss: 0.000569\n",
      "Loss: 0.000355\n",
      "Loss: 0.000973\n",
      "Loss: 0.000655\n",
      "Loss: 0.000669\n",
      "Loss: 0.000360\n",
      "Loss: 0.000798\n",
      "Loss: 0.000754\n",
      "Loss: 0.000733\n",
      "Loss: 0.000693\n",
      "Loss: 0.000517\n",
      "Loss: 0.000368\n",
      "Loss: 0.000671\n",
      "Loss: 0.000597\n",
      "Loss: 0.000614\n",
      "Loss: 0.000736\n",
      "Loss: 0.000827\n",
      "Loss: 0.000667\n",
      "Loss: 0.000728\n",
      "Loss: 0.001246\n",
      "Loss: 0.000367\n",
      "Loss: 0.000556\n",
      "Loss: 0.000759\n",
      "Loss: 0.000590\n",
      "Loss: 0.000571\n",
      "Loss: 0.000471\n",
      "Loss: 0.000973\n",
      "Loss: 0.000403\n",
      "Loss: 0.000639\n",
      "Loss: 0.000759\n",
      "Loss: 0.000363\n",
      "Loss: 0.000876\n",
      "Loss: 0.000376\n",
      "Loss: 0.000571\n",
      "Loss: 0.000666\n",
      "Loss: 0.000659\n",
      "Loss: 0.000674\n",
      "Loss: 0.000567\n",
      "Loss: 0.000755\n",
      "Loss: 0.000466\n",
      "Loss: 0.000855\n",
      "Loss: 0.000476\n",
      "Loss: 0.000656\n",
      "Loss: 0.000522\n",
      "Loss: 0.000685\n",
      "Loss: 0.000512\n",
      "Loss: 0.000616\n",
      "Loss: 0.000504\n",
      "Loss: 0.000530\n",
      "Loss: 0.000408\n",
      "Loss: 0.000527\n",
      "Loss: 0.000391\n",
      "Loss: 0.000731\n",
      "Loss: 0.000658\n",
      "Loss: 0.000607\n",
      "Loss: 0.000585\n",
      "Loss: 0.000388\n",
      "Loss: 0.000599\n",
      "Loss: 0.000963\n",
      "Loss: 0.000649\n",
      "Loss: 0.000568\n",
      "Loss: 0.000356\n",
      "Loss: 0.000553\n",
      "Loss: 0.000621\n",
      "Loss: 0.000541\n",
      "Loss: 0.000578\n",
      "Loss: 0.000584\n",
      "Loss: 0.000426\n",
      "Loss: 0.000289\n",
      "Loss: 0.000287\n",
      "Loss: 0.000341\n",
      "Loss: 0.000504\n",
      "Loss: 0.000488\n",
      "Loss: 0.000597\n",
      "Loss: 0.000575\n",
      "Loss: 0.000531\n",
      "Loss: 0.000427\n",
      "Loss: 0.000413\n",
      "Loss: 0.000590\n",
      "Loss: 0.000512\n",
      "Loss: 0.000629\n",
      "Loss: 0.000339\n",
      "Loss: 0.000612\n",
      "Loss: 0.000704\n",
      "Loss: 0.000390\n",
      "Loss: 0.000677\n",
      "Loss: 0.000997\n",
      "Loss: 0.000477\n",
      "Loss: 0.000554\n",
      "Loss: 0.001296\n",
      "Loss: 0.000499\n",
      "Loss: 0.000867\n",
      "Loss: 0.000725\n",
      "Loss: 0.000472\n",
      "Loss: 0.000658\n",
      "Loss: 0.000389\n",
      "Loss: 0.000420\n",
      "Loss: 0.000437\n",
      "Loss: 0.000530\n",
      "Loss: 0.000456\n",
      "Loss: 0.000483\n",
      "Loss: 0.000361\n",
      "Loss: 0.000503\n",
      "Loss: 0.000453\n",
      "Loss: 0.000521\n",
      "Loss: 0.000519\n",
      "Loss: 0.000577\n",
      "Loss: 0.000676\n",
      "Loss: 0.000389\n",
      "Loss: 0.000889\n",
      "Loss: 0.000347\n",
      "Loss: 0.000437\n",
      "Loss: 0.000267\n",
      "Loss: 0.000268\n",
      "Loss: 0.000479\n",
      "Loss: 0.000362\n",
      "Loss: 0.000306\n",
      "Loss: 0.000480\n",
      "Loss: 0.000507\n",
      "Loss: 0.000428\n",
      "Loss: 0.000299\n",
      "Loss: 0.000471\n",
      "Loss: 0.000522\n",
      "Loss: 0.000572\n",
      "Loss: 0.000594\n",
      "Loss: 0.000581\n",
      "Loss: 0.000550\n",
      "Loss: 0.000349\n",
      "Loss: 0.000449\n",
      "Loss: 0.000517\n",
      "Loss: 0.000351\n",
      "Loss: 0.000283\n",
      "Loss: 0.000371\n",
      "Loss: 0.000373\n",
      "Loss: 0.000429\n",
      "Loss: 0.000315\n",
      "Loss: 0.000324\n",
      "Loss: 0.000415\n",
      "Loss: 0.000470\n",
      "Loss: 0.000223\n",
      "Loss: 0.000355\n",
      "Loss: 0.000332\n",
      "Loss: 0.000341\n",
      "Loss: 0.000403\n",
      "Loss: 0.000500\n",
      "Loss: 0.000449\n",
      "Loss: 0.000319\n",
      "Loss: 0.000156\n",
      "Loss: 0.000308\n",
      "Loss: 0.000390\n",
      "Loss: 0.000217\n",
      "Loss: 0.000646\n",
      "Loss: 0.000495\n",
      "Loss: 0.000378\n",
      "Loss: 0.000264\n",
      "Loss: 0.000309\n",
      "Loss: 0.000602\n",
      "Loss: 0.000385\n",
      "Loss: 0.000403\n",
      "Loss: 0.000412\n",
      "Loss: 0.000213\n",
      "Loss: 0.000252\n",
      "Loss: 0.000511\n",
      "Loss: 0.000320\n",
      "Loss: 0.000569\n",
      "Loss: 0.000292\n",
      "Loss: 0.000310\n",
      "Loss: 0.000469\n",
      "Loss: 0.000361\n",
      "Loss: 0.000260\n",
      "Loss: 0.000492\n",
      "Loss: 0.000323\n",
      "Loss: 0.000334\n",
      "Loss: 0.000401\n",
      "Loss: 0.000331\n",
      "Loss: 0.000607\n",
      "Loss: 0.000193\n",
      "Loss: 0.000717\n",
      "Loss: 0.000370\n",
      "Loss: 0.000212\n",
      "Loss: 0.000290\n",
      "Loss: 0.000663\n",
      "Loss: 0.000427\n",
      "Loss: 0.000267\n",
      "Loss: 0.000255\n",
      "Loss: 0.000261\n",
      "Loss: 0.000228\n",
      "Loss: 0.000256\n",
      "Loss: 0.000252\n",
      "Loss: 0.000238\n",
      "Loss: 0.000359\n",
      "Loss: 0.000261\n",
      "Loss: 0.000387\n",
      "Loss: 0.000133\n",
      "Loss: 0.000458\n",
      "Loss: 0.000808\n",
      "Loss: 0.000323\n",
      "Loss: 0.000596\n",
      "Loss: 0.000391\n",
      "Loss: 0.000349\n",
      "Loss: 0.000334\n",
      "Loss: 0.000735\n",
      "Loss: 0.000342\n",
      "Loss: 0.000369\n",
      "Loss: 0.000808\n",
      "Loss: 0.000262\n",
      "Loss: 0.000496\n",
      "Loss: 0.000539\n",
      "Loss: 0.000269\n",
      "Loss: 0.000282\n",
      "Loss: 0.000359\n",
      "Loss: 0.000334\n",
      "Loss: 0.000720\n",
      "Loss: 0.000256\n",
      "Loss: 0.000420\n",
      "Loss: 0.000228\n",
      "Loss: 0.000528\n",
      "Loss: 0.000311\n",
      "Loss: 0.000507\n",
      "Loss: 0.000345\n",
      "Loss: 0.000311\n",
      "Loss: 0.000359\n",
      "Loss: 0.000392\n",
      "Loss: 0.000868\n",
      "Loss: 0.000392\n",
      "Loss: 0.000304\n",
      "Loss: 0.000395\n",
      "Loss: 0.000229\n",
      "Loss: 0.000417\n",
      "Loss: 0.000261\n",
      "Loss: 0.000429\n",
      "Loss: 0.000504\n",
      "Loss: 0.000284\n",
      "Loss: 0.000282\n",
      "Loss: 0.000523\n",
      "Loss: 0.000233\n",
      "Loss: 0.000235\n",
      "Loss: 0.000438\n",
      "Loss: 0.000573\n",
      "Loss: 0.000429\n",
      "Loss: 0.000187\n",
      "Loss: 0.000666\n",
      "Loss: 0.000469\n",
      "Loss: 0.000249\n",
      "Loss: 0.000306\n",
      "Loss: 0.000512\n",
      "Loss: 0.000652\n",
      "Loss: 0.000435\n",
      "Loss: 0.000341\n",
      "Loss: 0.000645\n",
      "Loss: 0.000430\n",
      "Loss: 0.000256\n",
      "Loss: 0.000210\n",
      "Loss: 0.000232\n",
      "Loss: 0.000669\n",
      "Loss: 0.000292\n",
      "Loss: 0.000506\n",
      "Loss: 0.000431\n",
      "Loss: 0.000277\n",
      "Loss: 0.000290\n",
      "Loss: 0.000570\n",
      "Loss: 0.000307\n",
      "Loss: 0.000282\n",
      "Loss: 0.000392\n",
      "Loss: 0.000427\n",
      "Loss: 0.000236\n",
      "Loss: 0.000200\n",
      "Loss: 0.000518\n",
      "Loss: 0.000297\n",
      "Loss: 0.000285\n",
      "Loss: 0.000285\n",
      "Loss: 0.000291\n",
      "Loss: 0.000478\n",
      "Loss: 0.000263\n",
      "Loss: 0.000417\n",
      "Loss: 0.000411\n",
      "Loss: 0.000455\n",
      "Loss: 0.000430\n",
      "Loss: 0.000492\n",
      "Loss: 0.000128\n",
      "Loss: 0.000206\n",
      "Loss: 0.000419\n",
      "Loss: 0.000285\n",
      "Loss: 0.000216\n",
      "Loss: 0.000328\n",
      "Loss: 0.000278\n",
      "Loss: 0.000520\n",
      "Loss: 0.000449\n",
      "Loss: 0.000639\n",
      "Loss: 0.000225\n",
      "Loss: 0.000272\n",
      "Loss: 0.000390\n",
      "Trained on 2 - Batch 1\n",
      "Loss: 0.000327\n",
      "Loss: 0.000316\n",
      "Loss: 0.000302\n",
      "Loss: 0.000164\n",
      "Loss: 0.000170\n",
      "Loss: 0.000167\n",
      "Loss: 0.000192\n",
      "Loss: 0.000398\n",
      "Loss: 0.000210\n",
      "Loss: 0.000267\n",
      "Loss: 0.000253\n",
      "Loss: 0.000296\n",
      "Loss: 0.000291\n",
      "Loss: 0.000544\n",
      "Loss: 0.000361\n",
      "Loss: 0.000267\n",
      "Loss: 0.000487\n",
      "Loss: 0.000251\n",
      "Loss: 0.000434\n",
      "Loss: 0.000408\n",
      "Loss: 0.000350\n",
      "Loss: 0.000395\n",
      "Loss: 0.000563\n",
      "Loss: 0.000695\n",
      "Loss: 0.000365\n",
      "Loss: 0.000326\n",
      "Loss: 0.000562\n",
      "Loss: 0.000273\n",
      "Loss: 0.000293\n",
      "Loss: 0.000254\n",
      "Loss: 0.000233\n",
      "Loss: 0.000285\n",
      "Loss: 0.000440\n",
      "Loss: 0.000312\n",
      "Loss: 0.000226\n",
      "Loss: 0.000383\n",
      "Loss: 0.000529\n",
      "Loss: 0.000312\n",
      "Loss: 0.000475\n",
      "Loss: 0.000486\n",
      "Loss: 0.000428\n",
      "Loss: 0.000415\n",
      "Loss: 0.000300\n",
      "Loss: 0.000256\n",
      "Loss: 0.000455\n",
      "Loss: 0.000138\n",
      "Loss: 0.000190\n",
      "Loss: 0.000734\n",
      "Loss: 0.000464\n",
      "Loss: 0.000304\n",
      "Loss: 0.000212\n",
      "Loss: 0.000496\n",
      "Loss: 0.000237\n",
      "Loss: 0.000179\n",
      "Loss: 0.000352\n",
      "Loss: 0.000240\n",
      "Loss: 0.000395\n",
      "Loss: 0.000198\n",
      "Loss: 0.000359\n",
      "Loss: 0.000326\n",
      "Loss: 0.000321\n",
      "Loss: 0.000383\n",
      "Loss: 0.000284\n",
      "Loss: 0.000837\n",
      "Loss: 0.000240\n",
      "Loss: 0.000259\n",
      "Loss: 0.000529\n",
      "Loss: 0.000383\n",
      "Loss: 0.000254\n",
      "Loss: 0.000339\n",
      "Loss: 0.000230\n",
      "Loss: 0.000392\n",
      "Loss: 0.000194\n",
      "Loss: 0.000187\n",
      "Loss: 0.000586\n",
      "Loss: 0.000446\n",
      "Loss: 0.000287\n",
      "Loss: 0.000209\n",
      "Loss: 0.000200\n",
      "Loss: 0.000190\n",
      "Loss: 0.000209\n",
      "Loss: 0.000472\n",
      "Loss: 0.000422\n",
      "Loss: 0.000346\n",
      "Loss: 0.000235\n",
      "Loss: 0.000234\n",
      "Loss: 0.000350\n",
      "Loss: 0.000376\n",
      "Loss: 0.000369\n",
      "Loss: 0.000409\n",
      "Loss: 0.000242\n",
      "Loss: 0.000312\n",
      "Loss: 0.000238\n",
      "Loss: 0.000463\n",
      "Loss: 0.000376\n",
      "Loss: 0.000155\n",
      "Loss: 0.000230\n",
      "Loss: 0.000267\n",
      "Loss: 0.000152\n",
      "Loss: 0.000295\n",
      "Loss: 0.000406\n",
      "Loss: 0.000181\n",
      "Loss: 0.000281\n",
      "Loss: 0.000238\n",
      "Loss: 0.000153\n",
      "Loss: 0.000373\n",
      "Loss: 0.000380\n",
      "Loss: 0.000378\n",
      "Loss: 0.000198\n",
      "Loss: 0.000263\n",
      "Loss: 0.000367\n",
      "Loss: 0.000378\n",
      "Loss: 0.000348\n",
      "Loss: 0.000314\n",
      "Loss: 0.000297\n",
      "Loss: 0.000134\n",
      "Loss: 0.000297\n",
      "Loss: 0.000479\n",
      "Loss: 0.000146\n",
      "Loss: 0.000156\n",
      "Loss: 0.000167\n",
      "Loss: 0.000314\n",
      "Loss: 0.000443\n",
      "Loss: 0.000138\n",
      "Loss: 0.000486\n",
      "Loss: 0.000149\n",
      "Loss: 0.000555\n",
      "Loss: 0.000220\n",
      "Loss: 0.000284\n",
      "Loss: 0.000289\n",
      "Loss: 0.000147\n",
      "Loss: 0.000611\n",
      "Loss: 0.000529\n",
      "Loss: 0.000404\n",
      "Loss: 0.000853\n",
      "Loss: 0.000351\n",
      "Loss: 0.000338\n",
      "Loss: 0.000290\n",
      "Loss: 0.000428\n",
      "Loss: 0.000318\n",
      "Loss: 0.000127\n",
      "Loss: 0.000206\n",
      "Loss: 0.000141\n",
      "Loss: 0.000349\n",
      "Loss: 0.000303\n",
      "Loss: 0.000195\n",
      "Loss: 0.000372\n",
      "Loss: 0.000249\n",
      "Loss: 0.000253\n",
      "Loss: 0.000170\n",
      "Loss: 0.000251\n",
      "Loss: 0.000206\n",
      "Loss: 0.000200\n",
      "Loss: 0.000347\n",
      "Loss: 0.000235\n",
      "Loss: 0.000224\n",
      "Loss: 0.000248\n",
      "Loss: 0.000239\n",
      "Loss: 0.000187\n",
      "Loss: 0.000424\n",
      "Loss: 0.000142\n",
      "Loss: 0.000760\n",
      "Loss: 0.000225\n",
      "Loss: 0.000142\n",
      "Loss: 0.000218\n",
      "Loss: 0.000265\n",
      "Loss: 0.000311\n",
      "Loss: 0.000144\n",
      "Loss: 0.000276\n",
      "Loss: 0.000170\n",
      "Loss: 0.000318\n",
      "Loss: 0.000401\n",
      "Loss: 0.000115\n",
      "Loss: 0.000325\n",
      "Loss: 0.000272\n",
      "Loss: 0.000138\n",
      "Loss: 0.000247\n",
      "Loss: 0.000276\n",
      "Loss: 0.000395\n",
      "Loss: 0.000249\n",
      "Loss: 0.000422\n",
      "Loss: 0.000432\n",
      "Loss: 0.000170\n",
      "Loss: 0.000332\n",
      "Loss: 0.000498\n",
      "Loss: 0.000469\n",
      "Loss: 0.000515\n",
      "Loss: 0.000220\n",
      "Loss: 0.000160\n",
      "Loss: 0.000369\n",
      "Loss: 0.000379\n",
      "Loss: 0.000396\n",
      "Loss: 0.000137\n",
      "Loss: 0.000356\n",
      "Loss: 0.000208\n",
      "Loss: 0.000194\n",
      "Loss: 0.000234\n",
      "Loss: 0.000596\n",
      "Loss: 0.000122\n",
      "Loss: 0.000365\n",
      "Loss: 0.000231\n",
      "Loss: 0.000245\n",
      "Loss: 0.000115\n",
      "Loss: 0.000317\n",
      "Loss: 0.000175\n",
      "Loss: 0.000212\n",
      "Loss: 0.000162\n",
      "Loss: 0.000082\n",
      "Loss: 0.000276\n",
      "Loss: 0.000391\n",
      "Loss: 0.000118\n",
      "Loss: 0.000242\n",
      "Loss: 0.000380\n",
      "Loss: 0.000114\n",
      "Loss: 0.000404\n",
      "Loss: 0.000247\n",
      "Loss: 0.000257\n",
      "Loss: 0.000405\n",
      "Loss: 0.000232\n",
      "Loss: 0.000333\n",
      "Loss: 0.000330\n",
      "Loss: 0.000301\n",
      "Loss: 0.000164\n",
      "Loss: 0.000323\n",
      "Loss: 0.000292\n",
      "Loss: 0.000287\n",
      "Loss: 0.000383\n",
      "Loss: 0.000301\n",
      "Loss: 0.000188\n",
      "Loss: 0.000350\n",
      "Loss: 0.000240\n",
      "Loss: 0.000422\n",
      "Loss: 0.000379\n",
      "Loss: 0.000299\n",
      "Loss: 0.000209\n",
      "Loss: 0.000400\n",
      "Loss: 0.000120\n",
      "Loss: 0.000503\n",
      "Loss: 0.000418\n",
      "Loss: 0.000277\n",
      "Loss: 0.000155\n",
      "Loss: 0.000276\n",
      "Loss: 0.000228\n",
      "Loss: 0.000230\n",
      "Loss: 0.000436\n",
      "Loss: 0.000372\n",
      "Loss: 0.000524\n",
      "Loss: 0.000165\n",
      "Loss: 0.000460\n",
      "Trained on 2 - Batch 2\n",
      "Loading train file 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000269\n",
      "Loss: 0.000247\n",
      "Loss: 0.000303\n",
      "Loss: 0.000403\n",
      "Loss: 0.000181\n",
      "Loss: 0.000311\n",
      "Loss: 0.000367\n",
      "Loss: 0.000320\n",
      "Loss: 0.000403\n",
      "Loss: 0.000360\n",
      "Loss: 0.000271\n",
      "Loss: 0.000369\n",
      "Loss: 0.000211\n",
      "Loss: 0.000227\n",
      "Loss: 0.000269\n",
      "Loss: 0.000533\n",
      "Loss: 0.000207\n",
      "Loss: 0.000290\n",
      "Loss: 0.000495\n",
      "Loss: 0.000447\n",
      "Loss: 0.000227\n",
      "Loss: 0.000225\n",
      "Loss: 0.000353\n",
      "Loss: 0.000363\n",
      "Loss: 0.000239\n",
      "Loss: 0.000337\n",
      "Loss: 0.000299\n",
      "Loss: 0.000105\n",
      "Loss: 0.000124\n",
      "Loss: 0.000150\n",
      "Loss: 0.000358\n",
      "Loss: 0.000356\n",
      "Loss: 0.000281\n",
      "Loss: 0.000388\n",
      "Loss: 0.000126\n",
      "Loss: 0.000254\n",
      "Loss: 0.000187\n",
      "Loss: 0.000181\n",
      "Loss: 0.000393\n",
      "Loss: 0.000050\n",
      "Loss: 0.000273\n",
      "Loss: 0.000264\n",
      "Loss: 0.000282\n",
      "Loss: 0.000149\n",
      "Loss: 0.000167\n",
      "Loss: 0.000268\n",
      "Loss: 0.000197\n",
      "Loss: 0.000108\n",
      "Loss: 0.000082\n",
      "Loss: 0.000235\n",
      "Loss: 0.000233\n",
      "Loss: 0.000142\n",
      "Loss: 0.000316\n",
      "Loss: 0.000480\n",
      "Loss: 0.000132\n",
      "Loss: 0.000199\n",
      "Loss: 0.000119\n",
      "Loss: 0.000318\n",
      "Loss: 0.000198\n",
      "Loss: 0.000424\n",
      "Loss: 0.000140\n",
      "Loss: 0.000246\n",
      "Loss: 0.000359\n",
      "Loss: 0.000367\n",
      "Loss: 0.000163\n",
      "Loss: 0.000334\n",
      "Loss: 0.000383\n",
      "Loss: 0.000273\n",
      "Loss: 0.000205\n",
      "Loss: 0.000201\n",
      "Loss: 0.000285\n",
      "Loss: 0.000218\n",
      "Loss: 0.000509\n",
      "Loss: 0.000260\n",
      "Loss: 0.000328\n",
      "Loss: 0.000705\n",
      "Loss: 0.000329\n",
      "Loss: 0.000168\n",
      "Loss: 0.000232\n",
      "Loss: 0.000527\n",
      "Loss: 0.000259\n",
      "Loss: 0.000195\n",
      "Loss: 0.000528\n",
      "Loss: 0.000321\n",
      "Loss: 0.000289\n",
      "Loss: 0.000154\n",
      "Loss: 0.000184\n",
      "Loss: 0.000066\n",
      "Loss: 0.000177\n",
      "Loss: 0.000289\n",
      "Loss: 0.000283\n",
      "Loss: 0.000250\n",
      "Loss: 0.000310\n",
      "Loss: 0.000253\n",
      "Loss: 0.000226\n",
      "Loss: 0.000628\n",
      "Loss: 0.000209\n",
      "Loss: 0.000127\n",
      "Loss: 0.000263\n",
      "Loss: 0.000419\n",
      "Loss: 0.000338\n",
      "Loss: 0.000180\n",
      "Loss: 0.000189\n",
      "Loss: 0.000168\n",
      "Loss: 0.000190\n",
      "Loss: 0.000214\n",
      "Loss: 0.000270\n",
      "Loss: 0.000296\n",
      "Loss: 0.000225\n",
      "Loss: 0.000261\n",
      "Loss: 0.000188\n",
      "Loss: 0.000316\n",
      "Loss: 0.000280\n",
      "Loss: 0.000411\n",
      "Loss: 0.000339\n",
      "Loss: 0.000241\n",
      "Loss: 0.000297\n",
      "Loss: 0.000283\n",
      "Loss: 0.000156\n",
      "Loss: 0.000269\n",
      "Loss: 0.000384\n",
      "Loss: 0.000471\n",
      "Loss: 0.000066\n",
      "Loss: 0.000249\n",
      "Loss: 0.000334\n",
      "Loss: 0.000199\n",
      "Loss: 0.000399\n",
      "Loss: 0.000080\n",
      "Loss: 0.000284\n",
      "Loss: 0.000305\n",
      "Loss: 0.000234\n",
      "Loss: 0.000255\n",
      "Loss: 0.000118\n",
      "Loss: 0.000521\n",
      "Loss: 0.000506\n",
      "Loss: 0.000437\n",
      "Loss: 0.000246\n",
      "Loss: 0.000175\n",
      "Loss: 0.000297\n",
      "Loss: 0.000097\n",
      "Loss: 0.000297\n",
      "Loss: 0.000281\n",
      "Loss: 0.000499\n",
      "Loss: 0.000142\n",
      "Loss: 0.000159\n",
      "Loss: 0.000318\n",
      "Loss: 0.000220\n",
      "Loss: 0.000369\n",
      "Loss: 0.000120\n",
      "Loss: 0.000183\n",
      "Loss: 0.000603\n",
      "Loss: 0.000300\n",
      "Loss: 0.000422\n",
      "Loss: 0.000151\n",
      "Loss: 0.000359\n",
      "Loss: 0.000090\n",
      "Loss: 0.000462\n",
      "Loss: 0.000350\n",
      "Loss: 0.000287\n",
      "Loss: 0.000348\n",
      "Loss: 0.000285\n",
      "Loss: 0.000568\n",
      "Loss: 0.000211\n",
      "Loss: 0.000239\n",
      "Loss: 0.000474\n",
      "Loss: 0.000289\n",
      "Loss: 0.000500\n",
      "Loss: 0.000215\n",
      "Loss: 0.000500\n",
      "Loss: 0.000269\n",
      "Loss: 0.000312\n",
      "Loss: 0.000189\n",
      "Loss: 0.000801\n",
      "Loss: 0.000159\n",
      "Loss: 0.000096\n",
      "Loss: 0.000198\n",
      "Loss: 0.000344\n",
      "Loss: 0.000268\n",
      "Loss: 0.000126\n",
      "Loss: 0.000220\n",
      "Loss: 0.000261\n",
      "Loss: 0.000376\n",
      "Loss: 0.000299\n",
      "Loss: 0.000103\n",
      "Loss: 0.000162\n",
      "Loss: 0.000192\n",
      "Loss: 0.000205\n",
      "Loss: 0.000203\n",
      "Loss: 0.000116\n",
      "Loss: 0.000209\n",
      "Loss: 0.000164\n",
      "Loss: 0.000111\n",
      "Loss: 0.000114\n",
      "Loss: 0.000060\n",
      "Loss: 0.000139\n",
      "Loss: 0.000082\n",
      "Loss: 0.000211\n",
      "Loss: 0.000370\n",
      "Loss: 0.000316\n",
      "Loss: 0.000120\n",
      "Loss: 0.000117\n",
      "Loss: 0.000362\n",
      "Loss: 0.000255\n",
      "Loss: 0.000324\n",
      "Loss: 0.000108\n",
      "Loss: 0.000292\n",
      "Loss: 0.000132\n",
      "Loss: 0.000269\n",
      "Loss: 0.000108\n",
      "Loss: 0.000055\n",
      "Loss: 0.000530\n",
      "Loss: 0.000335\n",
      "Loss: 0.000039\n",
      "Loss: 0.000327\n",
      "Loss: 0.000086\n",
      "Loss: 0.000054\n",
      "Loss: 0.000155\n",
      "Loss: 0.000092\n",
      "Loss: 0.000227\n",
      "Loss: 0.000086\n",
      "Loss: 0.000154\n",
      "Loss: 0.000398\n",
      "Loss: 0.000348\n",
      "Loss: 0.000224\n",
      "Loss: 0.000186\n",
      "Loss: 0.000156\n",
      "Loss: 0.000439\n",
      "Loss: 0.000217\n",
      "Loss: 0.000194\n",
      "Loss: 0.000193\n",
      "Loss: 0.000139\n",
      "Loss: 0.000360\n",
      "Loss: 0.000042\n",
      "Loss: 0.000224\n",
      "Loss: 0.000134\n",
      "Loss: 0.000136\n",
      "Loss: 0.000198\n",
      "Loss: 0.000362\n",
      "Loss: 0.000223\n",
      "Loss: 0.000196\n",
      "Loss: 0.000303\n",
      "Loss: 0.000231\n",
      "Loss: 0.000205\n",
      "Loss: 0.000070\n",
      "Loss: 0.000141\n",
      "Loss: 0.000105\n",
      "Loss: 0.000156\n",
      "Loss: 0.000120\n",
      "Loss: 0.000272\n",
      "Loss: 0.000173\n",
      "Loss: 0.000332\n",
      "Loss: 0.000051\n",
      "Loss: 0.000242\n",
      "Loss: 0.000573\n",
      "Loss: 0.000158\n",
      "Loss: 0.000178\n",
      "Loss: 0.000284\n",
      "Loss: 0.000180\n",
      "Loss: 0.000144\n",
      "Loss: 0.000196\n",
      "Loss: 0.000301\n",
      "Loss: 0.000188\n",
      "Loss: 0.000367\n",
      "Loss: 0.000190\n",
      "Loss: 0.000193\n",
      "Loss: 0.000077\n",
      "Loss: 0.000115\n",
      "Loss: 0.000278\n",
      "Loss: 0.000325\n",
      "Loss: 0.000251\n",
      "Loss: 0.000114\n",
      "Loss: 0.000138\n",
      "Loss: 0.000125\n",
      "Loss: 0.000357\n",
      "Loss: 0.000364\n",
      "Loss: 0.000298\n",
      "Loss: 0.000164\n",
      "Loss: 0.000251\n",
      "Loss: 0.000413\n",
      "Loss: 0.000381\n",
      "Loss: 0.000501\n",
      "Loss: 0.000263\n",
      "Loss: 0.000184\n",
      "Loss: 0.000169\n",
      "Loss: 0.000225\n",
      "Loss: 0.000359\n",
      "Loss: 0.000201\n",
      "Loss: 0.000340\n",
      "Loss: 0.000393\n",
      "Loss: 0.000466\n",
      "Loss: 0.000056\n",
      "Loss: 0.000137\n",
      "Loss: 0.000251\n",
      "Loss: 0.000466\n",
      "Loss: 0.000274\n",
      "Loss: 0.000314\n",
      "Loss: 0.000281\n",
      "Loss: 0.000526\n",
      "Loss: 0.000082\n",
      "Loss: 0.000358\n",
      "Trained on 3 - Batch 1\n",
      "Loss: 0.000251\n",
      "Loss: 0.000269\n",
      "Loss: 0.000216\n",
      "Loss: 0.000347\n",
      "Loss: 0.000655\n",
      "Loss: 0.000174\n",
      "Loss: 0.000194\n",
      "Loss: 0.000177\n",
      "Loss: 0.000220\n",
      "Loss: 0.000271\n",
      "Loss: 0.000186\n",
      "Loss: 0.000221\n",
      "Loss: 0.000483\n",
      "Loss: 0.000415\n",
      "Loss: 0.000164\n",
      "Loss: 0.000089\n",
      "Loss: 0.000316\n",
      "Loss: 0.000088\n",
      "Loss: 0.000148\n",
      "Loss: 0.000276\n",
      "Loss: 0.000289\n",
      "Loss: 0.000263\n",
      "Loss: 0.000203\n",
      "Loss: 0.000089\n",
      "Loss: 0.000252\n",
      "Loss: 0.000414\n",
      "Loss: 0.000336\n",
      "Loss: 0.000375\n",
      "Loss: 0.000108\n",
      "Loss: 0.000122\n",
      "Loss: 0.000144\n",
      "Loss: 0.000306\n",
      "Loss: 0.000198\n",
      "Loss: 0.000256\n",
      "Loss: 0.000102\n",
      "Loss: 0.000132\n",
      "Loss: 0.000235\n",
      "Loss: 0.000226\n",
      "Loss: 0.000091\n",
      "Loss: 0.000215\n",
      "Loss: 0.000091\n",
      "Loss: 0.000240\n",
      "Loss: 0.000220\n",
      "Loss: 0.000335\n",
      "Loss: 0.000085\n",
      "Loss: 0.000153\n",
      "Loss: 0.000117\n",
      "Loss: 0.000260\n",
      "Loss: 0.000222\n",
      "Loss: 0.000142\n",
      "Loss: 0.000083\n",
      "Loss: 0.000113\n",
      "Loss: 0.000233\n",
      "Loss: 0.000159\n",
      "Loss: 0.000139\n",
      "Loss: 0.000343\n",
      "Loss: 0.000147\n",
      "Loss: 0.000143\n",
      "Loss: 0.000169\n",
      "Loss: 0.000242\n",
      "Loss: 0.000159\n",
      "Loss: 0.000240\n",
      "Loss: 0.000143\n",
      "Loss: 0.000286\n",
      "Loss: 0.000166\n",
      "Loss: 0.000104\n",
      "Loss: 0.000070\n",
      "Loss: 0.000114\n",
      "Loss: 0.000149\n",
      "Loss: 0.000076\n",
      "Loss: 0.000033\n",
      "Loss: 0.000219\n",
      "Loss: 0.000066\n",
      "Loss: 0.000292\n",
      "Loss: 0.000107\n",
      "Loss: 0.000115\n",
      "Loss: 0.000079\n",
      "Loss: 0.000124\n",
      "Loss: 0.000143\n",
      "Loss: 0.000079\n",
      "Loss: 0.000053\n",
      "Loss: 0.000187\n",
      "Loss: 0.000238\n",
      "Loss: 0.000222\n",
      "Loss: 0.000041\n",
      "Loss: 0.000067\n",
      "Loss: 0.000254\n",
      "Loss: 0.000108\n",
      "Loss: 0.000103\n",
      "Loss: 0.000206\n",
      "Loss: 0.000262\n",
      "Loss: 0.000085\n",
      "Loss: 0.000182\n",
      "Loss: 0.000165\n",
      "Loss: 0.000051\n",
      "Loss: 0.000077\n",
      "Loss: 0.000142\n",
      "Loss: 0.000064\n",
      "Loss: 0.000229\n",
      "Loss: 0.000117\n",
      "Loss: 0.000075\n",
      "Loss: 0.000153\n",
      "Loss: 0.000386\n",
      "Loss: 0.000059\n",
      "Loss: 0.000064\n",
      "Loss: 0.000203\n",
      "Loss: 0.000346\n",
      "Loss: 0.000156\n",
      "Loss: 0.000110\n",
      "Loss: 0.000167\n",
      "Loss: 0.000087\n",
      "Loss: 0.000154\n",
      "Loss: 0.000216\n",
      "Loss: 0.000065\n",
      "Loss: 0.000255\n",
      "Loss: 0.000078\n",
      "Loss: 0.000147\n",
      "Loss: 0.000112\n",
      "Loss: 0.000058\n",
      "Loss: 0.000085\n",
      "Loss: 0.000329\n",
      "Loss: 0.000120\n",
      "Loss: 0.000215\n",
      "Loss: 0.000363\n",
      "Loss: 0.000130\n",
      "Loss: 0.000358\n",
      "Loss: 0.000049\n",
      "Loss: 0.000048\n",
      "Loss: 0.000075\n",
      "Loss: 0.000160\n",
      "Loss: 0.000293\n",
      "Loss: 0.000153\n",
      "Loss: 0.000119\n",
      "Loss: 0.000222\n",
      "Loss: 0.000086\n",
      "Loss: 0.000126\n",
      "Loss: 0.000066\n",
      "Loss: 0.000158\n",
      "Loss: 0.000056\n",
      "Loss: 0.000253\n",
      "Loss: 0.000255\n",
      "Loss: 0.000090\n",
      "Loss: 0.000153\n",
      "Loss: 0.000069\n",
      "Loss: 0.000161\n",
      "Loss: 0.000205\n",
      "Loss: 0.000078\n",
      "Loss: 0.000069\n",
      "Loss: 0.000184\n",
      "Loss: 0.000210\n",
      "Loss: 0.000036\n",
      "Loss: 0.000281\n",
      "Loss: 0.000132\n",
      "Loss: 0.000093\n",
      "Loss: 0.000098\n",
      "Loss: 0.000053\n",
      "Loss: 0.000056\n",
      "Loss: 0.000189\n",
      "Loss: 0.000306\n",
      "Loss: 0.000192\n",
      "Loss: 0.000197\n",
      "Loss: 0.000152\n",
      "Loss: 0.000278\n",
      "Loss: 0.000144\n",
      "Loss: 0.000101\n",
      "Loss: 0.000034\n",
      "Loss: 0.000220\n",
      "Loss: 0.000293\n",
      "Loss: 0.000370\n",
      "Loss: 0.000057\n",
      "Loss: 0.000132\n",
      "Loss: 0.000106\n",
      "Loss: 0.000089\n",
      "Loss: 0.000154\n",
      "Loss: 0.000113\n",
      "Loss: 0.000158\n",
      "Loss: 0.000055\n",
      "Loss: 0.000210\n",
      "Loss: 0.000091\n",
      "Loss: 0.000193\n",
      "Loss: 0.000159\n",
      "Loss: 0.000161\n",
      "Loss: 0.000150\n",
      "Loss: 0.000136\n",
      "Loss: 0.000042\n",
      "Loss: 0.000095\n",
      "Loss: 0.000075\n",
      "Loss: 0.000103\n",
      "Loss: 0.000158\n",
      "Loss: 0.000136\n",
      "Loss: 0.000100\n",
      "Loss: 0.000213\n",
      "Loss: 0.000073\n",
      "Loss: 0.000150\n",
      "Loss: 0.000203\n",
      "Loss: 0.000077\n",
      "Loss: 0.000131\n",
      "Loss: 0.000036\n",
      "Loss: 0.000115\n",
      "Loss: 0.000347\n",
      "Loss: 0.000157\n",
      "Loss: 0.000356\n",
      "Loss: 0.000188\n",
      "Loss: 0.000215\n",
      "Loss: 0.000376\n",
      "Loss: 0.000168\n",
      "Loss: 0.000045\n",
      "Loss: 0.000132\n",
      "Loss: 0.000219\n",
      "Loss: 0.000171\n",
      "Loss: 0.000094\n",
      "Loss: 0.000169\n",
      "Loss: 0.000394\n",
      "Loss: 0.000156\n",
      "Loss: 0.000122\n",
      "Loss: 0.000359\n",
      "Loss: 0.000181\n",
      "Loss: 0.000223\n",
      "Loss: 0.000120\n",
      "Loss: 0.000317\n",
      "Loss: 0.000185\n",
      "Loss: 0.000060\n",
      "Loss: 0.000113\n",
      "Loss: 0.000219\n",
      "Loss: 0.000317\n",
      "Loss: 0.000066\n",
      "Loss: 0.000101\n",
      "Loss: 0.000069\n",
      "Loss: 0.000154\n",
      "Loss: 0.000115\n",
      "Loss: 0.000177\n",
      "Loss: 0.000041\n",
      "Loss: 0.000339\n",
      "Loss: 0.000143\n",
      "Loss: 0.000100\n",
      "Loss: 0.000071\n",
      "Loss: 0.000101\n",
      "Loss: 0.000105\n",
      "Loss: 0.000186\n",
      "Loss: 0.000248\n",
      "Loss: 0.000110\n",
      "Loss: 0.000206\n",
      "Loss: 0.000023\n",
      "Loss: 0.000364\n",
      "Loss: 0.000092\n",
      "Loss: 0.000181\n",
      "Loss: 0.000214\n",
      "Loss: 0.000094\n",
      "Loss: 0.000953\n",
      "Trained on 3 - Batch 2\n",
      "Loading train file 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000073\n",
      "Loss: 0.000031\n",
      "Loss: 0.000225\n",
      "Loss: 0.000040\n",
      "Loss: 0.000108\n",
      "Loss: 0.000317\n",
      "Loss: 0.000101\n",
      "Loss: 0.000146\n",
      "Loss: 0.000319\n",
      "Loss: 0.000100\n",
      "Loss: 0.000150\n",
      "Loss: 0.000192\n",
      "Loss: 0.000240\n",
      "Loss: 0.000262\n",
      "Loss: 0.000107\n",
      "Loss: 0.000077\n",
      "Loss: 0.000067\n",
      "Loss: 0.000116\n",
      "Loss: 0.000198\n",
      "Loss: 0.000201\n",
      "Loss: 0.000395\n",
      "Loss: 0.000237\n",
      "Loss: 0.000495\n",
      "Loss: 0.000191\n",
      "Loss: 0.000134\n",
      "Loss: 0.000121\n",
      "Loss: 0.000099\n",
      "Loss: 0.000123\n",
      "Loss: 0.000089\n",
      "Loss: 0.000095\n",
      "Loss: 0.000140\n",
      "Loss: 0.000215\n",
      "Loss: 0.000139\n",
      "Loss: 0.000220\n",
      "Loss: 0.000244\n",
      "Loss: 0.000218\n",
      "Loss: 0.000113\n",
      "Loss: 0.000205\n",
      "Loss: 0.000134\n",
      "Loss: 0.000149\n",
      "Loss: 0.000448\n",
      "Loss: 0.000131\n",
      "Loss: 0.000132\n",
      "Loss: 0.000325\n",
      "Loss: 0.000148\n",
      "Loss: 0.000257\n",
      "Loss: 0.000140\n",
      "Loss: 0.000043\n",
      "Loss: 0.000050\n",
      "Loss: 0.000241\n",
      "Loss: 0.000113\n",
      "Loss: 0.000332\n",
      "Loss: 0.000245\n",
      "Loss: 0.000433\n",
      "Loss: 0.000193\n",
      "Loss: 0.000074\n",
      "Loss: 0.000084\n",
      "Loss: 0.000149\n",
      "Loss: 0.000284\n",
      "Loss: 0.000155\n",
      "Loss: 0.000191\n",
      "Loss: 0.000239\n",
      "Loss: 0.000184\n",
      "Loss: 0.000166\n",
      "Loss: 0.000095\n",
      "Loss: 0.000176\n",
      "Loss: 0.000103\n",
      "Loss: 0.000146\n",
      "Loss: 0.000207\n",
      "Loss: 0.000102\n",
      "Loss: 0.000039\n",
      "Loss: 0.000171\n",
      "Loss: 0.000063\n",
      "Loss: 0.000070\n",
      "Loss: 0.000503\n",
      "Loss: 0.000048\n",
      "Loss: 0.000209\n",
      "Loss: 0.000392\n",
      "Loss: 0.000453\n",
      "Loss: 0.000330\n",
      "Loss: 0.000288\n",
      "Loss: 0.000113\n",
      "Loss: 0.000185\n",
      "Loss: 0.000587\n",
      "Loss: 0.000094\n",
      "Loss: 0.000062\n",
      "Loss: 0.000232\n",
      "Loss: 0.000298\n",
      "Loss: 0.000085\n",
      "Loss: 0.000065\n",
      "Loss: 0.000116\n",
      "Loss: 0.000141\n",
      "Loss: 0.000088\n",
      "Loss: 0.000245\n",
      "Loss: 0.000154\n",
      "Loss: 0.000188\n",
      "Loss: 0.000253\n",
      "Loss: 0.000151\n",
      "Loss: 0.000128\n",
      "Loss: 0.000292\n",
      "Loss: 0.000218\n",
      "Loss: 0.000027\n",
      "Loss: 0.000163\n",
      "Loss: 0.000264\n",
      "Loss: 0.000146\n",
      "Loss: 0.000329\n",
      "Loss: 0.000198\n",
      "Loss: 0.000227\n",
      "Loss: 0.000159\n",
      "Loss: 0.000102\n",
      "Loss: 0.000082\n",
      "Loss: 0.000254\n",
      "Loss: 0.000137\n",
      "Loss: 0.000115\n",
      "Loss: 0.000118\n",
      "Loss: 0.000230\n",
      "Loss: 0.000233\n",
      "Loss: 0.000121\n",
      "Loss: 0.000415\n",
      "Loss: 0.000036\n",
      "Loss: 0.000536\n",
      "Loss: 0.000081\n",
      "Loss: 0.000066\n",
      "Loss: 0.000180\n",
      "Loss: 0.000145\n",
      "Loss: 0.000167\n",
      "Loss: 0.000073\n",
      "Loss: 0.000228\n",
      "Loss: 0.000145\n",
      "Loss: 0.000210\n",
      "Loss: 0.000308\n",
      "Loss: 0.000206\n",
      "Loss: 0.000236\n",
      "Loss: 0.000199\n",
      "Loss: 0.000493\n",
      "Loss: 0.000290\n",
      "Loss: 0.000238\n",
      "Loss: 0.000412\n",
      "Loss: 0.000167\n",
      "Loss: 0.000279\n",
      "Loss: 0.000039\n",
      "Loss: 0.000145\n",
      "Loss: 0.000087\n",
      "Loss: 0.000190\n",
      "Loss: 0.000082\n",
      "Loss: 0.000063\n",
      "Loss: 0.000175\n",
      "Loss: 0.000462\n",
      "Loss: 0.000260\n",
      "Loss: 0.000165\n",
      "Loss: 0.000327\n",
      "Loss: 0.000067\n",
      "Loss: 0.000133\n",
      "Loss: 0.000079\n",
      "Loss: 0.000053\n",
      "Loss: 0.000145\n",
      "Loss: 0.000170\n",
      "Loss: 0.000232\n",
      "Loss: 0.000109\n",
      "Loss: 0.000200\n",
      "Loss: 0.000186\n",
      "Loss: 0.000193\n",
      "Loss: 0.000242\n",
      "Loss: 0.000054\n",
      "Loss: 0.000245\n",
      "Loss: 0.000052\n",
      "Loss: 0.000127\n",
      "Loss: 0.000148\n",
      "Loss: 0.000240\n",
      "Loss: 0.000050\n",
      "Loss: 0.000269\n",
      "Loss: 0.000195\n",
      "Loss: 0.000252\n",
      "Loss: 0.000057\n",
      "Loss: 0.000105\n",
      "Loss: 0.000038\n",
      "Loss: 0.000070\n",
      "Loss: 0.000066\n",
      "Loss: 0.000182\n",
      "Loss: 0.000112\n",
      "Loss: 0.000131\n",
      "Loss: 0.000289\n",
      "Loss: 0.000067\n",
      "Loss: 0.000180\n",
      "Loss: 0.000200\n",
      "Loss: 0.000049\n",
      "Loss: 0.000027\n",
      "Loss: 0.000230\n",
      "Loss: 0.000122\n",
      "Loss: 0.000435\n",
      "Loss: 0.000047\n",
      "Loss: 0.000102\n",
      "Loss: 0.000135\n",
      "Loss: 0.000130\n",
      "Loss: 0.000202\n",
      "Loss: 0.000115\n",
      "Loss: 0.000171\n",
      "Loss: 0.000070\n",
      "Loss: 0.000059\n",
      "Loss: 0.000210\n",
      "Loss: 0.000137\n",
      "Loss: 0.000015\n",
      "Loss: 0.000043\n",
      "Loss: 0.000023\n",
      "Loss: 0.000404\n",
      "Loss: 0.000031\n",
      "Loss: 0.000319\n",
      "Loss: 0.000163\n",
      "Loss: 0.000164\n",
      "Loss: 0.000073\n",
      "Loss: 0.000065\n",
      "Loss: 0.000191\n",
      "Loss: 0.000040\n",
      "Loss: 0.000091\n",
      "Loss: 0.000344\n",
      "Loss: 0.000245\n",
      "Loss: 0.000067\n",
      "Loss: 0.000246\n",
      "Loss: 0.000065\n",
      "Loss: 0.000047\n",
      "Loss: 0.000040\n",
      "Loss: 0.000101\n",
      "Loss: 0.000100\n",
      "Loss: 0.000042\n",
      "Loss: 0.000056\n",
      "Loss: 0.000178\n",
      "Loss: 0.000096\n",
      "Loss: 0.000024\n",
      "Loss: 0.000082\n",
      "Loss: 0.000055\n",
      "Loss: 0.000087\n",
      "Loss: 0.000144\n",
      "Loss: 0.000094\n",
      "Loss: 0.000087\n",
      "Loss: 0.000092\n",
      "Loss: 0.000031\n",
      "Loss: 0.000189\n",
      "Loss: 0.000151\n",
      "Loss: 0.000028\n",
      "Loss: 0.000406\n",
      "Loss: 0.000161\n",
      "Loss: 0.000037\n",
      "Loss: 0.000030\n",
      "Loss: 0.000089\n",
      "Loss: 0.000432\n",
      "Loss: 0.000049\n",
      "Loss: 0.000070\n",
      "Loss: 0.000248\n",
      "Loss: 0.000092\n",
      "Loss: 0.000185\n",
      "Loss: 0.000115\n",
      "Loss: 0.000259\n",
      "Loss: 0.000135\n",
      "Loss: 0.000118\n",
      "Loss: 0.000352\n",
      "Loss: 0.000142\n",
      "Loss: 0.000044\n",
      "Loss: 0.000063\n",
      "Loss: 0.000022\n",
      "Loss: 0.000039\n",
      "Loss: 0.000086\n",
      "Loss: 0.000118\n",
      "Loss: 0.000030\n",
      "Loss: 0.000094\n",
      "Loss: 0.000199\n",
      "Loss: 0.000138\n",
      "Loss: 0.000155\n",
      "Loss: 0.000201\n",
      "Loss: 0.000107\n",
      "Loss: 0.000092\n",
      "Loss: 0.000083\n",
      "Loss: 0.000066\n",
      "Loss: 0.000171\n",
      "Loss: 0.000152\n",
      "Loss: 0.000237\n",
      "Loss: 0.000166\n",
      "Loss: 0.000057\n",
      "Loss: 0.000172\n",
      "Loss: 0.000055\n",
      "Loss: 0.000069\n",
      "Loss: 0.000176\n",
      "Loss: 0.000188\n",
      "Loss: 0.000176\n",
      "Loss: 0.000061\n",
      "Loss: 0.000053\n",
      "Loss: 0.000168\n",
      "Loss: 0.000082\n",
      "Loss: 0.000076\n",
      "Loss: 0.000052\n",
      "Loss: 0.000045\n",
      "Loss: 0.000145\n",
      "Loss: 0.000383\n",
      "Loss: 0.000050\n",
      "Loss: 0.000187\n",
      "Loss: 0.000178\n",
      "Loss: 0.000062\n",
      "Loss: 0.000184\n",
      "Loss: 0.000058\n",
      "Loss: 0.000202\n",
      "Loss: 0.000126\n",
      "Trained on 4 - Batch 1\n",
      "Loss: 0.000082\n",
      "Loss: 0.000076\n",
      "Loss: 0.000153\n",
      "Loss: 0.000116\n",
      "Loss: 0.000010\n",
      "Loss: 0.000077\n",
      "Loss: 0.000187\n",
      "Loss: 0.000102\n",
      "Loss: 0.000102\n",
      "Loss: 0.000263\n",
      "Loss: 0.000118\n",
      "Loss: 0.000482\n",
      "Loss: 0.000014\n",
      "Loss: 0.000232\n",
      "Loss: 0.000150\n",
      "Loss: 0.000166\n",
      "Loss: 0.000143\n",
      "Loss: 0.000080\n",
      "Loss: 0.000111\n",
      "Loss: 0.000050\n",
      "Loss: 0.000156\n",
      "Loss: 0.000064\n",
      "Loss: 0.000347\n",
      "Loss: 0.000106\n",
      "Loss: 0.000043\n",
      "Loss: 0.000328\n",
      "Loss: 0.000114\n",
      "Loss: 0.000031\n",
      "Loss: 0.000240\n",
      "Loss: 0.000081\n",
      "Loss: 0.000038\n",
      "Loss: 0.000188\n",
      "Loss: 0.000260\n",
      "Loss: 0.000031\n",
      "Loss: 0.000078\n",
      "Loss: 0.000264\n",
      "Loss: 0.000147\n",
      "Loss: 0.000235\n",
      "Loss: 0.000124\n",
      "Loss: 0.000325\n",
      "Loss: 0.000076\n",
      "Loss: 0.000217\n",
      "Loss: 0.000111\n",
      "Loss: 0.000209\n",
      "Loss: 0.000104\n",
      "Loss: 0.000151\n",
      "Loss: 0.000082\n",
      "Loss: 0.000061\n",
      "Loss: 0.000160\n",
      "Loss: 0.000050\n",
      "Loss: 0.000498\n",
      "Loss: 0.000226\n",
      "Loss: 0.000103\n",
      "Loss: 0.000173\n",
      "Loss: 0.000069\n",
      "Loss: 0.000126\n",
      "Loss: 0.000103\n",
      "Loss: 0.000211\n",
      "Loss: 0.000115\n",
      "Loss: 0.000092\n",
      "Loss: 0.000067\n",
      "Loss: 0.000232\n",
      "Loss: 0.000027\n",
      "Loss: 0.000216\n",
      "Loss: 0.000023\n",
      "Loss: 0.000075\n",
      "Loss: 0.000038\n",
      "Loss: 0.000036\n",
      "Loss: 0.000052\n",
      "Loss: 0.000303\n",
      "Loss: 0.000095\n",
      "Loss: 0.000055\n",
      "Loss: 0.000024\n",
      "Loss: 0.000176\n",
      "Loss: 0.000218\n",
      "Loss: 0.000021\n",
      "Loss: 0.000357\n",
      "Loss: 0.000401\n",
      "Loss: 0.000346\n",
      "Loss: 0.000406\n",
      "Loss: 0.000019\n",
      "Loss: 0.000157\n",
      "Loss: 0.000453\n",
      "Loss: 0.000312\n",
      "Loss: 0.000122\n",
      "Loss: 0.000112\n",
      "Loss: 0.000389\n",
      "Loss: 0.000183\n",
      "Loss: 0.000113\n",
      "Loss: 0.000124\n",
      "Loss: 0.000165\n",
      "Loss: 0.000109\n",
      "Loss: 0.000274\n",
      "Loss: 0.000328\n",
      "Loss: 0.000033\n",
      "Loss: 0.000127\n",
      "Loss: 0.000140\n",
      "Loss: 0.000211\n",
      "Loss: 0.000248\n",
      "Loss: 0.000124\n",
      "Loss: 0.000146\n",
      "Loss: 0.000348\n",
      "Loss: 0.000126\n",
      "Loss: 0.000200\n",
      "Loss: 0.000236\n",
      "Loss: 0.000260\n",
      "Loss: 0.000079\n",
      "Loss: 0.000087\n",
      "Loss: 0.000087\n",
      "Loss: 0.000079\n",
      "Loss: 0.000072\n",
      "Loss: 0.000146\n",
      "Loss: 0.000051\n",
      "Loss: 0.000231\n",
      "Loss: 0.000150\n",
      "Loss: 0.000066\n",
      "Loss: 0.000081\n",
      "Loss: 0.000085\n",
      "Loss: 0.000095\n",
      "Loss: 0.000164\n",
      "Loss: 0.000071\n",
      "Loss: 0.000044\n",
      "Loss: 0.000061\n",
      "Loss: 0.000554\n",
      "Loss: 0.000188\n",
      "Loss: 0.000149\n",
      "Loss: 0.000256\n",
      "Loss: 0.000118\n",
      "Loss: 0.000230\n",
      "Loss: 0.000076\n",
      "Loss: 0.000026\n",
      "Loss: 0.000058\n",
      "Loss: 0.000057\n",
      "Loss: 0.000042\n",
      "Loss: 0.000107\n",
      "Loss: 0.000285\n",
      "Loss: 0.000204\n",
      "Loss: 0.000088\n",
      "Loss: 0.000141\n",
      "Loss: 0.000085\n",
      "Loss: 0.000107\n",
      "Loss: 0.000175\n",
      "Loss: 0.000044\n",
      "Loss: 0.000022\n",
      "Loss: 0.000045\n",
      "Loss: 0.000039\n",
      "Loss: 0.000382\n",
      "Loss: 0.000141\n",
      "Loss: 0.000103\n",
      "Loss: 0.000115\n",
      "Loss: 0.000161\n",
      "Loss: 0.000308\n",
      "Loss: 0.000076\n",
      "Loss: 0.000131\n",
      "Loss: 0.000101\n",
      "Loss: 0.000149\n",
      "Loss: 0.000049\n",
      "Loss: 0.000031\n",
      "Loss: 0.000131\n",
      "Loss: 0.000082\n",
      "Loss: 0.000022\n",
      "Loss: 0.000248\n",
      "Loss: 0.000079\n",
      "Loss: 0.000302\n",
      "Loss: 0.000034\n",
      "Loss: 0.000023\n",
      "Loss: 0.000060\n",
      "Loss: 0.000193\n",
      "Loss: 0.000470\n",
      "Loss: 0.000187\n",
      "Loss: 0.000112\n",
      "Loss: 0.000249\n",
      "Loss: 0.000044\n",
      "Loss: 0.000055\n",
      "Loss: 0.000033\n",
      "Loss: 0.000020\n",
      "Loss: 0.000089\n",
      "Loss: 0.000150\n",
      "Loss: 0.000122\n",
      "Loss: 0.000052\n",
      "Loss: 0.000174\n",
      "Loss: 0.000067\n",
      "Loss: 0.000049\n",
      "Loss: 0.000029\n",
      "Loss: 0.000167\n",
      "Loss: 0.000053\n",
      "Loss: 0.000055\n",
      "Loss: 0.000121\n",
      "Loss: 0.000098\n",
      "Loss: 0.000024\n",
      "Loss: 0.000087\n",
      "Loss: 0.000043\n",
      "Loss: 0.000274\n",
      "Loss: 0.000161\n",
      "Loss: 0.000090\n",
      "Loss: 0.000046\n",
      "Loss: 0.000025\n",
      "Loss: 0.000102\n",
      "Loss: 0.000054\n",
      "Loss: 0.000043\n",
      "Loss: 0.000190\n",
      "Loss: 0.000127\n",
      "Loss: 0.000091\n",
      "Loss: 0.000102\n",
      "Loss: 0.000055\n",
      "Loss: 0.000057\n",
      "Loss: 0.000054\n",
      "Loss: 0.000085\n",
      "Loss: 0.000039\n",
      "Loss: 0.000044\n",
      "Loss: 0.000033\n",
      "Loss: 0.000044\n",
      "Loss: 0.000142\n",
      "Loss: 0.000036\n",
      "Loss: 0.000035\n",
      "Loss: 0.000052\n",
      "Loss: 0.000031\n",
      "Loss: 0.000208\n",
      "Loss: 0.000051\n",
      "Loss: 0.000088\n",
      "Loss: 0.000021\n",
      "Loss: 0.000067\n",
      "Loss: 0.000124\n",
      "Loss: 0.000108\n",
      "Loss: 0.000064\n",
      "Loss: 0.000299\n",
      "Loss: 0.000093\n",
      "Loss: 0.000139\n",
      "Loss: 0.000261\n",
      "Loss: 0.000070\n",
      "Loss: 0.000016\n",
      "Loss: 0.000029\n",
      "Loss: 0.000249\n",
      "Loss: 0.000151\n",
      "Loss: 0.000037\n",
      "Loss: 0.000136\n",
      "Loss: 0.000053\n",
      "Loss: 0.000122\n",
      "Loss: 0.000110\n",
      "Loss: 0.000039\n",
      "Loss: 0.000236\n",
      "Loss: 0.000128\n",
      "Loss: 0.000054\n",
      "Loss: 0.000064\n",
      "Loss: 0.000223\n",
      "Loss: 0.000073\n",
      "Loss: 0.000045\n",
      "Loss: 0.000275\n",
      "Loss: 0.000283\n",
      "Trained on 4 - Batch 2\n",
      "Loading train file 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000248\n",
      "Loss: 0.000063\n",
      "Loss: 0.000048\n",
      "Loss: 0.000026\n",
      "Loss: 0.000293\n",
      "Loss: 0.000083\n",
      "Loss: 0.000081\n",
      "Loss: 0.000087\n",
      "Loss: 0.000054\n",
      "Loss: 0.000068\n",
      "Loss: 0.000128\n",
      "Loss: 0.000049\n",
      "Loss: 0.000140\n",
      "Loss: 0.000034\n",
      "Loss: 0.000071\n",
      "Loss: 0.000048\n",
      "Loss: 0.000106\n",
      "Loss: 0.000021\n",
      "Loss: 0.000103\n",
      "Loss: 0.000370\n",
      "Loss: 0.000140\n",
      "Loss: 0.000055\n",
      "Loss: 0.000121\n",
      "Loss: 0.000053\n",
      "Loss: 0.000061\n",
      "Loss: 0.000168\n",
      "Loss: 0.000204\n",
      "Loss: 0.000188\n",
      "Loss: 0.000338\n",
      "Loss: 0.000071\n",
      "Loss: 0.000130\n",
      "Loss: 0.000365\n",
      "Loss: 0.000193\n",
      "Loss: 0.000088\n",
      "Loss: 0.000064\n",
      "Loss: 0.000041\n",
      "Loss: 0.000016\n",
      "Loss: 0.000458\n",
      "Loss: 0.000173\n",
      "Loss: 0.000375\n",
      "Loss: 0.000234\n",
      "Loss: 0.000157\n",
      "Loss: 0.000130\n",
      "Loss: 0.000111\n",
      "Loss: 0.000097\n",
      "Loss: 0.000187\n",
      "Loss: 0.000157\n",
      "Loss: 0.000060\n",
      "Loss: 0.000072\n",
      "Loss: 0.000191\n",
      "Loss: 0.000025\n",
      "Loss: 0.000036\n",
      "Loss: 0.000166\n",
      "Loss: 0.000011\n",
      "Loss: 0.000040\n",
      "Loss: 0.000133\n",
      "Loss: 0.000043\n",
      "Loss: 0.000136\n",
      "Loss: 0.000025\n",
      "Loss: 0.000088\n",
      "Loss: 0.000042\n",
      "Loss: 0.000112\n",
      "Loss: 0.000032\n",
      "Loss: 0.000015\n",
      "Loss: 0.000023\n",
      "Loss: 0.000019\n",
      "Loss: 0.000206\n",
      "Loss: 0.000017\n",
      "Loss: 0.000073\n",
      "Loss: 0.000082\n",
      "Loss: 0.000035\n",
      "Loss: 0.000116\n",
      "Loss: 0.000181\n",
      "Loss: 0.000121\n",
      "Loss: 0.000195\n",
      "Loss: 0.000016\n",
      "Loss: 0.000194\n",
      "Loss: 0.000080\n",
      "Loss: 0.000136\n",
      "Loss: 0.000031\n",
      "Loss: 0.000029\n",
      "Loss: 0.000052\n",
      "Loss: 0.000012\n",
      "Loss: 0.000140\n",
      "Loss: 0.000071\n",
      "Loss: 0.000339\n",
      "Loss: 0.000023\n",
      "Loss: 0.000347\n",
      "Loss: 0.000121\n",
      "Loss: 0.000142\n",
      "Loss: 0.000219\n",
      "Loss: 0.000063\n",
      "Loss: 0.000115\n",
      "Loss: 0.000092\n",
      "Loss: 0.000245\n",
      "Loss: 0.000056\n",
      "Loss: 0.000129\n",
      "Loss: 0.000071\n",
      "Loss: 0.000047\n",
      "Loss: 0.000120\n",
      "Loss: 0.000115\n",
      "Loss: 0.000081\n",
      "Loss: 0.000229\n",
      "Loss: 0.000022\n",
      "Loss: 0.000033\n",
      "Loss: 0.000228\n",
      "Loss: 0.000603\n",
      "Loss: 0.000258\n",
      "Loss: 0.000142\n",
      "Loss: 0.000082\n",
      "Loss: 0.000121\n",
      "Loss: 0.000055\n",
      "Loss: 0.000063\n",
      "Loss: 0.000050\n",
      "Loss: 0.000089\n",
      "Loss: 0.000084\n",
      "Loss: 0.000063\n",
      "Loss: 0.000127\n",
      "Loss: 0.000097\n",
      "Loss: 0.000086\n",
      "Loss: 0.000111\n",
      "Loss: 0.000026\n",
      "Loss: 0.000113\n",
      "Loss: 0.000006\n",
      "Loss: 0.000030\n",
      "Loss: 0.000188\n",
      "Loss: 0.000314\n",
      "Loss: 0.000179\n",
      "Loss: 0.000051\n",
      "Loss: 0.000107\n",
      "Loss: 0.000145\n",
      "Loss: 0.000150\n",
      "Loss: 0.000069\n",
      "Loss: 0.000131\n",
      "Loss: 0.000177\n",
      "Loss: 0.000027\n",
      "Loss: 0.000295\n",
      "Loss: 0.000120\n",
      "Loss: 0.000260\n",
      "Loss: 0.000049\n",
      "Loss: 0.000126\n",
      "Loss: 0.000287\n",
      "Loss: 0.000191\n",
      "Loss: 0.000110\n",
      "Loss: 0.000039\n",
      "Loss: 0.000088\n",
      "Loss: 0.000390\n",
      "Loss: 0.000070\n",
      "Loss: 0.000093\n",
      "Loss: 0.000022\n",
      "Loss: 0.000129\n",
      "Loss: 0.000175\n",
      "Loss: 0.000062\n",
      "Loss: 0.000068\n",
      "Loss: 0.000078\n",
      "Loss: 0.000151\n",
      "Loss: 0.000201\n",
      "Loss: 0.000334\n",
      "Loss: 0.000204\n",
      "Loss: 0.000265\n",
      "Loss: 0.000059\n",
      "Loss: 0.000104\n",
      "Loss: 0.000097\n",
      "Loss: 0.000038\n",
      "Loss: 0.000096\n",
      "Loss: 0.000191\n",
      "Loss: 0.000134\n",
      "Loss: 0.000102\n",
      "Loss: 0.000168\n",
      "Loss: 0.000060\n",
      "Loss: 0.000050\n",
      "Loss: 0.000067\n",
      "Loss: 0.000120\n",
      "Loss: 0.000235\n",
      "Loss: 0.000144\n",
      "Loss: 0.000065\n",
      "Loss: 0.000094\n",
      "Loss: 0.000059\n",
      "Loss: 0.000116\n",
      "Loss: 0.000127\n",
      "Loss: 0.000102\n",
      "Loss: 0.000148\n",
      "Loss: 0.000159\n",
      "Loss: 0.000030\n",
      "Loss: 0.000022\n",
      "Loss: 0.000190\n",
      "Loss: 0.000090\n",
      "Loss: 0.000012\n",
      "Loss: 0.000039\n",
      "Loss: 0.000188\n",
      "Loss: 0.000058\n",
      "Loss: 0.000229\n",
      "Loss: 0.000023\n",
      "Loss: 0.000051\n",
      "Loss: 0.000305\n",
      "Loss: 0.000102\n",
      "Loss: 0.000103\n",
      "Loss: 0.000070\n",
      "Loss: 0.000104\n",
      "Loss: 0.000178\n",
      "Loss: 0.000047\n",
      "Loss: 0.000217\n",
      "Loss: 0.000100\n",
      "Loss: 0.000055\n",
      "Loss: 0.000083\n",
      "Loss: 0.000037\n",
      "Loss: 0.000206\n",
      "Loss: 0.000465\n",
      "Loss: 0.000048\n",
      "Loss: 0.000167\n",
      "Loss: 0.000017\n",
      "Loss: 0.000324\n",
      "Loss: 0.000105\n",
      "Loss: 0.000204\n",
      "Loss: 0.000032\n",
      "Loss: 0.000157\n",
      "Loss: 0.000197\n",
      "Loss: 0.000073\n",
      "Loss: 0.000067\n",
      "Loss: 0.000068\n",
      "Loss: 0.000154\n",
      "Loss: 0.000028\n",
      "Loss: 0.000089\n",
      "Loss: 0.000282\n",
      "Loss: 0.000032\n",
      "Loss: 0.000028\n",
      "Loss: 0.000142\n",
      "Loss: 0.000036\n",
      "Loss: 0.000022\n",
      "Loss: 0.000038\n",
      "Loss: 0.000055\n",
      "Loss: 0.000210\n",
      "Loss: 0.000035\n",
      "Loss: 0.000137\n",
      "Loss: 0.000186\n",
      "Loss: 0.000175\n",
      "Loss: 0.000030\n",
      "Loss: 0.000070\n",
      "Loss: 0.000253\n",
      "Loss: 0.000157\n",
      "Loss: 0.000213\n",
      "Loss: 0.000140\n",
      "Loss: 0.000269\n",
      "Loss: 0.000252\n",
      "Loss: 0.000153\n",
      "Loss: 0.000083\n",
      "Loss: 0.000083\n",
      "Loss: 0.000401\n",
      "Loss: 0.000103\n",
      "Loss: 0.000142\n",
      "Loss: 0.000087\n",
      "Loss: 0.000069\n",
      "Loss: 0.000024\n",
      "Loss: 0.000084\n",
      "Loss: 0.000163\n",
      "Loss: 0.000054\n",
      "Loss: 0.000064\n",
      "Loss: 0.000040\n",
      "Loss: 0.000207\n",
      "Loss: 0.000110\n",
      "Loss: 0.000060\n",
      "Loss: 0.000124\n",
      "Loss: 0.000080\n",
      "Loss: 0.000049\n",
      "Loss: 0.000018\n",
      "Loss: 0.000014\n",
      "Loss: 0.000070\n",
      "Loss: 0.000055\n",
      "Loss: 0.000133\n",
      "Loss: 0.000077\n",
      "Loss: 0.000098\n",
      "Loss: 0.000211\n",
      "Loss: 0.000016\n",
      "Loss: 0.000101\n",
      "Loss: 0.000162\n",
      "Loss: 0.000115\n",
      "Loss: 0.000034\n",
      "Loss: 0.000146\n",
      "Loss: 0.000145\n",
      "Loss: 0.000184\n",
      "Loss: 0.000189\n",
      "Loss: 0.000021\n",
      "Loss: 0.000035\n",
      "Loss: 0.000224\n",
      "Loss: 0.000506\n",
      "Loss: 0.000122\n",
      "Loss: 0.000060\n",
      "Loss: 0.000078\n",
      "Loss: 0.000542\n",
      "Loss: 0.000064\n",
      "Loss: 0.000056\n",
      "Loss: 0.000191\n",
      "Loss: 0.000149\n",
      "Loss: 0.000525\n",
      "Loss: 0.000053\n",
      "Loss: 0.000275\n",
      "Loss: 0.000193\n",
      "Loss: 0.000089\n",
      "Loss: 0.000025\n",
      "Loss: 0.000175\n",
      "Trained on 5 - Batch 1\n",
      "Loss: 0.000300\n",
      "Loss: 0.000129\n",
      "Loss: 0.000118\n",
      "Loss: 0.000154\n",
      "Loss: 0.000250\n",
      "Loss: 0.000334\n",
      "Loss: 0.000262\n",
      "Loss: 0.000238\n",
      "Loss: 0.000255\n",
      "Loss: 0.000243\n",
      "Loss: 0.000235\n",
      "Loss: 0.000120\n",
      "Loss: 0.000085\n",
      "Loss: 0.000251\n",
      "Loss: 0.000070\n",
      "Loss: 0.000173\n",
      "Loss: 0.000194\n",
      "Loss: 0.000182\n",
      "Loss: 0.000172\n",
      "Loss: 0.000077\n",
      "Loss: 0.000190\n",
      "Loss: 0.000047\n",
      "Loss: 0.000107\n",
      "Loss: 0.000239\n",
      "Loss: 0.000052\n",
      "Loss: 0.000223\n",
      "Loss: 0.000139\n",
      "Loss: 0.000038\n",
      "Loss: 0.000482\n",
      "Loss: 0.000025\n",
      "Loss: 0.000126\n",
      "Loss: 0.000295\n",
      "Loss: 0.000131\n",
      "Loss: 0.000097\n",
      "Loss: 0.000251\n",
      "Loss: 0.000488\n",
      "Loss: 0.000046\n",
      "Loss: 0.000090\n",
      "Loss: 0.000306\n",
      "Loss: 0.000137\n",
      "Loss: 0.000099\n",
      "Loss: 0.000064\n",
      "Loss: 0.000089\n",
      "Loss: 0.000071\n",
      "Loss: 0.000136\n",
      "Loss: 0.000142\n",
      "Loss: 0.000110\n",
      "Loss: 0.000104\n",
      "Loss: 0.000033\n",
      "Loss: 0.000122\n",
      "Loss: 0.000358\n",
      "Loss: 0.000455\n",
      "Loss: 0.000102\n",
      "Loss: 0.000038\n",
      "Loss: 0.000222\n",
      "Loss: 0.000288\n",
      "Loss: 0.000239\n",
      "Loss: 0.000055\n",
      "Loss: 0.000023\n",
      "Loss: 0.000032\n",
      "Loss: 0.000285\n",
      "Loss: 0.000134\n",
      "Loss: 0.000179\n",
      "Loss: 0.000102\n",
      "Loss: 0.000128\n",
      "Loss: 0.000094\n",
      "Loss: 0.000119\n",
      "Loss: 0.000028\n",
      "Loss: 0.000107\n",
      "Loss: 0.000253\n",
      "Loss: 0.000053\n",
      "Loss: 0.000170\n",
      "Loss: 0.000469\n",
      "Loss: 0.000141\n",
      "Loss: 0.000050\n",
      "Loss: 0.000021\n",
      "Loss: 0.000139\n",
      "Loss: 0.000132\n",
      "Loss: 0.000050\n",
      "Loss: 0.000094\n",
      "Loss: 0.000043\n",
      "Loss: 0.000049\n",
      "Loss: 0.000131\n",
      "Loss: 0.000157\n",
      "Loss: 0.000116\n",
      "Loss: 0.000097\n",
      "Loss: 0.000075\n",
      "Loss: 0.000080\n",
      "Loss: 0.000099\n",
      "Loss: 0.000229\n",
      "Loss: 0.000122\n",
      "Loss: 0.000205\n",
      "Loss: 0.000041\n",
      "Loss: 0.000018\n",
      "Loss: 0.000167\n",
      "Loss: 0.000043\n",
      "Loss: 0.000109\n",
      "Loss: 0.000022\n",
      "Loss: 0.000034\n",
      "Loss: 0.000253\n",
      "Loss: 0.000022\n",
      "Loss: 0.000429\n",
      "Loss: 0.000061\n",
      "Loss: 0.000073\n",
      "Loss: 0.000024\n",
      "Loss: 0.000041\n",
      "Loss: 0.000097\n",
      "Loss: 0.000027\n",
      "Loss: 0.000086\n",
      "Loss: 0.000243\n",
      "Loss: 0.000107\n",
      "Loss: 0.000091\n",
      "Loss: 0.000029\n",
      "Loss: 0.000136\n",
      "Loss: 0.000040\n",
      "Loss: 0.000100\n",
      "Loss: 0.000130\n",
      "Loss: 0.000100\n",
      "Loss: 0.000019\n",
      "Loss: 0.000036\n",
      "Loss: 0.000026\n",
      "Loss: 0.000033\n",
      "Loss: 0.000048\n",
      "Loss: 0.000022\n",
      "Loss: 0.000114\n",
      "Loss: 0.000036\n",
      "Loss: 0.000083\n",
      "Loss: 0.000251\n",
      "Loss: 0.000097\n",
      "Loss: 0.000155\n",
      "Loss: 0.000117\n",
      "Loss: 0.000225\n",
      "Loss: 0.000097\n",
      "Loss: 0.000152\n",
      "Loss: 0.000065\n",
      "Loss: 0.000065\n",
      "Loss: 0.000076\n",
      "Loss: 0.000042\n",
      "Loss: 0.000116\n",
      "Loss: 0.000038\n",
      "Loss: 0.000015\n",
      "Loss: 0.000171\n",
      "Loss: 0.000055\n",
      "Loss: 0.000023\n",
      "Loss: 0.000068\n",
      "Loss: 0.000052\n",
      "Loss: 0.000035\n",
      "Loss: 0.000043\n",
      "Loss: 0.000048\n",
      "Loss: 0.000046\n",
      "Loss: 0.000099\n",
      "Loss: 0.000084\n",
      "Loss: 0.000047\n",
      "Loss: 0.000021\n",
      "Loss: 0.000121\n",
      "Loss: 0.000019\n",
      "Loss: 0.000025\n",
      "Loss: 0.000025\n",
      "Loss: 0.000008\n",
      "Loss: 0.000057\n",
      "Loss: 0.000113\n",
      "Loss: 0.000046\n",
      "Loss: 0.000329\n",
      "Loss: 0.000132\n",
      "Loss: 0.000004\n",
      "Loss: 0.000073\n",
      "Loss: 0.000047\n",
      "Loss: 0.000074\n",
      "Loss: 0.000025\n",
      "Loss: 0.000012\n",
      "Loss: 0.000208\n",
      "Loss: 0.000042\n",
      "Loss: 0.000296\n",
      "Loss: 0.000058\n",
      "Loss: 0.000132\n",
      "Loss: 0.000150\n",
      "Loss: 0.000009\n",
      "Loss: 0.000090\n",
      "Loss: 0.000048\n",
      "Loss: 0.000159\n",
      "Loss: 0.000041\n",
      "Loss: 0.000100\n",
      "Loss: 0.000205\n",
      "Loss: 0.000035\n",
      "Loss: 0.000172\n",
      "Loss: 0.000229\n",
      "Loss: 0.000075\n",
      "Loss: 0.000063\n",
      "Loss: 0.000046\n",
      "Loss: 0.000037\n",
      "Loss: 0.000014\n",
      "Loss: 0.000069\n",
      "Loss: 0.000044\n",
      "Loss: 0.000193\n",
      "Loss: 0.000208\n",
      "Loss: 0.000023\n",
      "Loss: 0.000136\n",
      "Loss: 0.000097\n",
      "Loss: 0.000134\n",
      "Loss: 0.000086\n",
      "Loss: 0.000049\n",
      "Loss: 0.000133\n",
      "Loss: 0.000421\n",
      "Loss: 0.000016\n",
      "Loss: 0.000067\n",
      "Loss: 0.000107\n",
      "Loss: 0.000045\n",
      "Loss: 0.000037\n",
      "Loss: 0.000063\n",
      "Loss: 0.000138\n",
      "Loss: 0.000014\n",
      "Loss: 0.000049\n",
      "Loss: 0.000021\n",
      "Loss: 0.000046\n",
      "Loss: 0.000211\n",
      "Loss: 0.000140\n",
      "Loss: 0.000161\n",
      "Loss: 0.000089\n",
      "Loss: 0.000190\n",
      "Loss: 0.000151\n",
      "Loss: 0.000153\n",
      "Loss: 0.000071\n",
      "Loss: 0.000111\n",
      "Loss: 0.000110\n",
      "Loss: 0.000030\n",
      "Loss: 0.000068\n",
      "Loss: 0.000110\n",
      "Loss: 0.000055\n",
      "Loss: 0.000031\n",
      "Loss: 0.000331\n",
      "Loss: 0.000046\n",
      "Loss: 0.000194\n",
      "Loss: 0.000114\n",
      "Loss: 0.000149\n",
      "Loss: 0.000227\n",
      "Loss: 0.000056\n",
      "Loss: 0.000063\n",
      "Loss: 0.000029\n",
      "Loss: 0.000126\n",
      "Loss: 0.000097\n",
      "Loss: 0.000041\n",
      "Loss: 0.000138\n",
      "Loss: 0.000628\n",
      "Loss: 0.000018\n",
      "Loss: 0.000007\n",
      "Loss: 0.000126\n",
      "Loss: 0.000169\n",
      "Loss: 0.000040\n",
      "Loss: 0.000307\n",
      "Trained on 5 - Batch 2\n",
      "End time: 1739323701.2481396\n"
     ]
    }
   ],
   "source": [
    "def mlp_all():\n",
    "    all_indices = list(range(1, 11))   ##########\n",
    "    log_print(f\"Start time: {time.time()}\")\n",
    "\n",
    "    for fold in range(1):      ###########\n",
    "        try:\n",
    "            train_files = all_indices\n",
    "          \n",
    "            # 创建 MLP 模型\n",
    "            model = MLP(input_size=2400).to(device)\n",
    "            criterion = nn.BCELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "            # train all data\n",
    "            for file_idx in train_files:\n",
    "                log_print(f\"Loading train file {file_idx}\")\n",
    "\n",
    "                batch_idx = 1\n",
    "                while True:\n",
    "                    pickle_file = f\"/data2/xpgeng/iML1515_MLP/{file_idx}_{batch_idx}.pkl\"  ####\n",
    "                    if not os.path.exists(pickle_file):\n",
    "                        break  # 没有更多批次数据\n",
    "                        \n",
    "                    with open(pickle_file, \"rb\") as f:\n",
    "                        batch_data, batch_labels = pickle.load(f)\n",
    "\n",
    "                    # 转换为张量\n",
    "                    batch_data = torch.tensor(batch_data, dtype=torch.float32).to(device)\n",
    "                    batch_labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "                    \n",
    "                    # Split into smaller batches of size 200\n",
    "                    for i in range(0, len(batch_data), 200):\n",
    "                        small_batch_data = batch_data[i:i+200]\n",
    "                        small_batch_labels = batch_labels[i:i+200]\n",
    "                        \n",
    "                        # Train the model on this smaller batch\n",
    "                        model = train_mlp(small_batch_data, small_batch_labels, model, criterion, optimizer, num_epochs=4)\n",
    "                   \n",
    "                    log_print(f\"Trained on {file_idx} - Batch {batch_idx}\")\n",
    "                    batch_idx += 1\n",
    "                    \n",
    "            # Save model parameters\n",
    "            model_save_path = os.path.join(os.getcwd(), f\"mlp_all_data.pth\")\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error in fold {fold+1}: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            log_error(error_message)\n",
    "\n",
    "    log_print(f\"End time: {time.time()}\")\n",
    "\n",
    "mlp_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorchh] *",
   "language": "python",
   "name": "conda-env-.conda-pytorchh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
