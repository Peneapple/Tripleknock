{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ead5b5",
   "metadata": {},
   "source": [
    "# Tripleknock Revision Notebook (E. coli iML1515)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa4ef8-2227-4515-9d1e-9af1748c4e15",
   "metadata": {},
   "source": [
    "### data preparation on 5000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8896f8-1622-4005-b4bb-61f19e99a217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /data1/xpgeng/cross_pathogen/sci_rep_revision_20260113/Baseline/triples_no_essential_123630-0_123630-1.sample2000.csv rows: 2000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "infile = Path(\"/data1/xpgeng/cross_pathogen/sci_rep_revision_20260113/Baseline/triples_no_essential_123630-0_123630-1.csv\")\n",
    "outfile = infile.with_name(infile.stem + \".sample2000.csv\")\n",
    "\n",
    "SEED = 2026  # 固定随机种子，保证可复现；想每次不同就删掉 random_state\n",
    "\n",
    "df = pd.read_csv(infile)\n",
    "df_small = df.sample(n=2000, random_state=SEED)\n",
    "\n",
    "df_small.to_csv(outfile, index=False)\n",
    "print(\"Wrote:\", outfile, \"rows:\", len(df_small))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8741db1-f8a7-4a50-9928-dd6d5dce1565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b391c-78b0-4300-a320-1f8c314d7ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ce94ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:2\n",
      "[2026-02-20 21:20:16] Config loaded.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Part 0: Imports + Global Config\n",
    "# =========================\n",
    "import os, time, random, traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# ---- device ----\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "# =========================\n",
    "# Adjustable parameters / 可调参数\n",
    "# =========================\n",
    "\n",
    "# ---- Data files ----\n",
    "# Two balanced datasets:\n",
    "#  - triples_no_essential_123630-0_123630-1.csv\n",
    "#  - triples_with_essential_123630-0_123630-1.csv\n",
    "# We will CONCAT them, then do random 5-fold CV (ALLOW gene repetition across folds).\n",
    "DATA_FILE_0 = '/data1/xpgeng/cross_pathogen/sci_rep_revision_20260113/Baseline/triples_no_essential_123630-0_123630-1.csv'\n",
    "#DATA_FILE_1 = '/data1/xpgeng/cross_pathogen/sci_rep_revision_20260113/Baseline/triples_with_essential_0-0_123630-1.csv'\n",
    "\n",
    "#FILE_PARTS = [DATA_FILE_0, DATA_FILE_1]\n",
    "FILE_PARTS = [DATA_FILE_0]\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "\n",
    "# ---- Sampling sizes (fold pools) ----\n",
    "# Goal: total 1,000,000 samples for 5-fold CV (200k per fold).\n",
    "# In each CV iteration: Test = 200k (1 fold), TrainPool = 800k (other 4 folds).\n",
    "# Then split TrainPool into Train/Val with VAL_FRACTION (default 10%).\n",
    "FOLD_SIZE_PER_FOLD = None     # unused in RANDOM split (we use full fold size)\n",
    "TRAIN_SIZE_PER_FOLD = None       # None => use full TrainPool (default 800k)\n",
    "VAL_FRACTION        = None #0.10       # 10% of TrainPool\n",
    "TEST_SIZE_PER_FOLD  = None     # unused in RANDOM split\n",
    "\n",
    "# (Deprecated sizes kept for backward-compat; not used when VAL_FRACTION is enabled)\n",
    "VAL_SIZE_PER_FOLD   = None\n",
    "\n",
    "# ---- Chunk training to avoid GPU OOM ----\n",
    "# 如果训练集超过2万：分chunk训练，每次最多训练2万样本，然后继续下一批。\n",
    "TRAIN_CHUNK_SIZE = 20000\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# ---- Optimization ----\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "MAX_EPOCHS = 3\n",
    "PATIENCE = 2\n",
    "MIN_DELTA = 5e-4\n",
    "DROPOUT_RATE = 0.5 \n",
    "\n",
    "# ---- Feature options ----\n",
    "REST_SCALE = 0.1   # try: 0.0 / 0.02 / 0.05 / 0.1 / 0.2\n",
    "NORM_MODE = 'block' # 'block' (recommended) | 'per_sample' | 'none'\n",
    "\n",
    "# ---- Threshold search options ----\n",
    "THRESH_METRIC = 'youden'  # 'youden' | 'balanced_acc' | 'f1_pos'\n",
    "MIN_PRECISION_POS = None  # e.g. 0.45 or 0.5 to force better precision for y=1\n",
    "THRESH_MIN  = 0.05\n",
    "THRESH_MAX  = 0.95\n",
    "THRESH_STEP = 0.005\n",
    "\n",
    "# ---- Baseline experiment sizes ----\n",
    "BASELINE_TRAIN_SIZE = 50000\n",
    "\n",
    "# ---- Pair-disjoint switch ----\n",
    "RUN_PAIR_DISJOINT = False\n",
    "\n",
    "# -------------------------\n",
    "# Logging\n",
    "# -------------------------\n",
    "workdir = os.getcwd()\n",
    "OUTPUT_DIR = os.path.join(workdir, 'cv_results_baseline_compare_no_essential_data')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "log_file = os.path.join(workdir, 'baseline_tripleknock_lethalrules_no_essential_data_random_5fold_mlp_512_256_cv_log.txt')          ### 3\n",
    "err_file = os.path.join(workdir, 'baseline_tripleknock_lethalrules_no_essential_data_random_5fold_mlp_512_256_cv_err.txt')          ### 4\n",
    "\n",
    "def log_print(msg: str):\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    line = f'[{ts}] {msg}'\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(line + \"\\n\")\n",
    "    print(line)\n",
    "\n",
    "def log_error(msg: str):\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    line = f'[{ts}] {msg}'\n",
    "    with open(err_file, 'a') as f:\n",
    "        f.write(line + \"\\n\")\n",
    "    print(line)\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "log_print('Config loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43c8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essential genes loaded: 196\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Baseline: Essential-gene presence rule\n",
    "# pred_baseline = 1 if any of (g1,g2,g3) is an essential gene, else 0\n",
    "# Reviewer baseline: single-gene lethality -> triple lethality\n",
    "# =========================\n",
    "\n",
    "essential_gene = ['b0003', 'b0004', 'b0025', 'b0029', 'b0031', 'b0052', 'b0054', 'b0071', 'b0072', 'b0074', 'b0084', 'b0085', 'b0086', 'b0087', 'b0088', 'b0089', 'b0090', 'b0091', 'b0096', 'b0103', 'b0109', 'b0131', 'b0133', 'b0134', 'b0142', 'b0154', 'b0159', 'b0166', 'b0173', 'b0174', 'b0175', 'b0179', 'b0180', 'b0181', 'b0182', 'b0185', 'b0242', 'b0243', 'b0369', 'b0386', 'b0414', 'b0415', 'b0417', 'b0420', 'b0421', 'b0423', 'b0522', 'b0523', 'b0524', 'b0635', 'b0639', 'b0641', 'b0720', 'b0750', 'b0774', 'b0775', 'b0776', 'b0777', 'b0778', 'b0908', 'b0914', 'b0915', 'b0918', 'b1062', 'b1069', 'b1091', 'b1092', 'b1093', 'b1094', 'b1098', 'b1131', 'b1136', 'b1208', 'b1210', 'b1215', 'b1260', 'b1261', 'b1262', 'b1263', 'b1264', 'b1277', 'b1281', 'b1288', 'b1662', 'b1693', 'b1740', 'b1812', 'b2019', 'b2020', 'b2021', 'b2022', 'b2023', 'b2024', 'b2025', 'b2026', 'b2103', 'b2153', 'b2312', 'b2315', 'b2316', 'b2323', 'b2329', 'b2400', 'b2472', 'b2476', 'b2478', 'b2499', 'b2507', 'b2515', 'b2530', 'b2557', 'b2564', 'b2574', 'b2585', 'b2599', 'b2600', 'b2615', 'b2687', 'b2746', 'b2747', 'b2750', 'b2751', 'b2752', 'b2762', 'b2763', 'b2764', 'b2780', 'b2818', 'b2827', 'b2838', 'b2942', 'b3018', 'b3040', 'b3041', 'b3058', 'b3172', 'b3176', 'b3177', 'b3187', 'b3189', 'b3196', 'b3198', 'b3199', 'b3200', 'b3201', 'b3255', 'b3256', 'b3360', 'b3368', 'b3389', 'b3412', 'b3433', 'b3607', 'b3633', 'b3634', 'b3639', 'b3642', 'b3648', 'b3729', 'b3730', 'b3770', 'b3771', 'b3774', 'b3804', 'b3805', 'b3809', 'b3843', 'b3850', 'b3870', 'b3939', 'b3941', 'b3957', 'b3958', 'b3959', 'b3960', 'b3967', 'b3972', 'b3974', 'b3990', 'b3991', 'b3992', 'b3993', 'b3994', 'b3997', 'b4005', 'b4006', 'b4013', 'b4040', 'b4160', 'b4177', 'b4214', 'b4245', 'b4261', 'b4262', 'b4407', 's0001']\n",
    "\n",
    "essential_set = set(essential_gene)\n",
    "print('Essential genes loaded:', len(essential_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fdc277",
   "metadata": {},
   "source": [
    "## Helper utilities / 工具函数\n",
    "\n",
    "这一节包含：\n",
    "- gene / triple 排序（保证 `g1 < g2 < g3`，避免重复）\n",
    "- stratified sampling（可选）\n",
    "- threshold search（Youden / balanced accuracy / f1_pos）\n",
    "- evaluation report（AUC + confusion matrix + classification report）\n",
    "\n",
    "This section includes sorting helpers, sampling helpers, threshold search, and evaluation utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d87c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Part 1: Utilities\n",
    "# =========================\n",
    "\n",
    "def gene_key(g: str):\n",
    "    \"\"\"Sort key: b0002 < b0003 < ...\"\"\"\n",
    "    g = str(g).strip()\n",
    "    prefix = ''.join([c for c in g if not c.isdigit()])\n",
    "    digits = ''.join([c for c in g if c.isdigit()])\n",
    "    return (prefix, int(digits) if digits else -1)\n",
    "\n",
    "\n",
    "def sort_triple(g1, g2, g3):\n",
    "    return tuple(sorted([str(g1).strip(), str(g2).strip(), str(g3).strip()], key=gene_key))\n",
    "\n",
    "\n",
    "def stratified_subsample(df: pd.DataFrame, n: int, seed: int = 42):\n",
    "    \"\"\"Subsample with roughly preserved label ratio.\"\"\"\n",
    "    if n >= len(df):\n",
    "        return df.copy()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos = df[df['y'] == 1]\n",
    "    neg = df[df['y'] == 0]\n",
    "    pos_n = int(n * pos.shape[0] / df.shape[0])\n",
    "    neg_n = n - pos_n\n",
    "    pos_idx = rng.choice(pos.index.to_numpy(), size=min(pos_n, len(pos)), replace=False)\n",
    "    neg_idx = rng.choice(neg.index.to_numpy(), size=min(neg_n, len(neg)), replace=False)\n",
    "    out = pd.concat([df.loc[pos_idx], df.loc[neg_idx]], axis=0)\n",
    "    out = out.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def find_best_threshold(y_true, y_prob,\n",
    "                        metric='youden',\n",
    "                        min_precision_pos=None,\n",
    "                        t_min=0.05, t_max=0.95, t_step=0.005):\n",
    "    \"\"\"Search threshold on VAL set.\n",
    "\n",
    "    metric:\n",
    "      - 'youden': maximize TPR - FPR (robust to class ratio shift)\n",
    "      - 'balanced_acc': maximize (TPR + TNR)/2\n",
    "      - 'f1_pos': maximize F1 for positive class (often pushes threshold too low)\n",
    "\n",
    "    min_precision_pos:\n",
    "      - if not None, only consider thresholds with precision_pos >= this value\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    t_values = np.arange(t_min, t_max + 1e-12, t_step)\n",
    "\n",
    "    for t in t_values:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn + 1e-12)\n",
    "        tnr = tn / (tn + fp + 1e-12)\n",
    "        fpr = fp / (fp + tn + 1e-12)\n",
    "\n",
    "        precision_pos = tp / (tp + fp + 1e-12)\n",
    "        recall_pos = tpr\n",
    "        f1_pos = 2 * precision_pos * recall_pos / (precision_pos + recall_pos + 1e-12)\n",
    "\n",
    "        if min_precision_pos is not None and precision_pos < min_precision_pos:\n",
    "            continue\n",
    "\n",
    "        if metric == 'youden':\n",
    "            score = tpr - fpr\n",
    "        elif metric == 'balanced_acc':\n",
    "            score = 0.5 * (tpr + tnr)\n",
    "        elif metric == 'f1_pos':\n",
    "            score = f1_pos\n",
    "        else:\n",
    "            score = tpr - fpr\n",
    "\n",
    "        cand = {\n",
    "            'threshold': float(t),\n",
    "            'score': float(score),\n",
    "            'f1_pos': float(f1_pos),\n",
    "            'precision_pos': float(precision_pos),\n",
    "            'recall_pos': float(recall_pos),\n",
    "            'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
    "        }\n",
    "\n",
    "        if (best is None) or (cand['score'] > best['score']):\n",
    "            best = cand\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def eval_binary(y_true, y_prob, threshold=0.5, prefix=''):\n",
    "    \"\"\"Return AUC + confusion matrix + report.\"\"\"\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    return auc, cm, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3733929",
   "metadata": {},
   "source": [
    "## Feature preparation / 特征准备\n",
    "\n",
    "已经有的 `two_mer_dict` 和 `ae1_2`。\n",
    "\n",
    "- `two_mer_dict[gene]` gives a 400-d vector for each gene (2-mer frequency).\n",
    "- `ae1_2({g1,g2,g3})` returns (3,400) tensor for the rest-of-genome compressed features.\n",
    "\n",
    "下面是**可直接运行**的实现（与当前版本保持一致），只需要修改路径。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbfcd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total genes in FASTA: 4305\n",
      "Example: [('b0001', 'MKRISTTITTTITITTGNGAG')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building two_mer_dict: 100%|█████████████████████████████████████████████| 4305/4305 [00:00<00:00, 7354.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0001 [0.   0.   0.   0.   0.   0.05 0.   0.   0.   0.  ]\n",
      "b0002 [0.01587302 0.001221   0.00854701 0.01098901 0.002442   0.00854701\n",
      " 0.         0.003663   0.00610501 0.00732601]\n",
      "b0003 [0.01294498 0.00647249 0.00647249 0.01294498 0.         0.00647249\n",
      " 0.00323625 0.00323625 0.00323625 0.01294498]\n",
      "[2026-02-20 21:21:32] two_mer_dict & ae1_2 ready.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Part 2: Build two_mer_dict (2-mer features) + load Autoencoders + define ae1_2\n",
    "# =========================\n",
    "\n",
    "# ---- (A) Build two_mer_dict from FASTA ----\n",
    "# 如果你已经有 two_mer_dict，可以把 BUILD_2MER=False，然后跳过。\n",
    "BUILD_2MER = True\n",
    "\n",
    "if BUILD_2MER:\n",
    "    from Bio import SeqIO\n",
    "    from collections import Counter\n",
    "\n",
    "    fasta_path = '/data1/xpgeng/cross_pathogen/autoencoder/E.coli.tag_seq.fasta'\n",
    "\n",
    "    def read_fasta(fp):\n",
    "        gene_sequence_dict = {}\n",
    "        for record in SeqIO.parse(fp, 'fasta'):\n",
    "            gene_sequence_dict[record.id] = str(record.seq)\n",
    "        return gene_sequence_dict\n",
    "\n",
    "    gene_sequence_dict = read_fasta(fasta_path)\n",
    "    all_genes = set(gene_sequence_dict.keys())\n",
    "\n",
    "    print('Total genes in FASTA:', len(all_genes))\n",
    "    print('Example:', list(gene_sequence_dict.items())[:1])\n",
    "\n",
    "    standard_amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    all_2mers = [a + b for a in standard_amino_acids for b in standard_amino_acids]\n",
    "    two_mer_index = {two_mer: idx for idx, two_mer in enumerate(all_2mers)}\n",
    "\n",
    "    two_mer_dict = {}\n",
    "\n",
    "    for gene, sequence in tqdm(gene_sequence_dict.items(), desc='Building two_mer_dict'):\n",
    "        sequence = ''.join([aa for aa in sequence if aa in standard_amino_acids])\n",
    "\n",
    "        if len(sequence) < 2:\n",
    "            two_mer_dict[gene] = np.zeros(400, dtype=np.float32)\n",
    "            continue\n",
    "\n",
    "        two_mer_counts = Counter(sequence[i:i+2] for i in range(len(sequence)-1))\n",
    "        total_two_mers = sum(two_mer_counts.values())\n",
    "\n",
    "        feature_vector = np.zeros(400, dtype=np.float32)\n",
    "        for two_mer, count in two_mer_counts.items():\n",
    "            idx = two_mer_index.get(two_mer)\n",
    "            if idx is not None:\n",
    "                feature_vector[idx] = count / total_two_mers\n",
    "\n",
    "        two_mer_dict[gene] = feature_vector\n",
    "\n",
    "    for gene, vec in list(two_mer_dict.items())[:3]:\n",
    "        print(gene, vec[:10])\n",
    "\n",
    "# ---- (B) Load Autoencoders and define ae1_2 ----\n",
    "# 你可以保留你的架构和权重加载方式（与原来一致）\n",
    "\n",
    "LOAD_AUTOENCODERS = True\n",
    "\n",
    "if LOAD_AUTOENCODERS:\n",
    "\n",
    "    class Autoencoder(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Autoencoder, self).__init__()\n",
    "            self.encoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(400, 256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.35),\n",
    "                torch.nn.Linear(256, 128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.35),\n",
    "                torch.nn.Linear(128, 3),\n",
    "            )\n",
    "            self.decoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(3, 128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(128, 256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(256, 400),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "\n",
    "\n",
    "    class Autoencoder2(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Autoencoder2, self).__init__()\n",
    "            self.encoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(4304, 3000),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.2),\n",
    "                torch.nn.Linear(3000, 1000),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.3),\n",
    "                torch.nn.Linear(1000, 400),\n",
    "            )\n",
    "            self.decoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(400, 1000),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(1000, 3000),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(3000, 4304),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "\n",
    "\n",
    "    model = Autoencoder().to(device)\n",
    "    model.load_state_dict(torch.load('/data1/xpgeng/cross_pathogen/autoencoder/ae1_all_data_training.pth', map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    model2 = Autoencoder2().to(device)\n",
    "    model2.load_state_dict(torch.load('/data1/xpgeng/cross_pathogen/autoencoder/ae2_all_data_training.pth', map_location=device))\n",
    "    model2.eval()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ae1_2(three_genes):\n",
    "    # 输入：set({g1,g2,g3})\n",
    "    # 输出：(3,400) tensor\n",
    "    rest_genes = list(all_genes - three_genes)\n",
    "    inputs = np.vstack([two_mer_dict[gene] for gene in rest_genes]).astype(np.float32)\n",
    "\n",
    "    zeros_400 = np.zeros((2, 400), dtype=np.float32)\n",
    "    inputs = np.vstack([inputs, zeros_400])\n",
    "\n",
    "    inputs = torch.tensor(inputs).to(device)\n",
    "    inputs = model.encoder(inputs)\n",
    "    inputs = inputs.cpu().detach().numpy().T\n",
    "    inputs = torch.tensor(inputs).to(device)\n",
    "    outputs = model2.encoder(inputs)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "log_print('two_mer_dict & ae1_2 ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83fe82",
   "metadata": {},
   "source": [
    "# Part A — Random 5-Fold CV + Baseline Compare / 随机五折 + baseline对比\n",
    "\n",
    "**Goal / 目标**\n",
    "\n",
    "- 合并两批数据（不含必需基因 + 含必需基因），**随机**分成 5 份（**允许单基因在不同折中重复出现**）。\n",
    "- 每一折同时输出：\n",
    "  1) **Tripleknock**（你的 MLP pipeline）在 TEST 上的性能；\n",
    "  2) **Baseline**（基于单基因必需性：只要三基因中包含任意必需基因就判定致死）在同一 TEST 上的性能。\n",
    "\n",
    "> 注意：之前严格的 *gene-disjoint* 划分逻辑已被替换/注释（reviewer 这次要求的是 baseline 对比 + 随机五折）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "060d072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-20 21:22:06] Reading 2 baseline CSV files and concatenating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV files: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-20 21:22:06] Loaded triples_no_essential_123630-0_123630-1.csv rows=247,260 time=0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-20 21:22:06] Concat done. Total rows=247,260, time=0.2s\n",
      "[2026-02-20 21:22:06] Dataset y=1 ratio=0.5000 | y counts={0: 123630, 1: 123630}\n",
      "[2026-02-20 21:22:06] Fold 0 pool ready: 49,452 (y_mean=0.5000)\n",
      "[2026-02-20 21:22:06] Fold 1 pool ready: 49,452 (y_mean=0.5000)\n",
      "[2026-02-20 21:22:06] Fold 2 pool ready: 49,452 (y_mean=0.5000)\n",
      "[2026-02-20 21:22:06] Fold 3 pool ready: 49,452 (y_mean=0.5000)\n",
      "[2026-02-20 21:22:06] Fold 4 pool ready: 49,452 (y_mean=0.5000)\n",
      "[2026-02-20 21:22:06] ✅ Random 5-fold split ready (gene repetition allowed).\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Part A1 (FAST): Read CSV(s) and build RANDOM 5 folds (ALLOW gene repetition)\n",
    "# ✅ This replaces the previous strict gene-disjoint split.\n",
    "#  - We DO NOT enforce gene-level exclusivity.\n",
    "#  - We ONLY do label-stratified random splitting into 5 folds.\n",
    "# =========================\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "log_print(\"Reading 2 baseline CSV files and concatenating...\")\n",
    "\n",
    "t0 = time.time()\n",
    "frames = []\n",
    "\n",
    "for fp in tqdm(FILE_PARTS, desc=\"Reading CSV files\"):\n",
    "    t1 = time.time()\n",
    "\n",
    "    # no header, 4 columns: g1,g2,g3,y\n",
    "    dfp = pd.read_csv(\n",
    "        fp,\n",
    "        header=None,\n",
    "        names=[\"g1\", \"g2\", \"g3\", \"y\"],\n",
    "        dtype={\"g1\": \"string\", \"g2\": \"string\", \"g3\": \"string\", \"y\": \"int8\"},\n",
    "        engine=\"c\",\n",
    "        low_memory=False\n",
    "    )\n",
    "    frames.append(dfp)\n",
    "    log_print(f\"Loaded {os.path.basename(fp)} rows={len(dfp):,} time={time.time()-t1:.1f}s\")\n",
    "\n",
    "df = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "# strip (fast)\n",
    "for c in [\"g1\", \"g2\", \"g3\"]:\n",
    "    df[c] = df[c].str.strip()\n",
    "df[\"y\"] = df[\"y\"].astype(int)\n",
    "\n",
    "log_print(f\"Concat done. Total rows={len(df):,}, time={time.time()-t0:.1f}s\")\n",
    "log_print(f\"Dataset y=1 ratio={df.y.mean():.4f} | y counts={df.y.value_counts().to_dict()}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Build RANDOM 5 folds (label-stratified)\n",
    "# fold_pools[i] is the i-th fold dataframe (used as TEST/VAL/Train parts later)\n",
    "# -----------------------------------------------------\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_pools = []\n",
    "fold_indices = []  # keep indices for optional debugging / saving\n",
    "\n",
    "y = df[\"y\"].values\n",
    "for fold_id, (_, test_idx) in enumerate(skf.split(np.zeros(len(df)), y)):\n",
    "    dff = df.iloc[test_idx].reset_index(drop=True)\n",
    "    fold_pools.append(dff)\n",
    "    fold_indices.append(test_idx)\n",
    "    log_print(f\"Fold {fold_id} pool ready: {len(dff):,} (y_mean={dff['y'].mean():.4f})\")\n",
    "\n",
    "log_print(\"✅ Random 5-fold split ready (gene repetition allowed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015d97a",
   "metadata": {},
   "source": [
    "## Part A2 — Training MLP (streaming + AUC + confusion matrix)\n",
    "\n",
    "这里我们只**整理代码**，不改变你的 MLP 架构。\n",
    "\n",
    "### Why streaming chunk training? / 为什么要分块训练？\n",
    "如果你把 60万样本一次性做成 `X_train` 再搬到 GPU，很容易 OOM。\n",
    "\n",
    "这里采取两层策略：\n",
    "1. **Feature building on CPU (numpy)**\n",
    "2. **Mini-batch training on GPU (DataLoader)**\n",
    "3. 如果训练样本数 > 20000：按 `TRAIN_CHUNK_SIZE` 分批构建特征并训练（继续更新同一个模型参数）\n",
    "\n",
    "> 你可以先用 `TRAIN_SIZE_PER_FOLD=20000` 快速跑通，再逐步放大。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c66c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-20 21:22:47] Fold pool sizes: [49452, 49452, 49452, 49452, 49452]\n",
      "[2026-02-20 21:22:47] \n",
      "================== Fold 0 ==================\n",
      "[2026-02-20 21:22:47] Train folds: [2, 3, 4] | Val fold: 1 | Test fold: 0\n",
      "[2026-02-20 21:22:47] Train=148,356, Val=49,452, Test=49,452\n",
      "[2026-02-20 21:22:47] y_train mean=0.5000, y_val mean=0.5000, y_test mean=0.5000\n",
      "[2026-02-20 21:22:47] [INFO] Gene overlap train∩val=1318, train∩test=1318, val∩test=1318 (allowed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 VAL:  50%|███████████████████████████▉                            | 24668/49452 [04:58<04:36, 89.60it/s]"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Part A2 (FULL): Feature builder + MLP + CV training\n",
    "# - Add validation loss logging (reviewer requirement)\n",
    "# - Fix shuffle per epoch (avoid same order every epoch)\n",
    "# - Keep MLP architecture unchanged\n",
    "# - Streaming training by chunks to avoid GPU OOM\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Helper: zscore normalize\n",
    "# -------------------------\n",
    "def zscore(vec, eps=1e-8):\n",
    "    return (vec - vec.mean()) / (vec.std() + eps)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Feature builder\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def build_feature_vector(g1, g2, g3, rest_scale=0.05, norm_mode='block'):\n",
    "    \"\"\"\n",
    "    Return 2400-d float32 feature.\n",
    "\n",
    "    norm_mode:\n",
    "      - 'none': no normalization\n",
    "      - 'per_sample': zscore on full 2400-d\n",
    "      - 'block': zscore 1200(three) and 1200(rest) separately (recommended)\n",
    "    \"\"\"\n",
    "    # 3 knocked-out genes 2-mer feature (3*400=1200)\n",
    "    three = np.array([two_mer_dict[g] for g in [g1, g2, g3]], dtype=np.float32).flatten()\n",
    "\n",
    "    # rest-of-genome embedding from AE (3*400=1200)\n",
    "    rest = ae1_2({g1, g2, g3}).detach().cpu().numpy().astype(np.float32).flatten()\n",
    "\n",
    "    if norm_mode == 'block':\n",
    "        three = zscore(three)\n",
    "        rest  = zscore(rest)\n",
    "\n",
    "    feat = np.concatenate([three, rest * rest_scale], axis=0).astype(np.float32)\n",
    "\n",
    "    if norm_mode == 'per_sample':\n",
    "        feat = zscore(feat)\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "def build_XY_from_df(dfx: pd.DataFrame, rest_scale=0.05, norm_mode='block', desc='Build XY'):\n",
    "    \"\"\"\n",
    "    Build X (N,2400) and y (N,) from df with cols [g1,g2,g3,y]\n",
    "    \"\"\"\n",
    "    triples = dfx[['g1','g2','g3']].values.tolist()\n",
    "    y = dfx['y'].values.astype(np.int64)\n",
    "\n",
    "    X = np.zeros((len(triples), 2400), dtype=np.float32)\n",
    "    for i, (g1, g2, g3) in enumerate(tqdm(triples, desc=desc, leave=False)):\n",
    "        X[i] = build_feature_vector(g1, g2, g3, rest_scale=rest_scale, norm_mode=norm_mode)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MLP model (UNCHANGED)\n",
    "# -------------------------\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # Keep your architecture unchanged / 保持你的架构不变\n",
    "    def __init__(self, input_size=2400, hidden_size1=512, hidden_size2=256, output_size=1, dropout_rate=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.sigmoid(x)\n",
    "'''\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=2400, hidden_dims=(512, 256), output_size=1, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_size\n",
    "\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev = h\n",
    "\n",
    "        layers.append(nn.Linear(prev, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.sigmoid(x)\n",
    "'''\n",
    "\n",
    "# -------------------------\n",
    "# Training (streaming by chunks to avoid OOM)\n",
    "# -------------------------\n",
    "def train_one_epoch_streaming(\n",
    "    model, optimizer, criterion,\n",
    "    df_train_pool: pd.DataFrame,\n",
    "    batch_size=512,\n",
    "    train_chunk_size=20000,\n",
    "    rest_scale=0.05,\n",
    "    norm_mode='block',\n",
    "    fold=0,\n",
    "    epoch=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    One epoch training over df_train_pool, streaming by chunks to avoid OOM.\n",
    "\n",
    "    ✅ Important fix:\n",
    "    shuffle seed changes with fold/epoch, NOT constant every epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    idx = np.arange(len(df_train_pool))\n",
    "    rng = np.random.default_rng(SEED + fold * 100000 + epoch)  # ✅ vary per fold & epoch\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    seen = 0\n",
    "\n",
    "    n_chunks = int(np.ceil(len(idx) / train_chunk_size))\n",
    "    chunk_id = 0\n",
    "\n",
    "    for start in range(0, len(idx), train_chunk_size):\n",
    "        chunk_id += 1\n",
    "        chunk_idx = idx[start:start+train_chunk_size]\n",
    "        dfx = df_train_pool.iloc[chunk_idx].reset_index(drop=True)\n",
    "\n",
    "        # build features on CPU\n",
    "        Xc, yc = build_XY_from_df(\n",
    "            dfx,\n",
    "            rest_scale=rest_scale,\n",
    "            norm_mode=norm_mode,\n",
    "            desc=f\"Train chunk {chunk_id}/{n_chunks}\"\n",
    "        )\n",
    "\n",
    "        # DataLoader (CPU -> GPU in minibatches)\n",
    "        ds = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(Xc),\n",
    "            torch.from_numpy(yc.astype(np.float32))\n",
    "        )\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pred = model(xb).view(-1)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * len(yb)\n",
    "            seen += len(yb)\n",
    "\n",
    "        # free memory\n",
    "        del Xc, yc, ds, dl, dfx\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / max(seen, 1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Prediction helper\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, X_np, batch_size=2048):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    for i in range(0, len(X_np), batch_size):\n",
    "        xb = torch.tensor(X_np[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "        pb = model(xb).view(-1).detach().cpu().numpy()\n",
    "        probs.append(pb)\n",
    "    return np.concatenate(probs, axis=0)\n",
    "\n",
    "\n",
    "# ✅ NEW: validation loss + auc (reviewer wants val loss curve)\n",
    "@torch.no_grad()\n",
    "def eval_loss_and_auc(model, X_np, y_np, criterion, batch_size=2048):\n",
    "    \"\"\"\n",
    "    中文：计算某个数据集（val/test）的平均 loss + AUC\n",
    "    English: compute mean loss and AUC on a dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    y_np = np.asarray(y_np).astype(np.int64)\n",
    "    total_loss = 0.0\n",
    "    seen = 0\n",
    "    probs_all = []\n",
    "\n",
    "    for i in range(0, len(X_np), batch_size):\n",
    "        xb = torch.tensor(X_np[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "        yb = torch.tensor(y_np[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "\n",
    "        pb = model(xb).view(-1)   # sigmoid prob\n",
    "        loss = criterion(pb, yb)\n",
    "\n",
    "        total_loss += loss.item() * len(yb)\n",
    "        seen += len(yb)\n",
    "\n",
    "        probs_all.append(pb.detach().cpu().numpy())\n",
    "\n",
    "    probs_all = np.concatenate(probs_all, axis=0)\n",
    "    auc = roc_auc_score(y_np, probs_all)\n",
    "    return total_loss / max(seen, 1), auc, probs_all\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Threshold search helper\n",
    "# -------------------------\n",
    "def find_best_threshold(\n",
    "    y_true,\n",
    "    y_prob,\n",
    "    metric=\"youden\",\n",
    "    min_precision_pos=None,\n",
    "    t_min=0.01,\n",
    "    t_max=0.99,\n",
    "    t_step=0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    metric:\n",
    "      - \"youden\": maximize TPR - FPR (stable under prevalence shift)\n",
    "      - \"balanced_acc\": maximize (TPR + TNR)/2\n",
    "      - \"f1_pos\": maximize F1 for positive class (tends to smaller threshold -> high recall, many FP)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).astype(float)\n",
    "\n",
    "    best = None\n",
    "\n",
    "    for thr in np.arange(t_min, t_max + 1e-12, t_step):\n",
    "        y_hat = (y_prob >= thr).astype(int)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn + 1e-12)\n",
    "        fpr = fp / (fp + tn + 1e-12)\n",
    "        tnr = tn / (tn + fp + 1e-12)\n",
    "\n",
    "        precision_pos = tp / (tp + fp + 1e-12)\n",
    "        recall_pos = tpr\n",
    "        f1_pos = 2 * precision_pos * recall_pos / (precision_pos + recall_pos + 1e-12)\n",
    "\n",
    "        if min_precision_pos is not None and precision_pos < min_precision_pos:\n",
    "            continue\n",
    "\n",
    "        if metric == \"youden\":\n",
    "            score = tpr - fpr\n",
    "        elif metric == \"balanced_acc\":\n",
    "            score = 0.5 * (tpr + tnr)\n",
    "        elif metric == \"f1_pos\":\n",
    "            score = f1_pos\n",
    "        else:\n",
    "            score = tpr - fpr\n",
    "\n",
    "        if (best is None) or (score > best[\"score\"]):\n",
    "            best = {\n",
    "                \"threshold\": float(thr),\n",
    "                \"score\": float(score),\n",
    "                \"f1_pos\": float(f1_pos),\n",
    "                \"precision_pos\": float(precision_pos),\n",
    "                \"recall_pos\": float(recall_pos),\n",
    "                \"tn\": int(tn),\n",
    "                \"fp\": int(fp),\n",
    "                \"fn\": int(fn),\n",
    "                \"tp\": int(tp),\n",
    "            }\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main CV runner (RANDOM 5-fold, gene repetition allowed)\n",
    "# -------------------------\n",
    "def run_random_5fold_cv(df: pd.DataFrame, fold_pools: list):\n",
    "    \"\"\"Run 5-fold CV on the merged dataset (no gene-disjoint constraint).\n",
    "\n",
    "    Split strategy (keeps your previous Train/Val/Test pattern, but now folds are RANDOM):\n",
    "      - Test = fold k\n",
    "      - Val  = fold (k+1) mod 5\n",
    "      - Train = remaining 3 folds\n",
    "\n",
    "    In each fold we report:\n",
    "      1) Tripleknock (MLP) metrics on TEST\n",
    "      2) Baseline metrics (essential-gene presence rule) on the SAME TEST\n",
    "    \"\"\"\n",
    "\n",
    "    # collect fold-wise results\n",
    "    rows = []\n",
    "    aucs_triple = []\n",
    "    aucs_base   = []\n",
    "\n",
    "    log_print(\"Fold pool sizes: \" + str([len(x) for x in fold_pools]))\n",
    "\n",
    "    for fold in range(N_FOLDS):\n",
    "        log_print(\"\\n\" + \"=\"*18 + f\" Fold {fold} \" + \"=\"*18)\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # RANDOM folds (no gene exclusivity):\n",
    "        #  - 3 folds train, 1 fold val, 1 fold test\n",
    "        # -----------------------------------------------------\n",
    "        test_fold = fold\n",
    "        val_fold  = (fold + 1) % N_FOLDS\n",
    "\n",
    "        df_test = fold_pools[test_fold].reset_index(drop=True)\n",
    "        df_val  = fold_pools[val_fold].reset_index(drop=True)\n",
    "\n",
    "        df_train_pool = pd.concat(\n",
    "            [fold_pools[i] for i in range(N_FOLDS) if i not in (test_fold, val_fold)],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        # Optional train subsample (only train is subsampled; val/test keep full)\n",
    "        if TRAIN_SIZE_PER_FOLD is None:\n",
    "            df_train = df_train_pool.reset_index(drop=True)\n",
    "        else:\n",
    "            df_train = stratified_subsample(df_train_pool, TRAIN_SIZE_PER_FOLD, seed=SEED+fold).reset_index(drop=True)\n",
    "\n",
    "        log_print(f\"Train folds: {[i for i in range(N_FOLDS) if i not in (test_fold, val_fold)]} | \"\n",
    "                  f\"Val fold: {val_fold} | Test fold: {test_fold}\")\n",
    "        log_print(f\"Train={len(df_train):,}, Val={len(df_val):,}, Test={len(df_test):,}\")\n",
    "        log_print(f\"y_train mean={df_train.y.mean():.4f}, y_val mean={df_val.y.mean():.4f}, y_test mean={df_test.y.mean():.4f}\")\n",
    "\n",
    "        # (Optional) just log gene overlaps; do NOT enforce anything.\n",
    "        def genes_in_df(dfx):\n",
    "            return set(pd.unique(dfx[[\"g1\",\"g2\",\"g3\"]].to_numpy().ravel()))\n",
    "\n",
    "        g_tr  = genes_in_df(df_train)\n",
    "        g_val = genes_in_df(df_val)\n",
    "        g_te  = genes_in_df(df_test)\n",
    "        log_print(f\"[INFO] Gene overlap train∩val={len(g_tr & g_val)}, train∩test={len(g_tr & g_te)}, val∩test={len(g_val & g_te)} (allowed)\")\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Build VAL/TEST features once (same as your original pipeline)\n",
    "        # -----------------------------------------------------\n",
    "        X_val, y_val = build_XY_from_df(df_val, rest_scale=REST_SCALE, norm_mode=NORM_MODE, desc=f\"Fold {fold} VAL\")\n",
    "        X_test, y_test = build_XY_from_df(df_test, rest_scale=REST_SCALE, norm_mode=NORM_MODE, desc=f\"Fold {fold} TEST\")\n",
    "\n",
    "        # model (UNCHANGED)\n",
    "        model_mlp = MLP(dropout_rate=DROPOUT_RATE).to(device)\n",
    "        optimizer = optim.AdamW(model_mlp.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        best_auc = -1\n",
    "        best_state = None\n",
    "        best_epoch = 0\n",
    "        patience_left = PATIENCE\n",
    "\n",
    "        # store curves (reviewer request)\n",
    "        history = {\"train_loss\": [], \"val_loss\": [], \"val_auc\": []}\n",
    "\n",
    "        for epoch in range(1, MAX_EPOCHS+1):\n",
    "            train_loss = train_one_epoch_streaming(\n",
    "                model_mlp, optimizer, criterion,\n",
    "                df_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                train_chunk_size=TRAIN_CHUNK_SIZE,\n",
    "                rest_scale=REST_SCALE,\n",
    "                norm_mode=NORM_MODE,\n",
    "                fold=fold,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "\n",
    "            # val loss + val auc\n",
    "            val_loss, val_auc, y_val_prob = eval_loss_and_auc(model_mlp, X_val, y_val, criterion)\n",
    "\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_auc\"].append(val_auc)\n",
    "\n",
    "            log_print(f\"[Fold {fold}] Epoch {epoch}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}, val_auc={val_auc:.6f}\")\n",
    "\n",
    "            # early stopping on AUC (keep your logic)\n",
    "            if val_auc > best_auc + MIN_DELTA:\n",
    "                best_auc = val_auc\n",
    "                best_epoch = epoch\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model_mlp.state_dict().items()}\n",
    "                patience_left = PATIENCE\n",
    "            else:\n",
    "                patience_left -= 1\n",
    "                if patience_left <= 0:\n",
    "                    log_print(f\"[Fold {fold}] Early stop at epoch {epoch} (best_val_auc={best_auc:.6f} @epoch {best_epoch})\")\n",
    "                    break\n",
    "\n",
    "        # restore best\n",
    "        if best_state is not None:\n",
    "            model_mlp.load_state_dict(best_state)\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Threshold search on VAL\n",
    "        # -----------------------------------------------------\n",
    "        val_loss_best, val_auc_best, y_val_prob = eval_loss_and_auc(model_mlp, X_val, y_val, criterion)\n",
    "\n",
    "        best_t = find_best_threshold(\n",
    "            y_true=y_val,\n",
    "            y_prob=y_val_prob,\n",
    "            metric=THRESH_METRIC,\n",
    "            min_precision_pos=MIN_PRECISION_POS,\n",
    "            t_min=THRESH_MIN,\n",
    "            t_max=THRESH_MAX,\n",
    "            t_step=THRESH_STEP,\n",
    "        )\n",
    "\n",
    "        if best_t is None:\n",
    "            best_thr = 0.5\n",
    "            log_print(f\"[Fold {fold}] No valid threshold found, fallback thr=0.5\")\n",
    "        else:\n",
    "            best_thr = best_t[\"threshold\"]\n",
    "            log_print(f\"[Fold {fold}] Best threshold from VAL = {best_thr:.3f} | metric={THRESH_METRIC}\")\n",
    "            log_print(f\"[Fold {fold}] VAL best stats = {best_t}\")\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # TEST (Tripleknock)\n",
    "        # -----------------------------------------------------\n",
    "        test_loss, test_auc, y_test_prob = eval_loss_and_auc(model_mlp, X_test, y_test, criterion)\n",
    "        aucs_triple.append(test_auc)\n",
    "\n",
    "        y_pred = (y_test_prob >= best_thr).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "        rep = classification_report(y_test, y_pred, digits=4)\n",
    "\n",
    "        # additional metrics\n",
    "        f1_pos = f1_score(y_test, y_pred, pos_label=1)\n",
    "        p_pos  = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "        r_pos  = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "        log_print(f\"[Fold {fold}] ✅ Tripleknock TEST AUC = {test_auc:.6f}\")\n",
    "        log_print(f\"[Fold {fold}] Tripleknock TEST loss = {test_loss:.6f}\")\n",
    "        log_print(f\"[Fold {fold}] Tripleknock Confusion Matrix [[TN,FP],[FN,TP]]:\\n{cm}\")\n",
    "        log_print(f\"[Fold {fold}] Tripleknock Report:\\n{rep}\")\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # TEST (Baseline: essential-gene presence rule)\n",
    "        # -----------------------------------------------------\n",
    "        y_base_pred = df_test[[\"g1\",\"g2\",\"g3\"]].isin(essential_set).any(axis=1).astype(int).to_numpy()\n",
    "        y_base_score = y_base_pred.astype(float)\n",
    "\n",
    "        base_cm = confusion_matrix(y_test, y_base_pred, labels=[0, 1])\n",
    "        base_rep = classification_report(y_test, y_base_pred, digits=4)\n",
    "\n",
    "        try:\n",
    "            base_auc = roc_auc_score(y_test, y_base_score)\n",
    "        except Exception:\n",
    "            base_auc = float('nan')\n",
    "\n",
    "        aucs_base.append(base_auc)\n",
    "\n",
    "        base_f1 = f1_score(y_test, y_base_pred, pos_label=1)\n",
    "        base_p  = precision_score(y_test, y_base_pred, pos_label=1, zero_division=0)\n",
    "        base_r  = recall_score(y_test, y_base_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "        log_print(f\"[Fold {fold}] ✅ Baseline (essential-rule) TEST AUC = {base_auc}\")\n",
    "        log_print(f\"[Fold {fold}] Baseline Confusion Matrix [[TN,FP],[FN,TP]]:\\n{base_cm}\")\n",
    "        log_print(f\"[Fold {fold}] Baseline Report:\\n{base_rep}\")\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Save per-fold predictions (same TEST set, both methods)\n",
    "        # -----------------------------------------------------\n",
    "        pred_df = df_test.copy()\n",
    "        pred_df[\"y_true\"] = y_test\n",
    "        pred_df[\"tripleknock_prob\"] = y_test_prob.astype(float)\n",
    "        pred_df[\"tripleknock_pred\"] = y_pred.astype(int)\n",
    "        pred_df[\"baseline_pred\"] = y_base_pred.astype(int)\n",
    "\n",
    "        pred_out = os.path.join(OUTPUT_DIR, f\"fold_{fold}_test_predictions.csv\")\n",
    "        pred_df.to_csv(pred_out, index=False)\n",
    "        log_print(f\"[Fold {fold}] Saved TEST predictions -> {pred_out}\")\n",
    "\n",
    "        # Save curves (optional but useful for reviewer)\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "        plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Fold {fold} Loss Curves\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"fold_{fold}_loss_curve.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history[\"val_auc\"], label=\"val_auc\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Fold {fold} Validation AUC\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"fold_{fold}_val_auc_curve.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        # -----------------------------------------------------\n",
    "        # Collect fold-wise metrics\n",
    "        # -----------------------------------------------------\n",
    "        rows.append({\n",
    "            \"fold\": fold,\n",
    "            \"n_train\": int(len(df_train)),\n",
    "            \"n_val\": int(len(df_val)),\n",
    "            \"n_test\": int(len(df_test)),\n",
    "            \"threshold\": float(best_thr),\n",
    "\n",
    "            \"triple_auc\": float(test_auc),\n",
    "            \"triple_f1_pos\": float(f1_pos),\n",
    "            \"triple_precision_pos\": float(p_pos),\n",
    "            \"triple_recall_pos\": float(r_pos),\n",
    "\n",
    "            \"baseline_auc\": float(base_auc) if base_auc == base_auc else float('nan'),\n",
    "            \"baseline_f1_pos\": float(base_f1),\n",
    "            \"baseline_precision_pos\": float(base_p),\n",
    "            \"baseline_recall_pos\": float(base_r),\n",
    "        })\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Summary + save\n",
    "    # -----------------------------------------------------\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    out_csv = os.path.join(OUTPUT_DIR, \"cv_metrics_summary.csv\")\n",
    "    res_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    log_print(\"\\n\" + \"=\"*20 + \" CV Summary (Tripleknock vs Baseline) \" + \"=\"*20)\n",
    "    log_print(f\"Saved summary -> {out_csv}\")\n",
    "    log_print(f\"Tripleknock AUCs per fold: {aucs_triple}\")\n",
    "    log_print(f\"Baseline   AUCs per fold: {aucs_base}\")\n",
    "    log_print(f\"Tripleknock Mean AUC = {np.nanmean(aucs_triple):.6f}, Std = {np.nanstd(aucs_triple):.6f}\")\n",
    "    log_print(f\"Baseline   Mean AUC = {np.nanmean(aucs_base):.6f}, Std = {np.nanstd(aucs_base):.6f}\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "\n",
    "# ---- Run RANDOM 5-fold CV ----\n",
    "results_df = run_random_5fold_cv(df, fold_pools)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c460e-888d-43b9-9804-5ed60d916162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorchh] *",
   "language": "python",
   "name": "conda-env-.conda-pytorchh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
